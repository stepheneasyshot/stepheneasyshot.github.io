---
layout: post
description: > 
  本文梳理了当下最流行的几种AI大模型的原理概念
image: 
  path: /assets/img/blog/blogs_ai_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_cover.png
    960w:  /assets/img/blog/blogs_ai_cover.png
    480w:  /assets/img/blog/blogs_ai_cover.png
accent_image: /assets/img/blog/blogs_ai_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# AI大模型原理概念调研
## 基础理解
### Transformer
**Transformer** 是一种基于自我注意力机制的新型神经网络架构，它特别适用于语言理解。是 **Google** 研究人员开发的一种深度学习架构，它基于多注意力机制，该机制是在 2017 年的论文《Attention Is All You Need》中提出的。

> 文本被转换为称为令牌的数字表示，每个令牌通过从单词嵌入表中查找转换为一个向量。在每一层，每个标记都会在上下文窗口的范围内，通过并行多头关注机制与其他（未屏蔽的）标记进行上下文关联，从而放大关键标记的信号，减弱不那么重要的标记。

Transformer 的优点是没有递归单元，因此与早期的递归神经架构（RNN）（如长短期记忆（LSTM））相比，所需的训练时间更短。后来被广泛用于在大型（语言）数据集（如维基百科语料库和 Common Crawl）上训练大型语言模型（LLM）。

Transformer 还需要更少的计算来进行训练，而且更适合现代机器学习硬件，可将训练速度提高一个数量级。

### 自注意力机制
近年来， **RNN** 已成为翻译的典型网络架构，它以 **从左到右或从右到左** 的方式依次处理语言。一次读取一个单词，这迫使 RNNs 执行多个步骤，根据相距甚远的单词做出决定。

神经网络通常通过生成固定或可变长度的向量空间表示来处理语言。神经网络从单个单词或甚至单词片段的表征开始，然后汇总周围单词的信息，以确定特定语言在上下文中的含义。例如，要确定句子 **“我穿过......到达银行”** 中 “银行” 一词最可能的含义和适当的表示方法，需要知道句子是以 “......路” 还是 “......河” 结尾。

在处理上述示例时，RNN 只有在一步步读取 “bank” 和 “river” 之间的每个单词后，才能确定 “bank ”可能指的是河岸。先前的研究表明，粗略地说，决策所需的步骤越多，递归网络就越难学会如何做出这些决策。

RNN 的顺序性也使其更难充分利用 TPU 和 GPU 等现代快速计算设备，这些设备擅长并行处理而非顺序处理。卷积神经网络（CNN）的顺序性远低于 RNN，但在 ByteNet 或 ConvS2S 等 CNN 体系结构中，其数量却远低于 RNN。

相比之下，Transformer 只执行少量、恒定的步骤（根据经验选择）。在每一步中，它都会应用一种自我注意机制，直接模拟句子中所有单词之间的关系，而不管它们各自的位置如何。在前面的例子 **“I arrived at the bank after crossing the river ”** 中，为了确定 “bank ”一词指的是河岸而不是金融机构，Transformer 可以学会 **立即注意 “river ”一词** ，并在一个步骤中做出判断。事实上，在我们的英法互译模型中，我们观察到的正是这种行为。

更具体地说，为了计算给定单词（例如 “银行”）的下一个表征， **transformer会将其与句子中的其他单词进行比较** 。这些比较的结果就是句子中其他每个单词的注意力分数。这些注意力分数决定了其他每个单词对 “bank” 的下一个表示的贡献程度。在本例中，在计算 “bank” 的新表示时，能消除歧义的 “river” 可能会得到较高的关注分。然后，将注意力分数作为权重，对所有单词的表征进行加权平均，并将其输入全连接网络，从而生成 “bank” 的新表征，反映出该句子是在谈论河岸。
### Tokenization
在自然语言处理中， **token** 是指文本中的最小单位。它们可以是单词、子词或字符，具体取决于所使用的模型和任务。例如，在英语中，单词是 token 的基本单位；在中文中，字是 token 的基本单位。

**Tokenization** 是一个简单的过程，它将原始数据转换成有用的数据字符串。因其在网络安全和创建 NFT 中的应用而广为人知，同时它也是 NLP 流程的重要组成部分。在自然语言处理中，Tokenization 用于将段落和句子分割成更小的单位，以便更容易赋予其含义。

NLP 流程的第一步是收集数据（句子）并将其分解为可理解的部分（单词）。Tokenization 是将这些单词分解为更小的单位的过程。例如，“我喜欢吃苹果” 这句话可以被分解为 “我”、“喜欢”、“吃” 和 “苹果” 四个单词。
### 预训练
预训练是一种深度学习模型训练的策略，通常在大规模的数据集上进行。预训练的目标是通过在一个相关但较大的任务上训练模型，使得模型学习到通用的特征表示。这样的预训练模型在其他具体任务上的表现通常更好，因为它已经学习到了普适的特征。

在深度学习中，预训练可以分为两种主要类型：无监督预训练和有监督预训练。

* 无监督预训练：在无监督预训练中，模型在没有标签的大规模数据上进行预训练。常见的无监督预训练方法包括自编码器、变分自编码器、对比预训练等。预训练后，模型的参数会被调整到一种更有用的表示形式，使得它能够从输入数据中提取有意义的特征。

* 有监督预训练：在有监督预训练中，模型在一个与最终任务相关的较大数据集上进行预训练。然后，可以使用这些预训练的权重作为最终任务（如分类、回归等）的初始参数。这种方法通常能够加速最终任务的训练过程，特别是在目标任务数据较少时。

预训练的好处在于，通过利用大规模数据进行训练，模型可以学习到更泛化的特征表示，从而在具体任务上表现更好。这对于数据较少的任务或者计算资源有限的情况下特别有用。预训练的模型也经常用于迁移学习，可以将预训练模型的部分或全部用于新的任务，以提高模型的性能。

#### 预训练作用
* 加速训练过程：通过预训练，在大规模数据上学习到的通用特征表示可以作为初始化参数，加速模型在特定任务上的训练过程。这是因为预训练的参数已经接近最优，并且已经捕捉到了输入数据中的一些通用模式，这样在目标任务上的优化过程更容易收敛。
* 提高性能：预训练的模型通常在具体任务上表现更好。这是因为在预训练阶段，模型学习到了大量的数据中的通用特征，这些特征对于许多任务都是有用的。在目标任务中，预训练的模型能够更好地利用这些通用特征，从而提高性能。
* 解决数据不足问题：在许多实际任务中，数据往往是有限的，特别是深度学习模型需要大量的数据进行训练。通过预训练，可以利用大规模数据集进行通用特征的学习，然后将这些学到的特征应用于目标任务，从而克服数据不足的问题。
* 迁移学习：预训练的模型可以作为迁移学习的基础。将预训练模型的参数应用于新的相关任务，可以利用预训练模型在大规模数据上学习到的通用特征，从而在新任务上提高性能。这对于目标任务数据较少的情况下特别有用。
* 提高泛化能力：预训练有助于提高模型的泛化能力，即在未见过的数据上表现良好。通过在大规模数据上学习通用特征，模型更能够从输入数据中捕捉普遍的模式，而不是过度拟合训练集。

### 微调
微调是指在预训练模型的基础上，对其进行进一步的训练，以适应特定的任务。

微调的过程通常包括将预训练模型的权重加载到新模型中，然后在特定任务的数据集上进行训练。微调的目的是使模型能够更好地适应特定任务，而不是从头开始学习。

### 为什么分开做预训练和微调
大模型训练分为预训练和微调的阶段，这种方法提升了模型的泛化能力，又可以降低开发成本。这是因为两阶段训练策略能充分发挥数据与模型架构的优势，使模型既具备通用性，又能在特定场景中表现优异。

![blogs_ai_pre_process](/assets/img/blog/blogs_ai_pre_process.png)

泛化能力是指模型从训练数据中学习到的知识和模式，能够应用到新的数据、任务或环境中的能力。简单来说，就是模型在面对未曾见过的情况时，依然能够做出合理的判断、预测或生成合适内容的能力。例如，一个图像分类模型在学习了各种动物的图片后，当看到一张从未见过的动物新品种的图片时，能够根据已学的动物特征（如四条腿、毛茸茸等）正确地对其进行分类，这就体现了模型的泛化能力。

**泛化能力堪称 AGI（通用人工智能） 的根基** 。它意味着模型能够从有限的经验里汲取养分，进而在全新的任务或环境中崭露头角。打个比方，就如同学生通过学习课本上有限的例题，掌握了解题方法，便能举一反三，应对考试中形形色色的新题目。

一方面，它为 **从已知迈向未知** 架起了 **推理** 的桥梁，让模型依据已有的知识储备，对未曾接触过的数据和情境做出合理判断。另一方面，在多领域的复杂任务矩阵中，泛化能力使得模型无需推倒重来，就能灵活运用所学，大大提升了知识的复用效率。

为何它至关重要，是因为现实世界犹如一个无穷无尽的宝库，数据的类型和分布千变万化，训练数据不过是沧海一粟，根本无法穷尽所有可能性。泛化能力强的模型，恰似拥有敏锐洞察力的探险家，能够迅速适应新场景，无论面对何种未知挑战，都能展现出强大的实用性和抗干扰的鲁棒性，真正将所学知识的价值最大化。

#### 预训练与微调是如何提升泛化能力的？

大模型训练分为预训练和微调的阶段，这种方法提升了模型的泛化能力。这是因为两阶段训练策略能充分发挥数据与模型架构的优势，使模型既具备通用性，又能在特定场景中表现优异。

**预训练提升了通用泛化能力**

1、海量数据学习通用知识
预训练阶段使用了多样化的海量语料（如书籍、文章、网站等），这些数据涵盖了广泛的领域和语言结构，帮助模型学习到语言的底层规律（如词汇语义、句法结构），不同场景下的通用模式和上下文关系。
2、构建广泛的知识基础
模型通过预训练，积累了关于语言和世界知识的普遍理解。这种知识能够在下游任务中跨领域迁移和应用。
3、减少过拟合的风险
预训练阶段的无监督学习方式依赖于大量未标注数据，使模型能够专注于学习语言规律，而非记忆训练数据，增强了对未见数据的泛化能力。

**微调提升了特定场景的泛化能力**

1、针对性调整
微调阶段使用与目标任务相关的小规模、高质量标注数据来进一步训练模型，让模型能更精确地适应特定的场景或任务（如情感分析、机器翻译、法律文本理解等）。
2、增强领域泛化能力
微调让模型可以在一个广泛知识的基础上，快速适应某些特定领域的特定需求，而不必从头训练，体现了迁移学习的强大之处。
3、降低训练数据需求
微调需要的数据量远远小于从头开始训练一个模型，这种高效性使得泛化能力更易拓展到更多场景。

**结合预训练和微调的好处**

1、通用性与特定性平衡
预训练提供了通用语言能力，微调则强化了特定任务的表现，这种组合让模型既有“广度”也有“深度”。
2、跨任务泛化
微调后的模型往往能在相关任务中表现出色，比如一个在医疗文本上微调过的模型，可能在类似领域（如法律文本）的任务中也具备一定的泛化能力。实际应用表明，经过预训练和微调的模型比传统的单任务训练模型在性能上有巨大提升。

总之预训练和微调的两阶段训练方式不仅提升了大模型的泛化能力，还显著提高了模型的实际应用价值。预训练让模型学会了普适性规律，而微调则针对特定需求进一步优化，从而在广泛的任务和领域中实现高效、可靠的表现。这种训练策略是大模型成功的重要原因之一。

### 预训练和微调的成本
预训练和微调是大模型训练的两个主要阶段，它们各自具有不同的成本和适用场景。

**预训练：一次性高投入，长期复用**

预训练是大模型训练的核心阶段，虽然需要大量计算资源和数据，但其成本可以通过以下方式分摊：
* 通用知识学习：预训练模型通过海量数据学习通用特征（如语言模式、图像特征），这些知识可以迁移到多种任务中，避免了为每个任务从头训练模型的成本。
* 模型复用：预训练模型可以作为一个通用基础模型，供多个任务和开发者使用。例如，Meta 的 LLama 系列、阿里的通义千问等模型被广泛应用于各种下游任务，显著降低了重复训练的成本。

**微调：低成本适应特定任务**

微调是在预训练模型的基础上，使用少量任务特定数据进行调整，其成本远低于从头训练模型：

* 数据效率：微调通常只需要少量标注数据（可能是预训练数据的千分之一甚至更少），大大减少了数据收集和标注的成本。
* 计算效率：微调只需要调整部分模型参数或少量训练步骤，计算资源需求显著低于预训练。例如，微调一个百亿参数模型可能只需要几小时到几天，而预训练可能需要数周甚至数月。
* 快速迭代：微调允许开发者快速试验和优化模型，适应不同任务需求，而无需重新进行昂贵的预训练。

#### 降低开发门槛
预训练和微调的分阶段设计降低了开发者的技术门槛和资源需求：
* 无需从头训练：开发者可以直接使用预训练模型，通过微调快速构建应用，而无需掌握复杂的模型设计和训练技术。
* 小团队也能参与：即使是资源有限的小团队或个人开发者，也可以通过微调预训练模型，开发出高性能的 AI 应用。

目前国内用户使用大模型时，大部分都是直接使用开源的预训练模型（如通义千问、LLama 等），这些模型已经通过海量数据训练，具备了强大的通用能力。用户只需根据自己的特殊需求，使用少量领域数据对模型进行微调，即可快速适配具体任务。这种方式不仅节省了从头训练模型的高昂成本，还大幅缩短了开发周期，降低了技术门槛，使得大模型能够更高效地应用于各行各业，如金融、医疗、教育等领域。
### 多模态
多模态AI是一种将不同形式的数据（如文本、图像、音频等）融合在一起的技术，旨在让模型从多个维度感知和理解信息。这种融合使得AI系统能够从每种模态中获取独特的但互补的信息，从而构建出更全面的世界观。例如，在一个自动驾驶场景中，图像数据可以帮助系统识别道路上的行人，而雷达数据则能够感知车距，两者结合能够显著提升决策准确性。

多模态AI的核心思想是突破单一模态的局限，通过多种模态的协同作用，提升模型的表现力和泛化能力。
### 大模型的定义
大模型是指具有**大量参数**的机器学习模型。这些模型通常在大规模数据集上进行训练，以学习复杂的模式和特征。大模型通常具有更高的计算能力和更多的参数，这使得它们能够更好地理解和生成人类语言。

大模型的优点包括：
* 更好的理解和生成人类语言：大模型可以更好地理解和生成人类语言，因为它们具有更广泛的词汇和更复杂的语法。
* 更好的性能：大模型通常在大规模数据集上进行训练，这使得它们能够更好地学习和泛化。
* 更好的适应性：大模型可以更好地适应不同的任务和领域，因为它们具有更广泛的知识和技能。

### 大模型的分类
大模型可以根据其训练数据和计算资源的不同进行分类。以下是一些常见的大模型分类：
* 基于 Transformer 的模型：这些模型使用 Transformer 架构，这是一种基于自我注意力机制的神经网络架构。这些模型通常在大规模数据集上进行训练，以学习语言的通用模式和特征。
* 基于 LSTM 的模型：这些模型使用 LSTM 架构，这是一种递归神经网络架构。这些模型通常在较小的数据集上进行训练，以学习语言的局部模式和特征。
* 基于 CNN 的模型：这些模型使用 CNN 架构，这是一种卷积神经网络架构。这些模型通常在图像和视频数据上进行训练，以学习图像和视频的特征。

### 大模型的应用
大模型在自然语言处理、计算机视觉、语音识别等领域都有广泛的应用。以下是一些常见的应用：
* 自然语言处理：大模型在自然语言处理任务中具有广泛的应用，如文本分类、命名实体识别、问答系统等。
* 计算机视觉：大模型在计算机视觉任务中也有广泛的应用，如图像分类、目标检测、图像生成等。
* 语音识别：大模型在语音识别任务中也有广泛的应用，如语音转文本、语音合成等。

### 大模型的训练数据

大模型的训练需要大量的数据。以下是一些常见的训练数据：
* 文本数据：文本数据是最常见的训练数据，包括书籍、文章、新闻、代码等。这些数据通常包含大量的文本和标签，用于训练模型理解和生成文本。
* 图像数据：图像数据包括图像和图像的标签，用于训练模型理解和生成图像。
* 语音数据：语音数据包括语音和语音的标签，用于训练模型理解和生成语音。

在使用大模型时，经常看到多少B或者多少M的模型，或者是他的上下文长度为多少K，下面就是对这些常见单位量级的理解：
* K（Kilo, 千）：表示 1,000。在机器学习模型中，通常用来描述较小模型的参数量，比如 100K（十万）参数。
* M（Million, 百万）：表示 1,000,000。一般用于中等规模的模型，比如 BERT-base（110M）。
* B（Billion, 十亿）：表示 1,000,000,000。大型模型通常达到这一量级，比如 GPT-3（175B）。
* T（Trillion, 万亿）：表示 1,000,000,000,000。这代表非常巨大的参数量。GPT-4
的一些版本和其他超大规模模型已经达到甚至超过 1T 参数。

### 训练方法
大模型的训练需要使用深度学习方法。以下是一些常见的深度学习方法：
* 监督学习：监督学习是一种深度学习方法，其中模型被训练来预测输入数据的标签。在自然语言处理任务中，监督学习通常用于文本分类、命名实体识别、问答系统等。
* 无监督学习：无监督学习是一种深度学习方法，其中模型被训练来从数据中学习模式和特征。在自然语言处理任务中，无监督学习通常用于文本聚类、主题建模、文本生成等。
* 强化学习：强化学习是一种深度学习方法，其中模型被训练来通过与环境的交互来学习如何做出最佳决策。在自然语言处理任务中，强化学习通常用于对话系统、推荐系统等。

### 训练硬件
大模型的训练需要大量的计算资源。以下是一些常见的训练硬件：
* GPU：GPU 是一种图形处理器，用于加速深度学习模型的训练。GPU 通常具有大量的核心和内存，可用于训练大型模型。
* TPU：TPU 是一种专用处理器，用于加速深度学习模型的训练。TPU 通常具有大量的核心和内存，可用于训练大型模型。
* 集群：集群是一种计算资源，用于加速深度学习模型的训练。集群通常由多个 GPU 或 TPU 组成，可用于训练大型模型。

## Chat GPT篇
GPT，generative pre-trained transformer，即生成式预训练transformer，是一种大型语言模型 (LLM，large language model)，利用深度学习生成类似人类的文本。神经网络在包含文本和代码的海量数据集上进行训练，使其能够理解并生成连贯且与上下文相关的响应。作为生成式人工智能领域的关键组成部分，GPT 突破了人工智能的极限，使机器能够生成具有创造性和人类品质的内容。

![blogs_ai_chatgpt](/assets/img/blog/blogs_ai_chatgpt.webp)

### GPT 如何工作？
GPT（生成式预训练 Transformer）的工作原理是分析输入文本，并根据训练它的海量文本数据集预测最可能的下一个单词或短语，本质上通过理解上下文并生成看似自然连贯的响应来模仿人类语言模式。

GPT 模型是一种复杂的人工神经元网络，分层组织以深入处理信息，就像人类大脑一样。它的架构被称为 **transformer** ，这是谷歌研究人员于 2017 年发明并开源的一种神经网络设计。transformer 允许它 **同时分析整个句子，而不是按顺序分析，从而掌握单词之间的关系** ，而不管单词之间的距离有多远。 

这种能力源自“自我注意力”，这种机制让模型能够 **权衡每个单词相对于所有其他单词的重要性** ，模仿人类如何关注句子的不同部分以获取上下文。  

训练这个模型需要输入大量的文本数据（书籍、文章、代码、在线对话），让模型接触人类语言的范围和细微差别。通过反复接触和“反向传播”过程，模型会从预测误差中学习，从而改进其语言的内部表征，变得非常善于理解和生成人类质量的文本。

GPT 工作原理的要点：

* 标记化：文本被分解成称为“标记”（单词或单词的一部分）的较小单元，模型对这些单元进行单独处理。
* 在大型数据集上进行训练：GPT 使用来自互联网的海量文本数据进行训练，包括书籍、文章和代码，从而使其能够学习多种语言模式。
* 预测下一个词：该模型分析输入序列的上下文并计算每个可能的下一个标记的概率，选择最有可能的标记作为输出。
* Transformer 架构：这一关键设计元素使模型能够同时考虑整个句子中单词之间的关系，而不是按顺序处理它们。
* 自注意力机制：在 Transformer 中，“自注意力”使模型在进行预测时能够专注于输入序列中最相关的部分。

### GPT的应用？
#### 内容创作
GPT 模型可以帮助为网站、博客、社交媒体等创建高质量的内容。对于需要定期创建引人入胜且信息丰富的内容的企业和个人来说，这可能是一种有价值的工具。

一个例子是使用 GPT 模型根据提供给模型的特定提示和信息来起草自定义社交媒体帖子或撰写产品描述。这可以帮助腾出时间来完成其他任务。

#### 客户服务
这些模型可用于支持聊天机器人和虚拟助手，以提供客户支持、回答问题和解决问题。这可以帮助企业提高客户满意度并降低支持成本。

想象一下，无论白天还是晚上，您都可以随时获得即时客户服务支持，而无需等待或浏览复杂的电话菜单。这就是人工智能客户服务的潜力。

#### 聊天机器人
除了客服之外，聊天机器人还可以被更广泛的受众用来回答问题，甚至进行随意的交谈。随着 GPT 技术的不断发展，未来有望出现更加复杂、更像人类的聊天机器人。

#### 代码生成
GPT 技术有可能彻底改变开发人员的工作方式。它可用于协助计算机代码生成，这对于希望自动化任务或加快开发过程的开发人员来说是一个有价值的工具。

这可以让开发人员腾出时间专注于更复杂、更有创意的任务。想象一下，在未来，即使是那些编码经验有限的人也可以借助人工智能代码生成工具将自己的想法变成现实。

#### 教育
GPT 有可能通过提供针对每个学生需求的个性化学习体验来改变教育。它可以提供量身定制的反馈、练习题、互动模块、学习计划、虚拟导师和语言支持。这种人工智能的整合可以为所有学生创造一个包容、引人入胜且有效的学习环境。

### GPT 为何重要？
GPT 的意义在于它能够通过语言弥合人与机器之间的鸿沟。它能够熟练地理解和生成类似人类的文本，为沟通、自动化和创意表达开辟了新的可能性。

此外，GPT 对各个领域和任务的适应性使其成为一种变革性技术，有可能彻底改变各种各样的行业。 

### GPT 培训
训练 GPT 模型是一个计算密集型的过程，需要输入大量文本数据并采用自我监督学习方法。该模型不依赖于明确标记的数据；相反，它通过识别数据本身的模式和关系来学习。

训练过程通常涉及以下步骤：

* 数据准备：第一步是收集并准备大量文本和代码数据集。该数据集经过精心策划，尽可能多样化和具有代表性，涵盖广泛的主题、写作风格和语言。
* 标记化：然后将文本数据划分为称为“标记”的较小单元。这些可以是单个单词、单词的一部分，甚至是字符，具体取决于特定的 GPT 模型和所需的粒度级别。
* 模型初始化： GPT 模型使用随机参数进行初始化。这些参数将在训练过程中随着模型从数据中学习而进行调整。
* 自监督学习：然后向模型输入标记化的文本数据，并让其预测序列中的下一个标记。例如，给定输入“The cat sat on the”，模型可能会预测“mat”。
* 反向传播和优化：将模型的预测与训练数据中的实际下一个标记进行比较，并使用它们之间的差异来计算“损失”值。此损失表示模型的预测与事实的偏差。然后，模型使用反向传播来调整其内部参数以最大限度地减少此损失。这种预测、损失计算和参数调整的迭代过程会持续许多个时期，模型会逐渐提高其准确预测序列中下一个标记的能力。

训练数据集的大小、GPT 模型的复杂性以及可用的计算资源在确定训练所需的时间和资源方面都起着至关重要的作用。训练大型 GPT 模型可能需要大量时间，需要专门的硬件和大量的能源消耗。

## Grok篇
Grok 是埃隆·马斯克的公司 **xAI** 开发的一款对话式人工智能聊天机器人。Grok 可以通过社交媒体平台 X 访问实时信息，据说可以回答大多数其他人工智能系统通常拒绝回答的“棘手”问题。与独立的 AI 工具不同，Grok 位于 X（前身为 Twitter）内。要访问它，用户必须 **登录 X 并购买 Grok 订阅** 。这种整合符合马斯克将社交媒体平台转变为“万能应用”的愿景，Grok 等工具可以补充平台的服务生态系统。

![blogs_ai_grok](/assets/img/blog/blogs_ai_grok.jpeg)

Grok 本质上是马斯克对 ChatGPT 的回应，后者的创造者 (OpenAI) 是马斯克于 2015 年共同创立的，但在 与现任首席执行官 Sam Altman发生权力斗争后于 2018 年离开。此后，马斯克谴责 ChatGPT 过于 **左倾和危险** 。据马斯克称，xAI 旨在成为 OpenAI 的直接竞争对手，其 Grok 聊天机器人不仅是 ChatGPT 的“反觉醒”对手，而且还展示了更大的 生成 AI领域的新可能性。

Grok-1 是 Grok 所依赖的大型语言模型，它使用基于软件管理系统 Kubernetes、机器学习框架 JAX 和编码语言 Rust的定制技术栈进行训练，所有这些都帮助 xAI 比其他聊天机器人更快、更高效地开发 Grok。 

与所有 LLM 一样，Grok-1 也接受了从互联网上抓取的大量文本数据的训练，这些数据包括从维基百科文章到科学论文的所有内容。但 Grok 的不同之处在于它 **可以直接访问 X 上的帖子** 。据该公司称，这使得 Grok 能够“实时了解世界”，这让它“比其他模型具有巨大优势”。

### 独特的两种模式
Grok 提供两种交互方式：“趣味模式”和“常规模式”。默认情况下，Grok 以“趣味模式”运行，这会导致聊天机器人呈现出更前卫或更幽默的个性，有时甚至会产生与事实不符的回答。“常规模式”通常会提供更准确的答案，但与所有 AI 聊天机器人一样，xAI 表示它仍然可能生成 虚假或矛盾的信息。

### 交互范围更宽泛
Grok 可以起草电子邮件、调试代码、产生想法等等，而且全部以 流畅的类人语言完成。它只需接收输入（如命令或问题），应用训练数据中的知识，然后使用复杂的 神经网络生成相关的文本输出。

虽然它的使用方式与其他 AI 聊天机器人相同，事实上，Grok 愿意回答大多数其他聊天机器人会拒绝的问题，无论这些问题有多么 禁忌或具有潜在危害。它的设计更像是一个“好玩又有趣的聊天机器人，你可以用它进行更另类或尖刻的对话。”

从用户界面的角度来看，Grok 还可以同时处理多个查询，用户可以在这些答案之间切换，正如 xAI 联合创始人 Toby Pohlen 在视频演示中展示的那样。代码生成可以直接在Visual Studio Code 编辑器中打开，而文本响应可以保存在 markdown 编辑器中以供日后使用。

### 同GPT的对比
#### Grok更具有实时性
Grok 可以直接实时访问 X 上的帖子，而 ChatGPT 的免费版本只知道截至 2022 年 1 月的信息，付费版本只知道截至 2023 年 4 月的信息。这意味着 Grok 可以参与有关最近事件的对话，例如以色列-哈马斯战争或 2024 年超级碗。事实上，根据提出的问题，Grok 实际上会显示它所引用的 X 上的真实帖子，以表明其观点来自何处。

然而，Vice 的一项调查发现，**Grok 倾向于散布有关时事的不准确信息，并让人们相信未经证实的阴谋论**

#### Grok更不具有政治正确性
用马斯克的话来说，Grok 是“最大限度寻求真相”和“基于事实”的，这意味着它毫无歉意并且在交流时不考虑政治正确性。  

AI 创建了一个不太政治正确的聊天机器人，而此时大多数其他大型人工智能公司都在努力让自己的聊天机器人更加政治化。OpenAI 声称，其新推出的 GPT-4 LLM（为 ChatGPT 的付费版本提供支持）对“不允许的内容”请求的响应可能性降低了 82%，其中包括“仇恨、骚扰”和“暴力”的材料。而 Anthropic 的 Claude聊天机器人是使用宪法人工智能进行训练的，这 有助于降低其产生有毒、危险或不道德反应的可能性。

## Gemini篇
Google Gemini（原名 Bard）是 **Google** 设计的一款人工智能 (AI)聊天机器人工具，使用自然语言处理 ( NLP ) 和机器学习模拟人类对话。除了补充 Google 搜索外，Gemini 还可以集成到网站、消息平台或应用程序中，为用户问题提供逼真的自然语言回答。

![blogs_ai_gemini](/assets/img/blog/blogs_ai_gemini.webp)

Google Gemini 是一系列多模式 AI大型语言模型 ( LLM )，具有语言、音频、代码和视频理解能力。

2024 年 12 月 11 日，谷歌发布了其 LLM 的更新版本，其中包含 **Gemini 2.0 Flash** ，这是 Google AI Studio 和 Vertex AI Gemini 应用程序编程接口 (API) 中集成的实验版本。

### 特性
Gemini 集成了 NLP 功能，可提供理解和处理语言的能力。Gemini 还用于理解输入查询和数据。它能够理解和识别图像，使其能够解析复杂的视觉效果，例如图表和数字，而无需外部光学字符识别 ( OCR )。它还具有广泛的多语言功能，可用于翻译任务和跨不同语言的功能。

与谷歌之前的 AI 模型不同，Gemini 本身就是多模态的，这意味着它对跨多种数据类型的数据集进行端到端训练。作为多模态模型，Gemini 具有跨模态推理能力。这意味着 Gemini 可以对一系列不同的输入数据类型进行推理，包括音频、图像和文本。例如，Gemini 可以理解手写笔记、图形和图表来解决复杂问题。Gemini 架构支持直接将文本、图像、音频波形和视频帧作为交错序列提取。

### Google Gemini 如何运作？
Google Gemini 首先在海量数据上进行训练。训练后，该模型使用多种神经网络技术来理解内容、回答问题、生成文本并产生输出。

具体来说，Gemini LLM 使用基于 **Transformer** 模型的神经网络架构。Gemini 架构已得到增强，可以处理不同数据类型（包括文本、音频和视频）的长上下文序列。Google DeepMind 在 Transformer 解码器中使用高效的 **注意力机制** ，帮助模型处理跨越不同模态的长上下文。

Gemini 模型已在 Google DeepMind 的多种多模式和多语言文本、图像、音频和视频数据集上进行了训练，并使用高级数据过滤来优化训练。随着不同的 Gemini 模型被部署以支持特定的 Google 服务，有一个有针对性的微调过程可用于进一步优化用例的模型。在训练和推理阶段，Gemini 受益于使用 Google 最新的张量处理单元芯片 Trillium（第六代 Google Cloud TPU）。与 TPU v5 相比，Trillium TPU 提供了更高的性能、更低的延迟和更低的成本。它们也比以前的版本更节能。

### Gemini 可以执行的任务
Google Gemini 可以实用地应用于完成各种任务。
LLM 面临的一个关键挑战是存在偏见和潜在有害内容的风险。据 Google 称，Gemini 针对偏见和毒性等风险进行了广泛的安全测试和缓解，以帮助提供一定程度的 LLM 安全性。为了进一步确保 Gemini 正常运行，这些模型根据语言、图像、音频、视频和代码领域的学术基准进行了测试。Google 向公众保证，它遵守一系列 AI 原则。

谷歌在 2023 年 12 月 6 日发布时表示，Gemini 将包含一系列不同大小的模型，每种模型都针对特定的用例和部署环境而设计。

* Ultra 模型是最高端的，专为高度复杂的任务而设计。
* Pro 模型专为大规模性能和部署而设计。截至 2023 年 12 月 13 日，谷歌已在 Google Cloud Vertex AI 和 Google AI Studio 中启用了对 Gemini Pro 的访问。对于代码，Gemini 的一个版本用于支持 Google AlphaCode 2 生成式 AI 编码技术。
* Nano 模型针对的是设备上的使用案例。Gemini Nano 有两个不同版本：Nano-1 模型有 18 亿个参数，而 Nano-2 有 32.5 亿个参数。Nano 被嵌入的地方包括 Google Pixel 9 智能手机。

### Gemini 有何用途？用例和应用
Google Gemini 模型有多种用途，包括文本、图像、音频和视频理解。Gemini 的多模态特性还使这些不同类型的输入能够组合起来生成输出。

* 文本摘要。Gemini模型可以从不同类型的数据中总结内容。
* 文本生成。Gemini可以根据用户提示生成文本。该文本也可以由问答类型的聊天机器人界面驱动。
* 文本翻译。Gemini模型具有广泛的多语言功能，可以翻译和理解 100 多种语言。
* 图像理解。Gemini无需外部 OCR 工具即可解析复杂的视觉效果，例如图表、图形和图解。它可用于图像字幕和视觉问答功能。
* 音频处理。Gemini支持100 多种语言的语音识别和音频翻译任务。
* 视频理解。Gemini可以处理和理解视频片段帧，以回答问题并生成描述。
* 多模态推理。Gemini的一个主要优势是它使用多模态 AI 推理，可以混合不同类型的数据以提示生成输出。
* 代码分析和生成。Gemini可以理解、解释和生成流行编程语言的代码，包括 Python、Java、C++ 和 Go。

#### 应用
Google 开发了 Gemini 作为基础模型，以便广泛集成到各种 Google 服务中。开发人员还可以使用它来构建自己的应用程序。使用 Gemini 的应用程序包括：

* AlphaCode 2. Google DeepMind 的 AlphaCode 2 代码生成工具使用了 Gemini Pro 的定制版本。
* Google Pixel。谷歌打造的 Pixel 8 Pro 智能手机是首款运行 Gemini Nano 的设备。Gemini 为现有 Google 应用中的新功能提供支持，例如 Recorder 中的摘要功能和 Gboard 中用于消息应用的智能回复功能。
* Android。Pixel 8 Pro 是首款受益于 Gemini 的 Android 智能手机。Android 开发人员可以通过 Android 操作系统的 AICore 系统功能使用 Gemini Nano 进行构建。
* Vertex AI。Google Cloud 的 Vertex AI 服务提供了开发人员可以用来构建应用程序的基础模型，同时还提供对 Gemini Pro 的访问权限。
* Google AI Studio。开发人员可以使用基于 Web 的 Google AI Studio 工具通过 Gemini 构建原型和应用程序。
* 搜索。谷歌已尝试在其AI 概览中使用 Gemini来减少延迟并提高质量。

### Gemini的局限性
一些限制可能会导致潜在最终用户犹豫。这些限制包括：

* 训练数据。与所有 AI 聊天机器人一样，Gemini 必须学会给出正确的答案。要做到这一点，模型必须接受正确信息训练，这些信息不能不准确或误导。然而，它们还必须能够识别错误或误导性的信息。
* 偏见和潜在危害。人工智能训练是一个永无止境的、计算密集型的过程，因为总有新的信息需要学习。谷歌声称，在所有 Gemini 模型中，它都遵循了负责任的开发实践，包括广泛的评估，以帮助限制偏见和潜在危害的风险。
* 原创性和创造性。Gemini制作的内容的原创性和创造性是有限的。免费版本尤其如此，它在处理复杂的提示、多个步骤和细微差别以及产生足够的输出方面存在困难。免费版本基于 Gemini Pro LLM，其功能更有限；该平台的付费版本提供更高级的功能。

#### 值得担心的事情

Gemini 的一个担忧在于它可能会向用户提供有偏见或虚假的信息。输入 Gemini 的训练数据中存在的任何偏见都可能导致问题。例如，与所有先进的人工智能软件一样，如果训练数据排除了特定人群中的某些群体，则会导致输出结果出现偏差。

Gemini 倾向于产生幻觉和其他虚构内容，并将它们当作真实内容传递给用户，这也是一个令人担忧的问题。自ChatGPT 诞生以来，这一直是其响应面临的最大风险之一，其他高级 AI 工具也是如此。此外，由于 Gemini 并不总是理解上下文，因此其响应可能与用户提供的提示和查询不相关。

## Deepseek篇
DeepSeek 是一家中国人工智能研究实验室，与 OpenAI 类似，由中国对冲基金 High-Flyer 创立。与其他商业研究实验室（可能 Meta 除外）不同，DeepSeek 主要将其模型开源。与 Meta 不同的是，DeepSeek 真正将其模型开源，允许任何人将其用于商业目的。它已发布了多个模型系列，每个模型都以 DeepSeek 命名，后面跟有版本号。

![blogs_ai_deepseek](/assets/img/blog/blogs_ai_deepseek.jpg)

官方介绍：

>我们推出的DeepSeek-V3是一个强大的专家混合（MoE）语言模型，拥有671B个总参数，每个标记有37B个激活参数。为了实现高效推理和低成本训练，DeepSeek-V3采用了多头潜意识（MLA）和DeepSeekMoE架构，这在DeepSeek-V2中得到了充分验证。此外，DeepSeek-V3 还率先采用了无辅助损失的负载均衡策略，并设定了多标记预测训练目标，以提高性能。我们在14.8万亿个不同的高质量代币上对DeepSeek-V3进行预训练，然后在监督微调和强化学习阶段充分发挥其能力。综合评估显示，DeepSeek-V3的性能优于其他开源模型，并可与领先的闭源模型相媲美。尽管性能卓越，DeepSeek-V3 的全部训练仅需 2.788M H800 GPU 小时。此外，其训练过程也非常稳定。在整个训练过程中，我们没有遇到任何不可恢复的损失峰值，也没有进行任何回滚。

### DeepSeek的低成本训练
根据 DeepSeek-R1 白皮书公布的内容，DeepSeek 使用了类似【知识蒸馏】的技术。

> 知识蒸馏的目标是让一个较小的模型（学生模型）学习一个较大的预训练模型（教师模型）的知识，从而在更少的计算资源和参数量的情况下，仍然能够达到与大模型接近的性能。即：“用一个大模型教会一个小模型”

通俗点讲，假如：ChatGPT 是个苦读 10 年的学霸，那么 DeepSeek 就是个花 1 年学完全部精华的高效学霸。

这也解释了为什么 DeepSeek 能以极低的成本做到接近 ChatGPT 的效果——它本质上是在 用 ChatGPT 教 ChatGPT，再加上国产 AI 生态的独特优化，使其更高效。

DeepSeek-R1 并不是人工智能技术的根本进步。它是训练效率的一个有趣的渐进式进步。然而，重新创建类似 GPT o1 的东西总是比第一次训练它更有效率。

### DeepSeek 与 ChatGPT 的差异
DeepSeek 与 ChatGPT 完全不同，DeepSeek 属于 推理型 大模型，在 深度思考-R1 模式下，可以看到详细的推理过程。

而 ChatGPT 属于 指令型 大模型。它们两者的区别在于：

* 指令型：需要给出具体的指令，大模型会根据你的指令执行。因此就会延伸出【提示词】的概念，详细提示词可以看下我之前写的 这篇关于 Cursor  提示词的文章
* 推理型：具有自己独立思考和分析的能力，它会根据你的【目的】，帮你分析需要怎么做的方式。

ChatGPT 更适合明确指令，DeepSeek 更适合复杂思考。
## AI测试评价尺度
不同的AI大模型，如何评估他们在各个不同场景下的表现，这里是常见的一些AI测试项目。
### MMMU
MMMU：一种新的基准，旨在评估多模态模型在需要大学水平的学科知识和深思熟虑的推理的大规模多学科任务中的表现。MMMU 包括从 **大学考试、测验和教科书** 中精心收集的11.5K 个多模态问题，涵盖六个核心学科： **艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程** 。这些问题涵盖30 个学科和183个子领域，包括 30 种高度异构的图像类型，例如图表、图表、地图、表格、乐谱和化学结构。与现有基准不同，MMMU 专注于使用领域特定知识进行高级感知和推理，挑战模型执行类似于专家面临的任务。我们对 14 个开源 LMM 和专有 GPT-4V（ision）的评估突出了 MMMU 带来的巨大挑战。即使是先进的 GPT-4V 也只能达到 56% 的准确率，这表明有很大的改进空间。我们相信 MMMU 将激励社区构建面向专家级通用人工智能的下一代多模式基础模型。

![blogs_ai_mmmu_test.jpeg](/assets/img/blog/blogs_ai_mmmu_test.jpeg)

MMMU 旨在衡量 LMM 中的三项基本技能：感知、知识和推理。

[【MMMU A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI】](https://mmmu-benchmark.github.io/)


# TODO
* AI测试评价尺度补齐
* 基础概念进一步扩展
* 实操部署deepseek记录
* 诸如llama，claude等热门模型的特点总结