---
layout: post
description: > 
  本文介绍了AI领域的不同范围的分层概念
image: 
  path: /assets/img/blog/blogs_ai_history_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_history_cover.png
    960w:  /assets/img/blog/blogs_ai_history_cover.png
    480w:  /assets/img/blog/blogs_ai_history_cover.png
accent_image: /assets/img/blog/blogs_ai_history_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】AI大模型是如何工作的
手动翻译自：

[How Large Language Models work? From zero to ChatGPT](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f)

得益于 **大型语言模型（Latge Language Models 简称LLM）** 的迅速发展，人工智能领域如今几乎吸引了所有人的目光。

ChatGPT，或许是最著名的LLM，由于**自然语言**本身就是一种非常自然的交互方式，这使得人工智能领域的最新突破变得触手可及，因此其人气迅速飙升。然而，除非您是数据科学家或其他与AI相关的职位，否则对于LLM的工作原理仍然是不太清楚的。在本文中，我将尝试帮您改变这个现状。

要讲清楚很难，毕竟，我们今天拥有的强大的大模型（LLM）是**数十年人工智能研究的结晶**。遗憾的是，大多数涉及AI的文章都属于以下两种情况：**要么技术性很强，需要大量的先验知识；要么内容过于琐碎，最终你并不会比之前获得更多收获。**

本文旨在在这两种方法之间取得平衡。或者换个说法，它旨在带你**从零开始，一路了解大模型（LLM）的培养方式**，以及它们为何如此出色。我们将通过逐步梳理所有相关内容来实现这一点。

本文不会深入探讨所有细节，因此我们将尽可能地**依靠直觉而非数学**，并尽可能地使用视觉工具。正如你将看到的，虽然LLM在细节上确实非常复杂，但**它背后的主要机制非常直观**，仅凭这一点就能让我们走得更远。

本文也能帮助你更好地利用像 ChatGPT 这样的大模型 (LLM)。事实上，我们将学习一些巧妙的技巧，帮助你提高获得有用回复的几率。

但首先，让我们尝试了解大模型在人工智能领域中的地位。
### AI领域分层概念
人工智能领域通常被形象地分为多个层次：

![](/assets/img/blog/blogs_ai_layers.png)

* **人工智能（AI）** 是一个非常宽泛的术语，但一般涉及智能机器。
* **机器学习（ML）** 是人工智能的一个子领域，专门研究数据中的模式识别。可以想象，一旦你识别出一种模式，你就可以将这种模式应用于新的观察。这就是该理念的精髓所在，不过我们稍后会讲到这一点。
* **深度学习** 是 ML 中专注于非结构化数据（包括文本和图像）的领域。它依赖于人工神经网络，这种方法的灵感（粗略地）来源于人脑。
* **大型语言模型（LLM）** 专门处理文本。

### 机器学习
机器学习的目标是**发现数据中的模式**。或者更具体地说，是描述 **输入和结果之间关系** 的模式。最好用一个例子来解释这一点。
#### **音乐类型区分**
假设我们想区分我最喜欢的两种音乐类型：**雷鬼音乐和R&B**。

> 雷鬼音乐是一种拉丁都市音乐，以其活泼的节奏和适合跳舞的韵律而闻名；而 R&B（节奏布鲁斯）则是一种根植于非裔美国音乐传统的音乐类型，其特点是深情的唱腔以及轻快和慢节奏歌曲的融合。

假设我们有 20 首歌曲。我们知道每首歌曲的节奏和能量，这两个指标对于任何歌曲来说都可以轻松测量或计算。此外，我们还给每首歌曲**贴上了不同的风格标签**，例如雷鬼或 R&B。当我们将数据可视化时，我们可以看到，**高能量、高节奏的歌曲主要属于雷鬼，而低节奏、低能量的歌曲主要属于 R&B**，这很合理。

然而，我们希望**避免总是手动标记**音乐类型，因为这既耗时又不可扩展。相反，我们可以**学习歌曲指标（节奏、能量）与音乐类型之间的关系**，然后仅使用现有的指标**进行预测**。

用机器学习的术语来说，我们称之为**分类问题**，因为结果变量（音乐类型）只能归类到一组固定的类别/标签之一，这里指的是雷鬼和 R&B。这与回归问题不同，回归问题的结果是连续值（例如温度或距离）。

现在，我们可以使用标记好的数据集（即使用一组我们已知类型的歌曲）**来“训练”一个机器学习模型（或“分类器”）** 。直观地说，模型的训练就是找到最能区分两个类别的界线。

这有什么用呢？嗯，既然我们知道了这条界线，对于任何一首新歌，我们都可以预测它是雷鬼音乐还是 R&B 歌曲，这取决于这首歌落在界线的哪一边。我们只需要节奏和能量，我们假设这些更容易获得。这比人工为每首歌分配类型要简单得多，也更具可扩展性。

此外，正如你所想象的，距离这条线越远，我们就越能确定预测的正确性。因此，我们通常也可以根据与这条线的距离来判断我们对预测的正确性的信心程度。例如，对于我们新的低能量、低节奏的歌曲，我们可能有 98% 的把握认为这是一首 R&B 歌曲，而只有 2% 的可能性认为它实际上是雷鬼音乐。

![](/assets/img/blog/blogs_ai_music_regreation.png)

但当然，现实往往比这更复杂。

划分类别的**最佳边界可能并非线性**的。换句话说，输入和结果之间的关系可能更加复杂。它**可能呈曲线状，甚至比曲线还复杂很多倍**。

现实里，评价的尺度也更多，也更加复杂，可能有**数十、数百甚至数千个输入变量**，而不是像我们示例中那样只有两个输入。此外，我们通常有两个以上的类别。所有的类别都可能通过一种极其复杂的非线性关系依赖于所有这些输入。

即使以我们的例子来说，我们也知道现实中音乐类型远不止两种，而且除了节奏和能量之外，我们还需要更多其他指标。它们之间的关系可能也没有那么简单。

我主要想让你记住的是：**输入和输出之间的关系越复杂，我们用来学习这种关系的机器学习模型就越复杂、越强大。通常，复杂性会随着输入数量和类别数量的增加而增加。**

除此之外，我们还需要更多数据。你很快就会明白为什么这很重要。
#### **图片内容识别**
现在我们来讨论一个略有不同的问题，但我们会尝试运用之前的思维模型。在新问题中，我们输入一张图片，例如这张包里有一只可爱的猫的图片（因为有猫的例子总是最好的）。

![](/assets/img/blog/blogs_ai_image_judge.png)

至于结果，假设这次我们**有三种可能的标签：老虎、猫和狐狸**。如果你需要一些动机来完成这项任务，假设我们想要保护一群羊，如果看到老虎就发出警报，但如果看到猫或狐狸就不发出警报。

我们已经知道这又是一个分类任务，因为输出只能属于几个固定的类别之一。因此，就像之前一样，我们可以简单地使用一些可用的标记数据（即带有指定类别标签的图像）来训练机器学习模型。

然而，由于**计算机只能处理数字输入，我们究竟如何处理视觉输入呢？**

当然，我们的歌曲指标——能量和节奏——都是数字。幸运的是，图像也只是数字输入，因为它们由像素组成。它们有高度、宽度和三个通道（红、绿、蓝）。因此，理论上，我们可以直接将像素输入机器学习模型（暂时忽略这里的空间元素，我们之前没有处理过）。

然而，现在我们面临两个问题。首先，即使是一张小尺寸、低质量的 224x224 图像，其像素也超过 15 万（224x224x3）。记住，我们之前讨论的是输入变量最多只有几百个（很少超过一千个），但现在突然就至少有 15 万个了。

其次，如果你思考**原始像素和类别标签之间的关系**，就会发现它**极其复杂**，至少从机器学习的角度来看是如此。我们人类的大脑拥有惊人的能力，通常能够轻松区分老虎、狐狸和猫。然而，如果你逐个查看这 15 万像素，你根本无法知道图像中包含什么。而这正是机器学习模型看待它们的方式，因此它需要从头学习这些原始像素与图像标签之间的映射关系，这并非易事。
#### **句子情感感知**
让我们考虑另一种极其复杂的输入输出关系——**句子与其情感之间的关系**。我们所说的情感通常是指句子所传达的情感，可以是积极的，也可以是消极的。

![](/assets/img/blog/blogs_ai_sentence_judge.png)

让我们再次形式化地描述问题设置：这里的**输入是一系列单词**，也就是一个句子，情绪是我们的结果变量。和之前一样，这是一个分类任务，这次的**输出目标有两个可能的标签，即正面或负面**。

与前面讨论的图像示例一样，作为人类，我们自然地理解这种关系，但是我们能否教会机器学习模型做同样的事情呢？

在回答这个问题之前，我们首先要明确，**如何将单词转换为机器学习模型的数字输入**。事实上，这比我们在图像中看到的情况要复杂一到两个层次，因为图像本质上已经是数字了，但单词的情况并非如此。我们在这里就不赘述了，但你需要知道的是，**每个单词都可以转换成一个词向量**。

简而言之，词向量表示单词在特定语境下的语义和句法意义。这些词向量可以在机器学习模型训练过程中获得，也可以通过单独的训练过程获得。**通常，每个单词的词向量包含数万个变量**。

总而言之，**我们可以将一个句子转换成一系列数字输入**，即词向量，它包含语义和句法含义。然后，我们可以将其输入到机器学习模型中。

很好，我们把一个句子成功转换为了数字输入，但是仍然面临和视觉输入相同的挑战。**可以想象，对于一个长句子（或段落，甚至整个文档），由于词向量的规模很大，我们就会面临大量的输入。**

第二个问题是语言与其情感之间的关系，这很复杂——非常复杂。举个例子：

```
“那真是一次伟大的跌倒”
```

这样的句子，它可能会有很多种解读方式（更不用说讽刺了）。

我们需要的是**极其强大的机器学习模型和海量数据**。这正是**深度学习**的用武之地。
### 深度学习
通过了解机器学习的基础知识和使用更强大模型背后的动机，我们已经朝着理解 LLM 迈出了重要一步，现在我们将继续尝试介绍深度学习。

我们讨论过，如果输入和输出之间的关系非常复杂，并且输入或输出变量的数量很大（之前的图像和语言示例就属于这种情况），我们就需要更灵活、更强大的模型。我们之前提到的**线性模型或任何类似的模型都无法解决这类视觉或情感分类任务**。

这就是 **神经网络（Neural networks）** 发挥作用的地方。

神经网络是强大的机器学习模型，能够对任意复杂的关系进行建模。它们是大规模学习此类复杂关系的引擎。

事实上，神经网络大致上**受到了人类大脑的启发**，尽管两者之间的相似之处尚有争议。它们的基本架构相对简单。它们由一系列相互连接的“神经元”层组成，输入信号经过这些神经元层来预测结果变量。你可以将它们想象成 **堆叠在一起的多层线性回归，并在其间添加非线性，这使得神经网络能够模拟高度非线性的关系**。

神经网络通常具有多层深度（因此称为深度学习），这意味着它们可以非常大。例如，**ChatGPT 的背后，是一个由 1760 亿个神经元组成的神经网络，这比人类大脑中大约 1000 亿个神经元还要多。**

因此，从这里开始我们将使用神经网络作为我们的机器学习模型，并考虑到我们还学习了如何处理图像和文本。
### 大语言模型LLM
最后，我们可以开始讨论大型语言模型了，这才是真正有趣的地方。如果你已经读到这里，那么你应该已经具备了理解 LLM 所需的所有知识。

有什么好的开始方式吗？或许可以先解释一下大型语言模型的真正含义。我们已经知道 **“大型”** 的含义，在这里，**它仅仅指的是神经网络中神经元（也称为参数）的数量**。大型语言模型的构成并没有明确的数字，但你可以将超过 10 亿个神经元的模型视为大型模型。

既然已经明确了这一点，**那么“语言模型”又是什么呢？** 接下来我们来讨论一下。并且，我们稍后还会了解 ChatGPT 中的 GPT 代表什么。不过，我们还是一步一步来吧。

让我们将以下想法构建成一个机器学习问题：

> 给定一个词序列，例如输入一个一个句子或段落，输出下一个可能的词是什么？

![](/assets/img/blog/blogs_ai_text_prediction.png)

换句话说，我们只是想学习如何随时预测下一个词。此前，我们已经学习了如何将一个句子转换陈数字输入，并判断其情绪。事实上，这个任务与我们之前看到的情绪分类并无二致。

就像那个例子一样，神经网络的输入是一系列单词，但现在，输出是预测下一个单词。同样，这只是一个分类任务。唯一的区别在于，我们不再只有两个或几个类别，而是拥有与单词数量一样多的类别——假设大约 50,000 个。**这就是语言建模的意义所在——学习预测下一个单词。**

好吧，你可以想象，这比二元情绪分类要复杂几个数量级。但既然我们也了解了神经网络及其强大的威力，对这个问题唯一的回应其实就是“为什么不呢？”

> 快速免责声明：我们在这里简化了很多内容，实际上情况要复杂一些，但这不应该妨碍我们理解主要机制，因此我们简化并省略了一些细节。

我们知道了任务，现在我们**需要数据来训练神经网络**。实际上，为我们的“下一个单词预测”任务创建大量数据并不难：互联网、书籍、研究论文等等都拥有丰富的文本。我们可以轻松地从所有这些文本中创建一个庞大的数据集。**我们甚至不需要标记数据，因为下一个单词本身就是标签，这就是为什么它也被称为自监督学习。**

![](/assets/img/blog/blogs_ai_text_prediction_train.png)

上图展示了这个过程。一个序列可以转换成多个序列进行训练。我们有很多这样的序列。重要的是，我们对许多短序列和长序列（有些序列长达数千个单词）都进行了同样的操作，以便在每个上下文中都能学习到下一个单词应该是什么。

总而言之，**我们在这里所做的就是训练一个神经网络（LLM）来预测给定单词序列中的下一个单词**，无论该序列是长是短，是德语、英语还是其他任何语言，无论是推文、数学公式、诗歌还是代码片段。所有这些序列都可以在训练数据中找到。

如果我们拥有足够大的神经网络和足够的数据，LLM 就能非常擅长预测下一个单词。它会完美吗？当然不会，因为一个序列后面通常有多个单词。但它会擅长选择一个在句法和语义上都合适的单词。

![](/assets/img/blog/blogs_ai_text_prediction_result.png)

现在我们的神经网络可以根据一段文本来预测下一个单词了，我们可以将扩展后的文本再次反馈给LLM，预测另一个单词，以此类推。

换句话说，使用我们训练好的LLM，我们现在可以生成文本，而不仅仅是一个单词。这就是为什么LLM是我们所说的生成式人工智能的一个例子。**我们刚刚教会了LLM说话，也就是说，一次一个单词。**

我认为还有一个细节需要理解。**我们不一定总是要预测出最准确的单词**。我们可以让模型在给定时间内，从最可能一些单词中选取出5个可能的结果。因此，我们或许**可以从大模型（LLM）中获得更多创造力**。有些大模型实际上允许你选择输出的确定性或创造性程度（temperature）。这也是为什么在使用这种取样策略的ChatGPT中，你重新生成响应时通常不会得到相同的答案。

说到 ChatGPT，你现在可能会问，为什么它不叫 ChatLLM。事实证明，语言建模（Language Models）并非故事的终点——事实上，它只是一个开始。那么，ChatGPT 中的 GPT 到底代表什么呢？
### GPT
![](/assets/img/blog/blogs_ai_gpt_meaning.png)

实际上，我们刚刚了解了 **`G`** 的含义，即 **“generative(生成式)”** ——这意味着它是以语言生成为目的进行训练的，这一点我们之前已经讨论过了。那么 P 和 T 呢？

这里我们先简单聊一下 **`T`** ，它代表 `Transformer`（转换器）。这个可不是说的不是电影里的那个（变形金刚就叫Transformer），而是指目前所使用的**一种神经网络架构**。

Transformer架构的的主要优势是什么呢？ **那么Transformer架构之所以如此高效，是因为它能够随时将注意力集中在输入序列中最相关的部分。** 你可能会说这与人类的工作方式类似。我们也需要将注意力集中在与任务最相关的部分，而忽略其他部分。

现在来谈谈 **`P`** ，它代表 **“pre-training(预训练)”** 。接下来，我们将讨论**为什么我们突然开始谈论“预训练”，而不是仅仅谈论“训练”。**

原因是，像 ChatGPT 这样的大型语言模型实际上是**分阶段训练**的。

![](/assets/img/blog/blogs_ai_llm_training_phases.png)

> （1）预训练，（2）指令微调，（3）人工反馈强化（RLHF）。

#### **预训练**
第一阶段是预训练，也就是我们刚才经历的阶段。这个阶段需要大量的数据来学习预测下一个单词。在这个阶段，模型不仅要学习掌握语言的语法和句法，还要获得大量关于世界的知识，甚至一些我们稍后会谈到的新兴能力。

但现在我有几个问题想问你：首先，这种预训练可能存在什么问题？当然存在一些问题，但我在这里想指出的一个问题是LLM到底学到了什么。

也就是说，它主要**学会了如何滔滔不绝地谈论某个话题**。它甚至可能做得非常好，**但它无法很好地响应你通常想给人工智能的输入**，比如一个问题或一个指令。问题在于，这个模型还没有学会如何成为一个助手，因此它的行为也并非助手。

例如，如果你问一个预训练的大模型

```
“你的名字是什么？”
```

它可能会回答

```
“你的姓氏是什么？”
```

仅仅是因为它在预训练过程中见过这种类型的数据，例如在许多空表格中。它只是试图完成输入序列。

它在遵循指令方面表现不佳，仅仅是**因为这种一问一答式的语言结构在训练数据中并不常见。** 或许 Quora 或 StackOverflow 是最接近这种结构的代表。

在这个阶段，我们认为 LLM 与人类意图不一致。一致性是 LLM 的一个重要课题，我们将学习如何在很大程度上解决这个问题，因为事实证明，这些预先训练好的 LLM 实际上非常易于操控。所以，即使它们最初对指令的反应不太好，它们也可以被训练来做到这一点。
#### **指令微调和 RLHF**
这就是指令调整的用武之地。我们基于预先训练的 LLM 当前的能力，再次让他做同样的输入和输出训练，即学习一次预测一个单词。**不一样的是，我们只使用高质量的指令和响应对作为我们的训练数据。**

这样，模型就不再只是学习完成文本，而是学习成为一个有用的助手，能够遵循指令并以符合用户意图的方式做出响应。该指令数据集的大小通常比预训练集小得多。**这是因为高质量的指令-响应对的创建成本要高得多，因为它们通常来自人工。**这与我们在预训练中使用的廉价自监督标签截然不同。这就是为什么这个阶段也称为**监督指令微调**。

有些大模型（比如 ChatGPT）还会经历第三个阶段，即**人类反馈强化学习（RLHF）**。我们在此不再赘述，但**其目的与指令微调类似**。强化学习高频 (RLHF) 也有助于对齐，并确保大模型 (LLM) 的输出能够反映人类的价值观和偏好。一些早期研究表明，这一阶段对于达到或超越人类水平的表现至关重要。事实上，强化学习和语言建模领域的结合已被证明前景广阔，并可能带来比我们现有的大模型 (LLM) 更显著的改进。

### 三个有趣的问题
现在让我们通过三个问题测试一下我们的理解。

**第一个问题，为什么 LLM 可以对较长的文本进行摘要？**

> 如果您还不知道，它确实做得很棒。只需粘贴一份文档并让它进行总结即可。

要理解其中的原因，我们需要思考一下训练数据。事实上，**人们经常在互联网、研究论文、书籍等等中进行总结**。因此，**用这些数据训练**的大模型（LLM）也学会了如何做到这一点。它学会了关注要点，并将其压缩成简短的文本。

请注意，在生成摘要这个用例中，**待总结的全文不是全部，只是LLM输入序列的一部分**。这个输入的结构类似于一篇研究论文，它前面是正文，而后面是全文总结。

因此，该技能可能在预训练阶段就已经习得，尽管指令微调肯定有助于进一步提升该技能。我们可以假设此阶段也包含了一些摘要示例。

**第二，为什么LLM可以回答常识性的问题？**

如上所述，能够充当助手并做出适当响应的能力归功于指令微调和 RLHF。但所有（或大部分）回答问题的知识本身在预训练期间就已经获得了。

当然，这又引出了另一个大问题：**如果大模型（LLM）不知道答案怎么办？不幸的是，在这种情况下，他们可能会编造一个答案。**要理解其中的原因，我们需要重新思考训练数据和训练目标。

> 问题：LLM 接受的训练是生成人类文字，而不是真实文字。

> 解决思路：我们需要让它们 "立足 "于现实，这样它们就不会胡编乱造。

> 措施：将相关知识纳入LLM的上下文中。

LLM 可能会出现**幻觉**，但可以通过提供额外的背景信息来缓解。“幻觉”一词，它指的是大模型 (LLM) 编造不该编造的事实的现象。

为什么会这样？**因为 LLM 学习的只是生成文本，而不是生成符合事实的真实文本。** 它的训练过程中，并没有为模型提供任何关于训练数据真实性或可靠性的指标。然而，这也不是主要问题，问题在于互联网和书籍上的文本**通常听起来都很有说服力，即使它是错误的**，所以 LLM 当然也会学习到这种做法。这样一来，从LLM 的角度出发，几乎看不到任何关于内容不确定性的迹象。

话虽如此，这是一个活跃的研究领域，我们可以预期，随着时间的推移，LLM 的幻觉倾向会降低。例如，在教学调整过程中，我们可以尝试教会 LLM 在一定程度上避免产生幻觉，但能否完全解决这个问题，只有时间才能证明。

你可能会惊讶，我们现在居然可以一起尝试解决这个问题。我们拥有所需的知识，能够找到一个至少能在一定程度上提供帮助、并且目前已被广泛应用的解决方案。

![](/assets/img/blog/blogs_ai_bing_chat_example.png)

Bing 聊天是基于搜索的 LLM 工作流程的一个示例。

假设你问大模型（LLM）这个问题： **哥伦比亚现任总统是谁？** 大模型很可能会答错。这可能有两个原因：
* 第一个是我们已经提到的：LLM 可能只是出现幻觉，并简单地用错误的名字甚至是假名来回应。
* 第二个我只想顺便提一下：大模型（LLM）的**训练数据仅限于某个截止日期**，最早可能是去年的数据。正因如此，LLM 甚至无法确切地知道现任总统是谁，因为**自数据创建以来，情况可能已经发生了变化**。

那么，我们该如何解决这两个问题呢？答案在于为模型**提供一些相关的上下文**。其原理是，**在 LLM 输入序列中的内容比它在预训练过程中获得的任何隐性知识都更容易被模型处理，容易检索，且更稳定。** 就像让他从一个已知的数组中找出最大的一个数字一样。

假设我们将维基百科上关于哥伦比亚政治史的文章作为大模型课程的背景信息。在这种情况下，它更有可能正确回答问题，因为它可以直接从上下文中提取姓名（前提是上下文是最新的，并且包含现任总统）。

在上图中，你可以看到带有附加上下文的 LLM 典型提示是什么样子的。（顺便说一下，prompt（提示）只是我们给 LLM 的指令的另一种说法，即构成输入序列的指令。）

这个过程被称为**将 LLM 置于上下文中**，或者如果你愿意的话，置于现实世界中，而不是让它自由生成。

这正是 Bing Chat 和其他基于搜索的 LLM 的工作原理。它们首先使用搜索引擎从网络中提取相关上下文，然后将所有信息连同用户的初始问题一起传递给 LLM。上图直观地展示了这个过程是如何实现的。

### 回到AI魔法
现在你基本上了解了最先进的 LLM 的主要机制（无论如何，截至 2023 年下半年）。

**你可能会想“这其实没什么神奇的”，这只是一次预测一个单词而已。毕竟，这只是纯粹的统计数据。事实真的如此吗？**

让我们回顾一下。这一切的神奇之处在于**它的效果如此出色**。

事实上，每个人，**甚至是 OpenAI 的研究人员，都对这种语言建模的进展感到惊讶**。过去几年，其发展如此迅猛的关键因素之一就是**神经网络和数据集的大规模扩展**，这推动了 LLM 表现的提升。例如，据报道，GPT-4 的参数总数超过一万亿个，它能够以前 10% 的成绩通过律师资格考试或 AP 生物学考试。

令人惊讶的是，这些大模型甚至表现出某些新兴能力，即解决任务和完成一些他们没有被训练过的事情。

在本文的最后一部分，我们将讨论一些新兴的能力，并向您展示一些如何使用它们来解决问题的技巧。

### 零样本和少量样本学习
![](/assets/img/blog/blogs_ai_zero_shot_prompt.png)

LLM 可以以零样本方式解决全新的任务。正如其名称所示，LLM 正在兴起一种普遍存在的能力：它可以执行训练中从未遇到过的全新任务，这被称为零样本（zero-shot）。它所需要的只是一些关于如何解决任务的指令。

为了用一个愚蠢的例子来说明这种能力，你可以要求大模型将一个句子从德语翻译成英语，同时句子中的每个单子都要以字母 **`f`** 开头。

例如，我们测试的其中一个大模型将

```
“Die Katze schläft gerne in der Box”（德语，字面意思是“猫喜欢睡在盒子里”）
```

翻译成了

```
“Feline friend finds fluffy fortress”
```

我认为这是一个非常酷的翻译。


对于更复杂的任务，您可能很快就会意识到零样本提示通常需要非常详细的说明，即使这样，性能也往往远非完美。

大模型就像人类一样，可以从示例或演示中提取有用的信息。**如果有人要求你执行一项全新的任务，你可能会要求提供一些示例或演示来说明如何执行该任务。大模型（LLM）也一样。**

![](/assets/img/blog/blogs_ai_frew_shot_prompt.png)

举个例子，假设你想要一个模型将不同的货币金额转换为通用格式。你可以详细描述你的目标，或者只给出一个简短的说明和一些示例演示。上图展示了一个示例任务。

使用这个提示，模型应该在最后一个例子上表现良好，即“牛排：24.99 美元”，并用 24.99 美元回答。

注意我们是如何省略掉上一个例子的答案的。记住，LLM 本质上仍然是一个文本补全器，**所以想要输出中保持一致的结构，你应该几乎强制模型只给出你想要的结果，就像我们在上面的例子中所做的那样。**

总而言之，如果LLM在零样本学习中遇到困难，一个通用的建议是提供一些示例。你会发现，这通常有助于LLM理解任务，从而使其输出的效果更好、更可靠。

### 长思维链
思路链为大模型 (LLM) 提供了工作记忆，这可以大大提高他们的表现，特别是在更复杂的任务上。这也让人联想到人类智能，当任务更加复杂，需要多步推理才能解决时，推理思路链尤其有用。

![](/assets/img/blog/blogs_ai_chain_of_thought.png)

假设我问你

```
“莱昂内尔·梅西出生前一年谁赢得了世界杯？”
```

你会怎么做？你可能会一步一步地解决这个问题，写下所有需要的中间解，直到得出正确答案。先找到梅西的生日，再找到对应前一年的这届世界杯冠军是哪个国家。而这正是大模型（LLM）也可以做到的。

**研究发现，只要简单地告诉大模型“一步一步思考”，就能在许多任务中显著提高其表现。**

为什么这样做有效？我们已经知道了回答这个问题所需的一切。**问题在于，这种不寻常的复合知识可能并不直接存在于大模型（LLM）的记忆中。** 然而，所有单独的事实，比如梅西的生日，以及历届世界杯的冠军，都可能存在。

**让 LLM 逐渐形成最终答案是有帮助的**，因为它给了模型时间去大声思考——可以说是工作记忆——并在给出最终答案之前解决更简单的子问题。

这里的关键在于记住，**待生成单词左侧的所有内容都是模型可以依赖的上下文**。因此，如上图所示，当模型说出答案 **“阿根廷”** 时，**梅西的生日**和**查询到的世界杯年份** 的数据已经存在于 LLM 的工作记忆中，这使得模型更容易正确回答。

## 结论
在结束之前，我想回答我在文章前面提出的一个问题。LLM 真的只是预测下一个单词，还是它还有其他功能？一些研究人员支持后者，他们认为，LLM 在任何情况下都如此擅长预测下一个单词，实际上一定是**在内部获得了对世界的压缩理解**。而不是像其他人所说的那样，模型只是学会了记忆和复制在训练中看到的模式，而没有真正理解语言、世界或其他任何东西。

目前来看，这两种观点或许并没有绝对的对错；或许只是看待同一事物的不同视角。显然，这些大模型已被证明非常有用，展现出令人印象深刻的知识和推理能力，甚至可能展现出一些通用智能体的火花。但它们是否或在多大程度上与人类智能相似仍有待确定，语言建模还能在多大程度上提升现有水平也同样有待确定。

---

我希望本文能帮助你了解大模型（LLM）及其当前的热潮，从而对人工智能的潜力和风险形成自己的见解。如何利用人工智能造福世界，不仅取决于人工智能研究人员和数据科学家；每个人都应该拥有发言权。正因如此，我才想写一篇不需要太多背景知识的文章。

如果您读完了这篇文章，我想您就大致了解一些最先进的 LLM 是如何运作的（截至 2023 年秋季），至少在大体上是这样。
