---
layout: post
description: > 
  本文介绍了LLM一般的开发流程和运行环境相关内容
image: 
  path: /assets/img/blog/blogs_ai_arithmetic_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_arithmetic_cover.png
    960w:  /assets/img/blog/blogs_ai_arithmetic_cover.png
    480w:  /assets/img/blog/blogs_ai_arithmetic_cover.png
accent_image: /assets/img/blog/blogs_ai_arithmetic_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】LLM开发流程和运行环境简介
## LLM一般的开发流程
### 1. 训练数据 (Training Data)
LLM 的能力来源于海量的训练数据。这些数据包括：

* **文本数据：** 网页、书籍、文章、代码、对话等等。这是最主要的数据来源，让模型学会语言的语法、语义、事实知识以及各种表达方式。
* **多模态数据（对于多模态 LLM）：** 图像、音频、视频等。这让模型能够理解和生成不同形式的信息，而不仅仅是文本。

这些数据通过复杂的预处理，被转换成模型可以理解的**数字表示**（比如词向量或 token embeddings）。

### 2. 模型架构 (Model Architecture)
目前，绝大多数大型 LLM 都基于**Transformer (注意力机制)** 架构。Transformer 是一种特殊的神经网络，它有两个主要部分：

* **编码器 (Encoder)：** 负责理解输入文本的上下文信息。
* **解码器 (Decoder)：** 负责根据编码器的理解生成输出文本。

Transformer 的关键是**自注意力机制 (Self-Attention)**。它允许模型在处理一个词时，同时考虑输入序列中所有其他词的重要性，从而捕捉词与词之间的长距离依赖关系。例如，在理解 "The bank of the river" 中的 "bank" 时，模型会更多地关注 "river"，而不是 "money"。

### 3. 预训练 (Pre-training)
预训练是 LLM 学习通用语言能力的关键阶段。在这个阶段，模型会执行一些任务，而不需要人工标注数据：

* **遮蔽语言模型 (Masked Language Modeling, MLM)：** 模型被要求预测一个被遮蔽（随机隐藏）的词。例如，“我爱吃 [MASK] 果。” 模型需要预测出“苹”或“水”等。这让模型学习词汇的上下文关系和语义。
* **下一句预测 (Next Sentence Prediction, NSP)：** 模型被要求判断两个句子是否是连续的。这有助于模型理解句子之间的逻辑关系和篇章结构。

通过在海量数据上进行这些任务，模型学习到了语言的内在规律、语法、常识以及大量的世界知识。

### 4. 微调 (Fine-tuning)
预训练完成的模型已经具备了强大的通用能力，但可能还不擅长执行特定任务，例如问答、摘要、翻译等。这时就需要进行**微调**。

微调是在相对较小的、针对特定任务的**标注数据集**上进行的。模型会根据任务的需求进行调整，从而更好地执行特定功能。例如，如果你想让模型更好地回答问题，你会用大量的问答对数据来微调它。

### 5. 推理 (Inference)
当用户向 LLM 提问时，这个过程称为**推理**。

1.  **输入处理：** 用户的输入（提示词/prompt）会被转换成模型可以理解的数字形式。
2.  **逐词生成：** 模型不会一次性生成所有答案，而是**逐个词**地生成。
    * 模型会根据当前的输入和已经生成的部分答案，预测下一个最有可能的词。
    * 这个词被添加到输出序列中。
    * 然后，模型会再次预测下一个词，直到生成一个完整的、有意义的回答。
3.  **概率分布：** 模型在预测每一个词时，实际上是计算了一个**概率分布**，表示每个词出现的可能性。模型会选择概率最高的那个词作为输出，或者根据一些策略（如采样）选择一个相对高概率的词，以增加多样性。

所以，LLM 的运作可以概括为：**在海量数据上通过Transformer架构进行预训练，学习语言和世界知识，然后通过微调适应特定任务，最终在用户输入时进行逐词预测生成输出。**
## 运行环境
### ollama

### huggingface

### llama.cpp
