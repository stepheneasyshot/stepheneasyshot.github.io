---
layout: post
description: > 
  本文介绍了LLM一般的开发流程和运行环境相关内容
image: 
  path: /assets/img/blog/blogs_ai_arithmetic_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_arithmetic_cover.png
    960w:  /assets/img/blog/blogs_ai_arithmetic_cover.png
    480w:  /assets/img/blog/blogs_ai_arithmetic_cover.png
accent_image: /assets/img/blog/blogs_ai_arithmetic_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】LLM开发流程和运行环境简介
接上文，了解了LLM的发展历程，从简单的线性回归模型到神经网络，再到生成式预训练转换器。

[【AI】AI大模型是如何工作的](./2025-7-19-【AI】AI大模型是如何工作的.md)

现在更多从实际部署运行的操作上，介绍下LLM在各个平台上的运行。
## LLM一般的开发流程
根据 OpenAI 联合创始人 `Andrej Karpathy` 在微软 Build 2023 大会上所公开的信息，OpenAI 所使用的大规模语言模型构建流程如下图所示。主要包含四个阶段：**预训练、有监督微调、奖励建模、强化学习**。这四个阶段都需要不同规模数据集合以及不同类型的算法，会产出不同类型的模型，同时所需要的资源也有非常大的差别。

![](/assets/img/blog/blogs_ai_training_phases_cn.png)

### **预训练（Pretraining）阶段** 
这个阶段需要利用海量的训练数据，包括互联网网页、维基百科、书籍、GitHub、论文、问答网站等，构建包含数千亿甚至数万亿单词的具有多样性的内容。利用**由数千块高性能 GPU 和高速网络组成超级计算机**，花费数十天完成深度神经网络参数训练，构建基础语言模型（Base Model）。基础大模型构建了长文本的建模能力，**使得模型具有语言生成能力**，根据输入的提示词（Prompt），模型可以生成文本补全句子。也有部分研究人员认为，语言模型建模过程中也隐含的构建了包括事实性知识（Factual Knowledge）和常识知识（Commonsense）在内的世界知识（World Knowledge）。根据介绍，GPT-3 完成一次训练的总计算量是 3640PFlops，按照NVIDIA A100 80G 和平均利用率达到 50% 计算，需要花费近一个月时间使用 1000 块 GPU 完成。
### **有监督微调（Supervised Finetuning）** 
有监督微调也称为指令微调（Instruction Tuning），利用少量高质量数据集合，包含用户输入的提示词（Prompt）和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。

例如：

> 提示词（Prompt）：复旦大学有几个校区？

> 理想输出：复旦大学现有 4 个校区，分别是邯郸校区、新江湾校区、枫林校区和张江校区。其中邯郸校区是复旦大学的主校区，邯郸校区与新江湾校区都位于杨浦区，枫林校区位于徐汇区，张江校区位于浦东新区。

利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础语言模型基础上再进行训练，从而得到有监督微调模型（SFT 模型）。经过训练的 SFT 模型具备了初步的指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备了一定的对未知任务的泛化能力。

由于有监督微调阶段的所需的训练语料数量较少，SFT 模型的训练过程并不需要消耗非常大量的计算。根据模型的大小和训练数据量，通常需要数十块 GPU，花费数天时间完成训练。

SFT 模型具备了初步的任务完成能力，可以开放给用户使用，很多类 ChatGPT的模型都属于该类型，包括：`Alpaca[38]`、`Vicuna[39]`、`MOSS` 、 `ChatGLM-6B` 等。

很多这类模型效果也非常好，甚至在一些评测中达到了 ChatGPT 的 90% 的效果。当前的一些研究表明有监督微调阶段数据选择对 SFT 模型效果有非常大的影响，因此如何构造少量并且高质量的训练数据是本阶段有监督微调阶段的研究重点。
### 奖励建模（Reward Modeling）
奖励建模阶段目标是**构建一个文本质量对比模型**，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序。

奖励模型（RM 模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM 模型与基础语言模型和 SFT 模型不同，RM 模型本身并不能单独提供给用户使用。

奖励模型的训练通常和 SFT 模型一样，使用数十块 GPU，通过几天时间完成训练。由于 RM 模型的准确率对于强化学习阶段的效果有着至关重要的影响，因此对于该模型的训练通常需要大规模的训练数据。

`Andrej Karpathy` 在报告中指出，该部分需要百万量级的对比数据标注，而且其中很多标注需要花费非常长的时间才能完成。

![](/assets/img/blog/blogs_ai_training_ranking_outputs.png)

如图，InstructGPT 系统中奖励模型训练样本标注示例。可以看到，示例中文本表达都较为流畅，标注其质量排序需要制定非常详细的规范，标注人员也需要非常认真的对标规范内容进行标注，需要消耗大量的人力，同时如何保持众包标注人员之间的一致性，也是奖励建模阶段需要解决的难点问题之一。

此外**奖励模型的泛化能力边界**也是在本阶段需要重点研究的另一个问题。如果 RM 模型的目标是针对所有提示词系统所生成输出都能够高质量的进行判断，该问题所面临的难度在某种程度上与文本生成等价，因此如何限定 RM 模型应用的泛化边界也是本阶段难点问题。
### 强化学习（Reinforcement Learning）
强化学习阶段**根据数十万用户给出的提示词**，利用在前一阶段训练的 RM 模型，给出 SFT 模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。

该阶段所使用的提示词数量与有监督微调阶段类似，数量在十万量级，并且不需要人工提前给出该提示词所对应的理想回复。使用强化学习，在 SFT 模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段所需要的计算量相较预训练阶段也少很多，通常也仅需要数十块 GPU，经过数天时间的即可完成训练。

强化学习和有监督微调的对比，在模型参数量相同的情况下，强化学习可以得到相较于有监督微调好得多的效果。关于为什么强化学习相比有监督微调可以得到更好结果的问题，截止到 2023 年 9 月也还没有完整和得到普遍共识的解释。

此外，Andrej Karpathy 也指出强化学习也并不是没有问题的，它会**使得基础模型的熵降低**，从而减少了模型输出的多样性。在经过强化学习方法训练完成后的 RL 模型，就是最终提供给用户使用具有理解用户指令和上下文的类 ChatGPT 系统。

由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加 RM 模型的准确率问题，使得在大规模语言模型如何能够有效应用强化学习非常困难。

### 训练数据 (Training Data)
LLM 的能力来源于海量的训练数据。这些数据包括：
* **文本数据：** 网页、书籍、文章、代码、对话等等。这是最主要的数据来源，让模型学会语言的语法、语义、事实知识以及各种表达方式。
* **多模态数据（对于多模态 LLM）：** 图像、音频、视频等。这让模型能够理解和生成不同形式的信息，而不仅仅是文本。

这些数据通过复杂的预处理，被转换成模型可以理解的**数字表示**（比如词向量或 token embeddings）。
### 模型架构 (Model Architecture)
目前，绝大多数大型 LLM 都基于**Transformer (注意力机制)** 架构。Transformer 是一种特殊的神经网络，它有两个主要部分：
* **编码器 (Encoder)：** 负责理解输入文本的上下文信息。
* **解码器 (Decoder)：** 负责根据编码器的理解生成输出文本。

Transformer 的关键是**自注意力机制 (Self-Attention)**。它允许模型在处理一个词时，同时考虑输入序列中所有其他词的重要性，从而捕捉词与词之间的长距离依赖关系。
## 运行环境
### ollama

### huggingface

### llama.cpp


## 参考资料
[1] 《大规模语言模型从理论到实践》 张奇 桂韬 郑锐 ⻩萱菁 著