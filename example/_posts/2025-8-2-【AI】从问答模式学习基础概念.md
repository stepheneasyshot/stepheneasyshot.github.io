---
layout: post
description: > 
  本文介绍了若干种常见的AI领域的算法及其应用场景
image: 
  path: /assets/img/blog/blogs_ai_common_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_common_cover.png
    960w:  /assets/img/blog/blogs_ai_common_cover.png
    480w:  /assets/img/blog/blogs_ai_common_cover.png
accent_image: /assets/img/blog/blogs_ai_common_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】从问答模式学习基础概念
## 一、 AI与机器学习基础概念
###  什么是人工智能（AI）、机器学习（ML）和深度学习（DL）？它们之间有什么关系？
AI是一个广泛的概念，指让机器模仿人类智能。ML是实现AI的一种方法，让机器从数据中学习。DL是ML的一个子集，使用深度神经网络（多层）进行学习。
### 机器学习主要有哪几类？请分别举例说明。
* **监督学习 (Supervised Learning):** 使用“有标签”的数据。例如：房价预测（回归）、垃圾邮件分类（分类）。
* **无监督学习 (Unsupervised Learning):** 使用“无标签”的数据。例如：用户分群（聚类）、降维（如PCA）。
* **强化学习 (Reinforcement Learning):** 通过“奖励”和“惩罚”学习。例如：AlphaGo下棋、自动驾驶策略。

### 什么是“过拟合”？什么是“欠拟合”？
过拟合指模型在训练集上表现很好，但在测试集上表现很差（泛化能力弱）。欠拟合指模型在训练集上表现就不好。
### 有哪些方法可以防止或缓解过拟合？
* 获取更多数据。
* 数据增强 (Data Augmentation)。
* 使用更简单的模型。
* **正则化 (Regularization)**（L1, L2）。
* **Dropout**（在神经网络中常用）。
* 早停 (Early Stopping)。

### 解释一下“偏差” (Bias) 和“方差” (Variance) 以及它们之间的权衡。
高偏差通常导致欠拟合（模型太简单），高方差通常导致过拟合（模型太复杂）。理想的模型是低偏差且低方差，但两者往往是此消彼长的。
## 二、 机器学习算法与模型
### 请解释一下线性回归的原理。
找到一条直线（或超平面）来最好地拟合数据点。关键是定义 **损失函数**（Loss Function），通常使用均方误差 (MSE)，然后通过优化算法（如梯度下降）最小化损失函数来求解参数。
### 逻辑回归和线性回归有什么区别？
逻辑回归是用来做**分类**任务的（尤其是二分类），线性回归是用来做**回归**（预测连续值）任务的。逻辑回归在线性回归的基础上，套用了一个Sigmoid函数，将输出值映射到0到1之间，表示概率。
### 什么是决策树？它的优缺点是什么？
优点是可解释性强、能处理非线性数据。缺点是容易过拟合。
### 什么是随机森林？它如何改进决策树？
随机森林是 **集成学习 (Ensemble Learning)** 中 Bagging 的一种。它通过构建多棵决策树，并让它们投票（分类）或取平均值（回归）来减少单棵决策树过拟合的风险。
### 简单介绍一下K近邻 (KNN) 算法。
“近朱者赤，近墨者黑”。它是一种懒惰学习（Lazy Learning），预测时才计算。对于新数据点，找到训练集中离它最近的K个点，然后根据这K个点的标签来决定新数据点的标签。
### 简单介绍一下K-Means聚类算法。
无监督学习。目标是将数据分成K个簇（Cluster）。步骤：1. 随机选K个中心点。2. 将每个数据点分配给最近的中心点。3. 重新计算每个簇的中心点（均值）。4. 重复2和3直到中心点不再变化。
## 三、 数据预处理与特征工程
### 为什么需要对数据进行“归一化” (Normalization) 或“标准化” (Standardization)？
解决不同特征（如“年龄”和“收入”）量纲（尺度）不同的问题。这可以加速梯度下降的收敛速度，并提高一些算法（如KNN、SVM）的准确性。
* **归一化 (Min-Max Scaling):** 将数据缩放到 [0, 1] 区间。
* **标准化 (Z-score Standardization):** 将数据转换成均值为0，标准差为1的正态分布。

### 何处理缺失值？
* 删除（删除行或列）。
* 填充（使用均值、中位数、众数填充）。
* 使用模型预测填充。

### 如何处理类别型特征？
* **标签编码 (Label Encoding):** 将 "红", "绿", "蓝" 编码为 0, 1, 2。适用于有序类别。
* **独热编码 (One-Hot Encoding):** 将 "红", "绿", "蓝" 编码为 `[1,0,0], [0,1,0], [0,0,1]`。适用于无序类别。

## 四、 模型评估
### 在分类任务中，有哪些常用的评估指标？
* **准确率 (Accuracy):** 预测正确的样本数 / 总样本数。
* **精确率 (Precision):** $TP / (TP + FP)$。在所有预测为“正”的样本中，有多少是真的“正”。
* **召回率 (Recall):** $TP / (TP + FN)$。在所有真的“正”样本中，有多少被成功预测出来了。
* **F1-Score:** 精确率和召回率的调和平均数， $2 \cdot (Precision \cdot Recall) / (Precision + Recall)$。

### 什么时候更关注精确率，什么时候更关注召回率？
* **关注精确率（宁可少抓，不可错抓）：** 垃圾邮件检测。你不想把重要邮件误判为垃圾邮件 (FP要低)。
* **关注召回率（宁可错抓，不可漏抓）：** 癌症诊断。你不想把癌症患者漏诊 (FN要低)。

### 什么是混淆矩阵？
一个表格，用于可视化分类模型的性能。包含四个值：真正 (TP), 假正 (FP), 真负 (TN), 假负 (FN)。

### 什么是交叉验证？为什么需要它？
为了更可靠地评估模型的泛化能力，防止数据划分的偶然性。最常用的是K折交叉验证（K-Fold Cross-Validation），将数据分成K份，轮流用其中K-1份训练，1份测试，最后取K次评估结果的平均值。

## 五、 深度学习基础（初级）
### 什么是神经网络 (Neural Network)？它由哪些基本部分组成？
输入层、隐藏层、输出层。神经元（节点）、权重 (Weights)、偏置 (Biases)、激活函数 (Activation Function)。

### 为什么需要激活函数？常用的激活函数有哪些？
为了给模型引入**非线性**。如果不用激活函数，多层神经网络也只相当于一个线性回归。
* **Sigmoid:** $1 / (1 + e^{-x})$，输出在 (0, 1) 之间。
* **ReLU (Rectified Linear Unit):** $\max(0, x)$。目前最常用，计算快，能缓解梯度消失。
* **Tanh:** 输出在 (-1, 1) 之间。

### 什么是梯度下降 (Gradient Descent)？
一种优化算法，用来最小化损失函数。通过计算损失函数对参数的**梯度**（导数），然后沿着梯度的**反方向**小步更新参数。

### 解释一下 Epoch, Batch, Iteration 的区别。
* **Epoch (时代):** 所有训练数据完整地训练一遍。
* **Batch (批次):** 将训练数据分成N个批次。
* **Iteration (迭代):** 训练一个Batch所需的步骤。 1个Epoch = N个Iteration (N = 总样本数 / Batch Size)。

### 简单介绍一下CNN（卷积神经网络）主要用在什么地方？
主要用于**图像处理**。它的核心是**卷积核 (Kernel)**，通过卷积操作提取图像的局部特征（如边缘、纹理）。

### 简单介绍一下RNN（循环神经网络）主要用在什么地方？
主要用于**序列数据**（如文本、时间序列）。它的特点是神经元的输出可以作为下一次的输入，使其具有“记忆”功能，能处理上下文依赖关系。

## 六、 LLM细分领域
### 什么是词嵌入 (Word Embedding)？
将词语（离散符号）映射到低维连续向量空间的过程。关键思想是让语义相近的词在向量空间中的距离也相近。
### 你知道哪些词嵌入技术？它们有什么区别？
* **Word2Vec (CBOW/Skip-gram):** 静态词向量。通过上下文预测中心词 (CBOW) 或通过中心词预测上下文 (Skip-gram)。
* **GloVe:** 静态词向量。利用全局共现矩阵。
* **BERT/ELMo:** 动态/上下文相关的词向量。同一个词在不同句子中的向量是不同的。

### 什么是 "Bank" 问题（一词多义）？Word2Vec 如何处理？BERT 如何处理？
Word2Vec 无法处理。"bank"（银行）和 "bank"（河岸）的词向量是相同的。BERT 可以处理，因为它会根据上下文（"I went to the bank to deposit money" vs "I sat on the river bank"）生成不同的向量。

### 什么是 Tokenization (分词)？LLM 中常用哪种方式？
BPE (Byte Pair Encoding) 或 SentencePiece。
* **为什么不用词作为单位？** 词表会过大（数百万），且无法处理未登录词 (OOV, Out-of-Vocabulary)。
* **为什么不用字符作为单位？** 序列会过长，单个字符的语义信息太少。
* **BPE 的优势：** 介于词和字符之间，通过合并高频字节对来构建词表，既能控制词表大小，又能很好地处理生僻词和拼写错误。

## 二、 核心架构：Transformer

这是LLM的基石，**必考领域**。

### Transformer 是为了解决什么问题而提出的？
解决 RNN/LSTM 的两个主要问题：
1. **无法并行计算**（必须按顺序处理）；
2. **长距离依赖捕获困难**（信息在长序列中传递会丢失）。

### 请解释一下自注意力机制的工作原理。
这是核心中的核心。简单说，就是让模型在处理一个词时，能“关注”到句子中所有其他词，并计算一个“注意力权重”分布，来决定哪些词对当前词的理解更重要。
### 什么是 $Q$ (Query), $K$ (Key), $V$ (Value)？它们是如何计算的？
$Q, K, V$ 都是从同一个输入向量（词嵌入）通过不同的线性变换（乘以权重矩阵 $W_q, W_k, W_v$）得到的。

计算过程：
1.  用 $Q$ 和 $K$ 计算相似度（通常是点积：$Q \cdot K^T$）。
2.  用 $\text{Softmax}$ 将相似度得分归一化为注意力权重。
3.  用这个权重对 $V$ 进行加权求和，得到最终的输出。
4.  公式（不一定要背，但要理解）：$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

### 为什么要除以 $\sqrt{d_k}$？
放 (Scaling)。防止点积结果过大，导致 $\text{Softmax}$ 函数进入梯度很小的区域，使得梯度消失，训练不稳定。

### 什么是多头注意力 (Multi-Head Attention)？为什么需要它？
将 $Q, K, V$ 拆分成 $h$ 个“头”（Head），分别进行注意力计算，最后再把 $h$ 个头的结果拼接起来。允许模型在不同的“表示子空间”中学习信息。好比“从不同角度去审视”这个句子。
### Transformer 中的位置编码是做什么的？为什么需要它？
因为 Self-Attention 机制本身是**位置无关**的（它只看词与词之间的关系，不看谁在谁前面）。

必须有一个东西告诉模型词的顺序信息。位置编码就是给每个位置的词嵌入加上一个独特的位置向量（通常用 $\sin$ 和 $\cos$ 函数生成）。

### Transformer 中的残差连接 (Residual Connection) 和层归一化 (Layer Normalization) 有什么作用？
* **残差连接：** $x + \text{SubLayer}(x)$。解决深度神经网络中的**梯度消失**问题，让梯度可以“抄近道”传递，使训练非常深的网络成为可能。
* **层归一化：** 在**特征维度**上进行归一化。帮助稳定训练过程，加速收敛。

## 三、 LLM 架构与代表模型
### LLM 主要有哪些架构？它们分别适用于什么任务？
* **Encoder-Only (编码器架构):** 如 BERT, RoBERTa。双向上下文。适用于**自然语言理解 (NLU)** 任务，如分类、命名实体识别 (NER)、问答。
* **Decoder-Only (解码器架构):** 如 GPT 系列 (GPT-3, ChatGPT), LLaMA。单向上下文（Causal LM）。适用于**自然语言生成 (NLG)** 任务，如文本续写、对话。
* **Encoder-Decoder (编码器-解码器架构):** 如 T5, BART。适用于**序列到序列 (Seq2Seq)** 任务，如翻译、摘要。

### BERT 的预训练任务是什么？
* **MLM (Masked Language Model, 掩码语言模型):** 随机遮盖 (Mask) 掉句子中 15% 的词，让模型去预测这些被遮盖的词是什么。这迫使模型学习双向上下文。
* **NSP (Next Sentence Prediction, 下一句预测):** 判断两个句子是否是原文中连续的。

### GPT 的预训练任务是什么？
Causal Language Model (CLM, 因果语言模型)，也就是“下一个词预测”。模型只能看到当前词以及它之前的所有词，来预测下一个词。

### 什么是 "In-Context Learning" (上下文学习)？ 什么是 Zero-shot, One-shot, Few-shot？
是 GPT-3 带来的一个重要能力。不需要更新模型参数（不需要微调），只在**提示词 (Prompt)** 中给模型几个例子，模型就能“学会”如何做这个新任务。
* **Zero-shot:** 不给例子，直接给指令。
* **One-shot:** 给 1 个例子。
* **Few-shot:** 给 2 个或更多例子。

## 四、 训练、微调与对齐
### 什么是预训练 (Pre-training) 和微调 (Fine-tuning)？
* **预训练：** 在海量的、无标签的文本数据上，使用自监督任务（如 MLM 或 CLM）训练一个通用的基础模型。这个过程非常昂贵。
* **微调：** 在预训练好的模型基础上，使用一个小的、有标签的、针对特定任务的数据集，继续训练模型，使其“适配”这个任务。

### 么是 RLHF？它主要分为哪几个步骤？
**RLHF (Reinforcement Learning from Human Feedback, 基于人类反馈的强化学习):** 这是让 ChatGPT 如此“听话”和“有用”的关键技术，用于**模型对齐 (Alignment)**。

步骤：
1.  **SFT (Supervised Fine-Tuning):** 收集高质量的“指令-回答”数据，对预训练模型进行有监督微调。
2.  **训练奖励模型 (Reward Model, RM):** 用 SFT 模型对同一个指令生成多个回答 (A, B, C, D)，然后请人类标注员对这些回答进行排序。用这个排序数据训练一个 RM，使其能给“好的”回答打高分，给“坏的”回答打低分。
3.  **RL 强化学习 (PPO 算法):** 用 SFT 模型作为策略 (Policy)，用 RM 作为奖励函数，使用强化学习（如 PPO）来优化模型，使其生成的回答能获得 RM 的高分。

### 什么是 PEFT？你了解哪些 PEFT 技术？
**PEFT (Parameter-Efficient Fine-Tuning, 参数高效微调):** 在微调 LLM 时，只训练模型的一小部分参数，而不是全部参数（如 175B 个）。

全参数微调成本太高（需要几十到几百 GB 的显存）。

例如 **LoRA (Low-Rank Adaptation)** 核心。冻结原始权重 $W$，在旁边加一个低秩矩阵 $W + \Delta W = W + BA$。只训练 $B$ 和 $A$（参数量远小于 $W$）。

## 五、 应用、挑战与前沿
### 什么是 RAG (Retrieval-Augmented Generation)？
这是目前最主流的 LLM 应用范式，**非常重要**。
* **工作原理：** 当用户提问时，系统**首先**去一个外部知识库（如向量数据库）中**检索 (Retrieval)** 相关的文档片段，然后把这些文档片段和用户的原始问题一起**拼接 (Augmented)** 成一个新的 Prompt，最后交给 LLM 去**生成 (Generation)** 答案。

### RAG 解决了 LLM 的什么问题？
* **缓解模型幻觉 (Hallucination):** 答案被“锚定”在检索到的事实上，而不是模型自己“编造”。
* **知识更新：** LLM 的知识是静态的（训练截止日期）。RAG 可以通过更新外部知识库，让 LLM 回答最新的问题。
* **可解释性/溯源：** 可以告诉用户答案是基于哪些文档生成的。

### 什么是向量数据库 (Vector Database)？
门用于存储和高效查询**向量 (Embedding)** 的数据库。RAG 的核心组件之一。

### LLM 目前面临哪些主要挑战？
* **幻觉 (Hallucination):** 编造事实。
* **上下文窗口 (Context Window) 限制：** 能处理的文本长度有限（虽然现在越来越长）。
* **成本高昂：** 训练和推理都需要强大的算力。
* **数据偏见 (Bias) 与安全性。**

## Python与工具库
### 你常用哪些Python库来做AI开发？
* **NumPy:** 科学计算，处理多维数组 (N-dimensional Array)。
* **Pandas:** 数据分析和处理，核心是 DataFrame 和 Series。
* **Scikit-learn (sklearn):** 传统的机器学习库，包含大量算法、预处理和评估工具。
* **TensorFlow / PyTorch:** 深度学习框架。

### Pandas中 `loc` 和 `iloc` 有什么区别？
* `loc` 是基于**标签 (label)** 的索引。
* `iloc` 是基于**位置 (integer position)** 的索引。

### 你如何用 Scikit-learn 训练一个完整的机器学习模型？
1. 加载数据 (load_data)。 
2. 数据预处理 (preprocessing)。 
3. 划分训练集和测试集 (train_test_split)。 
4. 初始化模型 (e.g., `model = LogisticRegression()`)。 
5. 训练模型 (`model.fit(X_train, y_train)`)。 
6. 评估模型 (`model.score(X_test, y_test)`)。

### 核心框架（模型开发与训练）
这是构建和训练 LLM 本身的基础。
* **PyTorch:**
    * **用途：** 目前 LLM 领域**事实上的标准**。绝大多数前沿研究（包括 GPT、LLaMA 等模型的原始实现）和开源项目都基于 PyTorch。它提供了灵活的张量计算和动态计算图，非常适合复杂的模型架构。
    * **面试重点：** 必须熟悉。

* **TensorFlow / Keras:**
    * **用途：** 另一个主要的深度学习框架。虽然在 LLM 研究领域的热度稍逊于 PyTorch，但在 Google 内部（如 PaLM, Gemini）和许多企业的生产环境中仍有广泛应用。Keras 提供了更高级、更易用的 API。

## 2. Hugging Face 生态（事实上的工具标准）
Hugging Face (HF) 提供了一套“全家桶”，极大地简化了 LLM 的使用和开发。

* **`transformers`:**
    * **用途：** **核心中的核心**。它提供了一个统一的 API，让你可以在几行代码内加载、训练和使用几乎所有主流的 Transformer 模型（如 BERT, GPT-2, LLaMA, T5 等）。它内置了模型架构、预训练权重、分词器 (Tokenizer) 和配置。
    * **面试重点：** 初级岗位必须熟练使用。

* **`datasets`:**
    * **用途：** 高效处理和加载海量文本数据集（GB 甚至 TB 级别）。它支持流式 (streaming) 加载、内存映射 (memory mapping) 和强大的数据预处理（如 `map` 操作），是预训练和微调必备的工具。

* **`tokenizers`:**
    * **用途：** `transformers` 库背后的高性能分词器库。它提供了 BPE, WordPiece 等主流分词算法的快速实现。

* **`accelerate`:**
    * **用途：** 简化 PyTorch 的**分布式训练和混合精度训练**。只需添加几行代码，就能让你的训练脚本轻松地在多 GPU、TPU 或多台机器上运行。

* **`evaluate`:**
    * **用途：** 方便地加载和计算各种 NLP 评估指标，如 BLEU, ROUGE (用于摘要/翻译) 或 PPL (Perplexity, 困惑度，用于评估语言模型本身)。

### 3. LLM 应用框架（构建应用）

当你**不训练**模型，而是**调用**模型（如 GPT-4 API 或开源模型）来构建 RAG、Agents 等应用时，会用到这些框架。

* **LangChain:**
    * **用途：** 目前最火的 LLM 应用开发框架。它是一个“胶水”层，帮你**编排（Orchestrate）** LLM 调用的各个环节。
    * **核心功能：** Prompt 模板管理、Chains（将多个 LLM 调用或工具调用链接起来）、Agents（让 LLM 决定使用哪些工具）、Memory（给对话添加记忆）、RAG（文档加载、切分、检索）。

* **LlamaIndex:**
    * **用途：** 另一个流行的框架，它更专注于 **RAG**（检索增强生成）。在数据摄入、索引构建和检索查询方面通常被认为更强大和灵活。

* **Semantic Kernel:**
    * **用途：** 微软开源的框架，思路与 LangChain 类似，但在 C# 和 Python 中都有实现，更侧重于与微软生态（如 Azure）的集成。

### 4. 向量数据库（RAG 的 "记忆"）

RAG 是 LLM 最核心的应用之一，而向量数据库是 RAG 的基础。

* **ChromaDB (Chroma):**
    * **用途：** 开源、轻量级、内存优先的向量数据库。非常适合快速原型设计和中小型项目，与 LangChain/LlamaIndex 集成度很高。

* **FAISS:**
    * **用途：** Facebook AI (Meta) 开源的向量**库**（不是数据库）。它提供了极其高效的向量相似度搜索算法，但本身不带数据库管理功能。通常被用作其他数据库的底层索引引擎。

* **Pinecone / Weaviate / Milvus:**
    * **用途：** 生产级别的、可扩展的向量数据库。当你需要处理数十亿级别的向量、需要高可用和复杂的元数据过滤时，会选择它们。

### 5. 推理与服务（部署）

当你需要将训练好的模型部署为 API 时使用。

* **FastAPI:**
    * **用途：** 目前 Python 领域**最快、最流行**的 Web 框架之一，特别适合构建 ML/AI 模型的 API 服务。它基于 ASGI（异步），性能极高，并且自带 Swagger UI 自动生成 API 文档。

* **Flask:**
    * **用途：** 轻量级的传统 Web 框架。对于简单的模型 API 仍然是一个不错的选择，但性能不如 FastAPI。

* **vLLM / TGI (Text Generation Inference):**
    * **用途：** 专门为 LLM 设计的高性能推理服务器。它们通过 PagedAttention、连续批处理 (Continuous Batching) 等技术，极大提升 LLM 的推理吞吐量（每秒处理的请求数）。

* **`bitsandbytes` / `auto-gptq`:**
    * **用途：** 模型**量化 (Quantization)** 库。用于将模型从 FP16/FP32 压缩到 8-bit 甚至 4-bit，从而在消费级显卡（如 3090, 4090）上运行大型模型。
