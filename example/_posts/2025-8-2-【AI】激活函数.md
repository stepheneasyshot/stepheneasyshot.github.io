---
layout: post
description: > 
  本文介绍了若干种常见的AI领域的算法及其应用场景
image: 
  path: /assets/img/blog/blogs_ai_common_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_common_cover.png
    960w:  /assets/img/blog/blogs_ai_common_cover.png
    480w:  /assets/img/blog/blogs_ai_common_cover.png
accent_image: /assets/img/blog/blogs_ai_common_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】激活函数
简单来说，激活函数的主要作用是 **向神经网络中引入非线性因素** 。可以将激活函数理解为神经网络中一个至关重要的“开关”和“调节器”。它被 **应用在每个神经元的输出** 上，用来决定这个神经元应该在多大程度上被“激活”，以及它应该向下一层传递多强的信号。

在人工神经网络中，一个神经元会接收来自上一层的多个输入信号。它会首先将这些输入信号进行 **“加权求和”** ，并加上一个 **偏置项** 。

这个加权求和的结果是一个线性的值。

$$z = \sum(weight \cdot input) + bias$$

**激活函数 f(z) 就是紧接着应用在这个线性结果 z 上的一个非线性函数**。它会产生该神经元的最终输出 **a = f(z)** ，这个输出 **a** 随后会作为输入传递给网络的下一层。

这一点至关重要。想象一下，如果**没有**激活函数会发生什么？

* 每一层的输出都只是上一层输入的线性组合。
* 无论你堆叠多少层神经网络，整个网络的最终输出也仍然只是最开始输入的线性组合。
* 这样的网络，无论多深，其能力都等同于一个单层的线性模型（比如线性回归或逻辑回归）。
* 它将完全无法学习和拟合现实世界中复杂的非线性关系（例如图像识别、语音识别等）。


**激活函数** 被应用到每个神经元的输出上，对加权求和后的结果进行一次 **非线性变换** 。正是这种非线性变换，使得神经网络能够：

1.  **拟合复杂模式**：能够学习和逼近几乎任何复杂的非线性函数，从而处理像图像识别、自然语言处理这样复杂的问题。
2.  **增强网络能力**：赋予了网络更强的表达能力，使其能够区分和学习那些线性模型无法区分的数据特征。
3.  **控制输出范围**：某些激活函数（如 Sigmoid）可以将输出值压缩到特定范围内（例如 0 到 1），这在特定任务中（如概率预测）非常有用。

### 激活函数分类和优缺点
以下是几种最常用和最重要的激活函数，包括它们的公式、特点以及优缺点。激活函数分类图表：

![](/assets/img/blog/blogs_ai_activation_func.png)

#### 1. Sigmoid (Logistic) 函数
公式：

$$f(x) = \frac{1}{1 + e^{-x}}$$

它能将输入值“压缩”到 0 和 1 之间，输出平滑且易于求导。在早期神经网络中很流行，常用于二元分类任务的输出层（输出概率）。

**缺点**：
1.  **梯度消失**：当输入值非常大或非常小时，函数的导数（梯度）趋近于 0。在反向传播过程中，误差（损失）的梯度需要从输出层一路“传播”回输入层，以便更新每一层的权重。梯度消失就是指，在传播过程中，梯度信号变得越来越小，当传到网络的浅层（靠近输入的层）时，梯度已经小到几乎为零。
2.  **输出非零中心**：输出始终为正数（0到1），这会导致后续网络层的输入是非零均值的，这使网络的收敛速度变慢，可能降低训练效率。

* **Sigmoid 的输出**：始终在 $(0, 1)$ 区间，**恒为正数**。
* **对下一层的影响**：在反向传播中，某一层的权重 $W$ 的梯度，会包含来自上一层的输入 $x$（即 Sigmoid 的输出）。

$$
\frac{\partial L}{\partial W} = \text{(上游梯度)} \times x
$$

由于 x（Sigmoid 的输出）始终为正，导致 $\frac{\partial L}{\partial W}$ 的**所有分量的符号（正或负）都完全取决于上游梯度**。

这会导致权重在更新时，要么**所有的权重都一起增加，要么所有的权重都一起减小** 。这限制了梯度下降的寻优路径，使其只能呈“Z”字形（ZigZag）下降，收敛效率低下。

相比之下， **Tanh** （输出为 $-1$ 到 $1$，零中心）在这个问题上表现更好。而 **ReLU** （$f(x) = \max(0, x)$）则彻底解决了梯度消失问题（在正区间，导数恒为 1），因此成为了现在最主流的激活函数。
#### 2. Tanh (双曲正切) 函数
公式：

$$f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

它将输入值“压缩”到 -1 和 1 之间。与 Sigmoid 相比，它的**输出是零中心**的（均值为 0），这通常能带来更快的收敛速度。

缺点是仍然存在**梯度消失**的问题，当输入值饱和时（接近 -1 或 1），梯度也会趋近于 0。
#### 3. ReLU (Rectified Linear Unit, 修正线性单元)
公式：

$$f(x) = \max(0, x)$$

**解决了梯度消失**（在正区间）：当输入 x > 0 时，导数恒为 1，这极大地缓解了梯度消失问题，使得训练深度网络成为可能。

计算非常简单（只是一个阈值判断），比 Sigmoid 和 Tanh 的指数运算快得多。

当输入 x < 0 时，输出为 0，这能使网络中的一些神经元“关闭”，带来稀疏性，可能有助于提取特征和防止过拟合。

缺点是 **Dying ReLU（神经元死亡）**如果一个神经元的输入在训练过程中始终为负数，那么它的输出将永远是 0，梯度也永远是 0。这个神经元将停止学习和更新。
#### 4. Leaky ReLU (LReLU, 泄露型 ReLU)
公式：

$$f(x) = \max(\alpha x, x)$$

（其中 alpha 是一个很小的常数，如 0.01）

为了解决 “Dying ReLU” 问题而设计。当输入 x < 0 时，它不再输出 0，而是输出一个非常小的正值（如 0.01x），从而保证了在负区间的梯度不为零。

PReLU (Parametric ReLU) 是 Leaky ReLU 的一个变种，alpha 不是固定的，而是作为一个参数通过网络训练学习得到。
#### 5. ELU (Exponential Linear Unit, 指数线性单元)
公式：

$$f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{if } x \le 0 \end{cases}$$

融合了 ReLU 和 Leaky ReLU 的优点。它在负区间有输出（避免神经元死亡），且输出均值接近于 0（类似 Tanh），有助于加速学习。在负区间的“软饱和”特性使其对噪声有一定的鲁棒性。

缺点是计算上比 ReLU 复杂（涉及指数运算）。
#### 6. Softmax 函数
公式：

$$f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$$


严格来说，它更像是一个“归一化”函数，而非隐藏层的激活函数。它**专门用于多分类问题的输出层**。

它能将一个包含任意实数的向量，转换成一个“概率分布”向量。向量中所有元素的和为 1，且每个元素都在 0 和 1 之间，可以被解释为该样本属于各个类别的概率。

### 如何选择激活函数？
在现代深度学习实践中（特别是作为 Android 开发者，你可能接触到的 TFLite 模型中）：
* **首选 ReLU**：在绝大多数情况下，**ReLU** 是隐藏层的**默认和首选**。它简单、高效，并且效果很好。
* **尝试 ReLU 变体**：如果发现 ReLU 导致了大量的“神经元死亡”，可以尝试使用 **Leaky ReLU**、**PReLU** 或 **ELU** 作为替代。
* **用于输出层**：
    * **二元分类**（是/否）：使用 **Sigmoid**。
    * **多元分类**（猫/狗/鸟）：使用 **Softmax**。
    * **回归任务**（预测一个连续值，如房价）：**不使用激活函数**（即线性输出）。
* **Tanh** 和 **Sigmoid** 现在较少用于深度网络的隐藏层，但在某些特定架构（如循环神经网络 RNN）中仍会见到 Tanh。