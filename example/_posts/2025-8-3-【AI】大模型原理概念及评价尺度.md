---
layout: post
description: > 
  本文梳理了当下最流行的几种AI大模型的原理概念
image: 
  path: /assets/img/blog/blogs_ai_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_cover.png
    960w:  /assets/img/blog/blogs_ai_cover.png
    480w:  /assets/img/blog/blogs_ai_cover.png
accent_image: /assets/img/blog/blogs_ai_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】大模型原理概念及评价尺度
## 概念理解
### Transformer
**Transformer** 是 **Google** 研究人员开发的一种深度学习架构，是一种基于自我注意力机制的新型神经网络架构，它基于多注意力机制，该机制是在 2017 年的论文《Attention Is All You Need》中首次提出的，特别适用于语言理解。

> 文本被转换为称为令牌的数字表示，每个令牌通过从单词嵌入表中查找转换为一个向量。在每一层，每个标记都会在上下文窗口的范围内，通过并行多头关注机制与其他（未屏蔽的）标记进行上下文关联，从而放大关键标记的信号，减弱不那么重要的标记。

Transformer 的优点是没有递归单元，因此与早期的递归神经架构（RNN）（如长短期记忆（LSTM））相比，所需的训练时间更短。后来被广泛用于在大型（语言）数据集（如维基百科语料库和 Common Crawl）上训练大型语言模型（LLM）。

Transformer 还需要更少的计算来进行训练，而且更适合现代机器学习硬件，可将训练速度提高一个数量级。

### 自注意力机制
近年来， **RNN** 已成为翻译的典型网络架构，它以 **从左到右或从右到左** 的方式依次处理语言。一次读取一个单词，这迫使 RNNs 执行多个步骤，根据相距甚远的单词做出决定。

神经网络通常通过生成固定或可变长度的向量空间表示来处理语言。神经网络从单个单词或甚至单词片段的表征开始，然后汇总周围单词的信息，以确定特定语言在上下文中的含义。例如，要确定句子 **“我穿过......到达银行”** 中 “银行” 一词最可能的含义和适当的表示方法，需要知道句子是以 “......路” 还是 “......河” 结尾。

在处理上述示例时，RNN 只有在一步步读取 “bank” 和 “river” 之间的每个单词后，才能确定 “bank ”可能指的是河岸。先前的研究表明，粗略地说，决策所需的步骤越多，递归网络就越难学会如何做出这些决策。

RNN 的顺序性也使其更难充分利用 TPU 和 GPU 等现代快速计算设备，这些设备擅长并行处理而非顺序处理。卷积神经网络（CNN）的顺序性远低于 RNN，但在 ByteNet 或 ConvS2S 等 CNN 体系结构中，其数量却远低于 RNN。

相比之下，Transformer 只执行少量、恒定的步骤（根据经验选择）。在每一步中，它都会应用一种自我注意机制，直接模拟句子中所有单词之间的关系，而不管它们各自的位置如何。在前面的例子 **“I arrived at the bank after crossing the river”** 中，为了确定 “bank ”一词指的是河岸而不是金融机构，Transformer 可以学会 **立即注意 “river ”一词** ，并在一个步骤中做出判断。事实上，在我们的英法互译模型中，我们观察到的正是这种行为。

更具体地说，为了计算给定单词（例如 “银行”）的下一个表征， **transformer会将其与句子中的其他单词进行比较** 。这些比较的结果就是句子中其他每个单词的注意力分数。这些注意力分数决定了其他每个单词对 “bank” 的下一个表示的贡献程度。在本例中，在计算 “bank” 的新表示时，能消除歧义的 “river” 可能会得到较高的关注分。然后，将注意力分数作为权重，对所有单词的表征进行加权平均，并将其输入全连接网络，从而生成 “bank” 的新表征，反映出该句子是在谈论河岸。
### Tokenization
在自然语言处理中， **token** 是指文本中的最小单位。它们可以是单词、子词或字符，具体取决于所使用的模型和任务。例如，在英语中，单词是 token 的基本单位；在中文中，字是 token 的基本单位。

**Tokenization** 是一个简单的过程，它将原始数据转换成有用的数据字符串。因其在网络安全和创建 NFT 中的应用而广为人知，同时它也是 NLP 流程的重要组成部分。在自然语言处理中，Tokenization 用于将段落和句子分割成更小的单位，以便更容易赋予其含义。

NLP 流程的第一步是收集数据（句子）并将其分解为可理解的部分（单词）。Tokenization 是将这些单词分解为更小的单位的过程。例如，“我喜欢吃苹果” 这句话可以被分解为 “我”、“喜欢”、“吃” 和 “苹果” 四个单词。
### 预训练
预训练是一种深度学习模型训练的策略，通常在大规模的数据集上进行。预训练的目标是通过在一个相关但较大的任务上训练模型，使得模型学习到通用的特征表示。这样的预训练模型在其他具体任务上的表现通常更好，因为它已经学习到了普适的特征。

在深度学习中，预训练可以分为两种主要类型：无监督预训练和有监督预训练。

* 无监督预训练：在无监督预训练中，模型在没有标签的大规模数据上进行预训练。常见的无监督预训练方法包括自编码器、变分自编码器、对比预训练等。预训练后，模型的参数会被调整到一种更有用的表示形式，使得它能够从输入数据中提取有意义的特征。

* 有监督预训练：在有监督预训练中，模型在一个与最终任务相关的较大数据集上进行预训练。然后，可以使用这些预训练的权重作为最终任务（如分类、回归等）的初始参数。这种方法通常能够加速最终任务的训练过程，特别是在目标任务数据较少时。

预训练的好处在于，通过利用 **大规模数据进行训练** ，模型可以学习到 **更泛化的特征表示** ，从而在具体任务上表现更好。这对于数据较少的任务或者计算资源有限的情况下特别有用。预训练的模型也经常用于迁移学习，可以将预训练模型的部分或全部用于新的任务，以提高模型的性能。

#### 预训练作用
* 加速训练过程：通过预训练，在大规模数据上学习到的通用特征表示可以作为初始化参数，加速模型在特定任务上的训练过程。这是因为预训练的参数已经接近最优，并且已经捕捉到了输入数据中的一些通用模式，这样在目标任务上的优化过程更容易收敛。
* 提高性能：预训练的模型通常在具体任务上表现更好。这是因为在预训练阶段，模型学习到了大量的数据中的通用特征，这些特征对于许多任务都是有用的。在目标任务中，预训练的模型能够更好地利用这些通用特征，从而提高性能。
* 解决数据不足问题：在许多实际任务中，数据往往是有限的，特别是深度学习模型需要大量的数据进行训练。通过预训练，可以利用大规模数据集进行通用特征的学习，然后将这些学到的特征应用于目标任务，从而克服数据不足的问题。
* 迁移学习：预训练的模型可以作为迁移学习的基础。将预训练模型的参数应用于新的相关任务，可以利用预训练模型在大规模数据上学习到的通用特征，从而在新任务上提高性能。这对于目标任务数据较少的情况下特别有用。
* 提高泛化能力：预训练有助于提高模型的泛化能力，即在未见过的数据上表现良好。通过在大规模数据上学习通用特征，模型更能够从输入数据中捕捉普遍的模式，而不是过度拟合训练集。

### 微调
微调是指在预训练模型的基础上，对其进行进一步的训练，以适应 **特定的任务** 。例如中医药相关产业，就可以将通用的大模型，通过微调，使其在中医学方面的表现更优异。

微调的过程通常包括将预训练模型的 **权重** 加载到新模型中，然后在 **特定任务的数据集** 上进行训练。微调的目的是使模型能够更好地适应特定任务，而不是从头开始学习。

### 为什么分开做预训练和微调
大模型训练分为预训练和微调的阶段，这种方法提升了模型的 **泛化能力** ，又可以降低开发成本。这是因为两阶段训练策略能充分发挥数据与模型架构的优势，使模型既具备通用性，又能在特定场景中表现优异。

![blogs_ai_pre_process](/assets/img/blog/blogs_ai_pre_process.png)

泛化能力是指模型从训练数据中学习到的知识和模式，能够应用到新的数据、任务或环境中的能力。简单来说，就是模型在面对未曾见过的情况时，依然能够做出合理的判断、预测或生成合适内容的能力。例如，一个图像分类模型在学习了各种动物的图片后，当看到一张从未见过的动物新品种的图片时，能够根据已学的动物特征（如四条腿、毛茸茸等）正确地对其进行分类，这就体现了模型的泛化能力。

**泛化能力堪称 AGI（通用人工智能） 的根基** 。它意味着模型能够从有限的经验里汲取养分，进而在全新的任务或环境中崭露头角。打个比方，就如同学生通过学习课本上有限的例题，掌握了解题方法，便能举一反三，应对考试中形形色色的新题目。

一方面，它为 **从已知迈向未知** 架起了 **推理** 的桥梁，让模型依据已有的知识储备，对未曾接触过的数据和情境做出合理判断。另一方面，在多领域的复杂任务矩阵中，泛化能力使得模型无需推倒重来，就能灵活运用所学，大大提升了知识的复用效率。

为何它至关重要，是因为现实世界犹如一个无穷无尽的宝库，数据的类型和分布千变万化，训练数据不过是沧海一粟，根本无法穷尽所有可能性。泛化能力强的模型，恰似拥有敏锐洞察力的探险家，能够迅速适应新场景，无论面对何种未知挑战，都能展现出强大的实用性和抗干扰的鲁棒性，真正将所学知识的价值最大化。

#### 预训练与微调是如何提升泛化能力的？

大模型训练分为预训练和微调的阶段，这种方法提升了模型的泛化能力。这是因为两阶段训练策略能充分发挥数据与模型架构的优势，使模型既具备通用性，又能在特定场景中表现优异。

**预训练提升了通用泛化能力**

1、海量数据学习通用知识
预训练阶段使用了多样化的海量语料（如书籍、文章、网站等），这些数据涵盖了广泛的领域和语言结构，帮助模型学习到语言的底层规律（如词汇语义、句法结构），不同场景下的通用模式和上下文关系。
2、构建广泛的知识基础
模型通过预训练，积累了关于语言和世界知识的普遍理解。这种知识能够在下游任务中跨领域迁移和应用。
3、减少过拟合的风险
预训练阶段的无监督学习方式依赖于大量未标注数据，使模型能够专注于学习语言规律，而非记忆训练数据，增强了对未见数据的泛化能力。

**微调提升了特定场景的泛化能力**

1. 针对性调整
微调阶段使用与目标任务相关的小规模、高质量标注数据来进一步训练模型，让模型能更精确地适应特定的场景或任务（如情感分析、机器翻译、法律文本理解等）。
1. 增强领域泛化能力
微调让模型可以在一个广泛知识的基础上，快速适应某些特定领域的特定需求，而不必从头训练，体现了迁移学习的强大之处。
1. 降低训练数据需求
微调需要的数据量远远小于从头开始训练一个模型，这种高效性使得泛化能力更易拓展到更多场景。

**结合预训练和微调的好处**

1. 通用性与特定性平衡
预训练提供了通用语言能力，微调则强化了特定任务的表现，这种组合让模型既有“广度”也有“深度”。
1. 跨任务泛化
微调后的模型往往能在相关任务中表现出色，比如一个在医疗文本上微调过的模型，可能在类似领域（如法律文本）的任务中也具备一定的泛化能力。实际应用表明，经过预训练和微调的模型比传统的单任务训练模型在性能上有巨大提升。

总之预训练和微调的两阶段训练方式不仅提升了大模型的泛化能力，还显著提高了模型的实际应用价值。 **预训练让模型学会了普适性规律，而微调则针对特定需求进一步优化，从而在广泛的任务和领域中实现高效、可靠的表现。** 这种训练策略是大模型成功的重要原因之一。

### 预训练和微调的成本
预训练和微调是大模型训练的两个主要阶段，它们各自具有不同的成本和适用场景。

**预训练：一次性高投入，长期复用**

预训练是大模型训练的核心阶段，虽然需要大量计算资源和数据，但其成本可以通过以下方式分摊：
* 通用知识学习：预训练模型通过海量数据学习通用特征（如语言模式、图像特征），这些知识可以迁移到多种任务中，避免了为每个任务从头训练模型的成本。
* 模型复用：预训练模型可以作为一个通用基础模型，供多个任务和开发者使用。例如，Meta 的 LLama 系列、阿里的通义千问等模型被广泛应用于各种下游任务，显著降低了重复训练的成本。

**微调：低成本适应特定任务**

微调是在预训练模型的基础上，使用少量任务特定数据进行调整，其成本远低于从头训练模型：

* 数据效率：微调通常只需要少量标注数据（可能是预训练数据的千分之一甚至更少），大大减少了数据收集和标注的成本。
* 计算效率：微调只需要调整部分模型参数或少量训练步骤，计算资源需求显著低于预训练。例如，微调一个百亿参数模型可能只需要几小时到几天，而预训练可能需要数周甚至数月。
* 快速迭代：微调允许开发者快速试验和优化模型，适应不同任务需求，而无需重新进行昂贵的预训练。

#### 降低开发门槛
预训练和微调的分阶段设计降低了开发者的技术门槛和资源需求：
* 无需从头训练：开发者可以直接使用预训练模型，通过微调快速构建应用，而无需掌握复杂的模型设计和训练技术。
* 小团队也能参与：即使是资源有限的小团队或个人开发者，也可以通过微调预训练模型，开发出高性能的 AI 应用。

目前国内用户使用大模型时，大部分都是直接使用开源的预训练模型（如通义千问、LLama 等），这些模型已经通过海量数据训练，具备了强大的通用能力。用户只需根据自己的特殊需求，使用少量领域数据对模型进行微调，即可快速适配具体任务。这种方式不仅节省了从头训练模型的高昂成本，还大幅缩短了开发周期，降低了技术门槛，使得大模型能够更高效地应用于各行各业，如金融、医疗、教育等领域。
### 多模态
多模态AI是一种将不同形式的数据（如文本、图像、音频等）融合在一起的技术，旨在让模型从多个维度感知和理解信息。这种融合使得AI系统能够从每种模态中获取独特的但互补的信息，从而构建出更全面的世界观。例如，在一个自动驾驶场景中，图像数据可以帮助系统识别道路上的行人，而雷达数据则能够感知车距，两者结合能够显著提升决策准确性。

多模态AI的核心思想是突破单一模态的局限，通过多种模态的协同作用，提升模型的表现力和泛化能力。
### 大模型的定义
大模型是指具有**大量参数**的机器学习模型。这些模型通常在大规模数据集上进行训练，以学习复杂的模式和特征。大模型通常具有更高的计算能力和更多的参数，这使得它们能够更好地理解和生成人类语言。

大模型的优点包括：
* 更好的理解和生成人类语言：大模型可以更好地理解和生成人类语言，因为它们具有更广泛的词汇和更复杂的语法。
* 更好的性能：大模型通常在大规模数据集上进行训练，这使得它们能够更好地学习和泛化。
* 更好的适应性：大模型可以更好地适应不同的任务和领域，因为它们具有更广泛的知识和技能。

### 大模型的分类
大模型可以根据其训练数据和计算资源的不同进行分类。以下是一些常见的大模型分类：
* 基于 Transformer 的模型：这些模型使用 Transformer 架构，这是一种基于自我注意力机制的神经网络架构。这些模型通常在大规模数据集上进行训练，以学习语言的通用模式和特征。
* 基于 LSTM 的模型：这些模型使用 LSTM 架构，这是一种递归神经网络架构。这些模型通常在较小的数据集上进行训练，以学习语言的局部模式和特征。
* 基于 CNN 的模型：这些模型使用 CNN 架构，这是一种卷积神经网络架构。这些模型通常在图像和视频数据上进行训练，以学习图像和视频的特征。

### 大模型的应用
大模型在自然语言处理、计算机视觉、语音识别等领域都有广泛的应用。以下是一些常见的应用：
* 自然语言处理：大模型在自然语言处理任务中具有广泛的应用，如文本分类、命名实体识别、问答系统等。
* 计算机视觉：大模型在计算机视觉任务中也有广泛的应用，如图像分类、目标检测、图像生成等。
* 语音识别：大模型在语音识别任务中也有广泛的应用，如语音转文本、语音合成等。

### 大模型的训练数据

大模型的训练需要大量的数据。以下是一些常见的训练数据：
* 文本数据：文本数据是最常见的训练数据，包括书籍、文章、新闻、代码等。这些数据通常包含大量的文本和标签，用于训练模型理解和生成文本。
* 图像数据：图像数据包括图像和图像的标签，用于训练模型理解和生成图像。
* 语音数据：语音数据包括语音和语音的标签，用于训练模型理解和生成语音。

在使用大模型时，经常看到多少B或者多少M的模型，或者是他的上下文长度为多少K，下面就是对这些常见单位量级的理解：
* K（Kilo, 千）：表示 1,000。在机器学习模型中，通常用来描述较小模型的参数量，比如 100K（十万）参数。
* M（Million, 百万）：表示 1,000,000。一般用于中等规模的模型，比如 BERT-base（110M）。
* B（Billion, 十亿）：表示 1,000,000,000。大型模型通常达到这一量级，比如 GPT-3（175B）。
* T（Trillion, 万亿）：表示 1,000,000,000,000。这代表非常巨大的参数量。GPT-4
的一些版本和其他超大规模模型已经达到甚至超过 1T 参数。

### 训练方法
大模型的训练需要使用深度学习方法。以下是一些常见的深度学习方法：
* 监督学习：监督学习是一种深度学习方法，其中模型被训练来预测输入数据的标签。在自然语言处理任务中，监督学习通常用于文本分类、命名实体识别、问答系统等。
* 无监督学习：无监督学习是一种深度学习方法，其中模型被训练来从数据中学习模式和特征。在自然语言处理任务中，无监督学习通常用于文本聚类、主题建模、文本生成等。
* 强化学习：强化学习是一种深度学习方法，其中模型被训练来通过与环境的交互来学习如何做出最佳决策。在自然语言处理任务中，强化学习通常用于对话系统、推荐系统等。

### 训练硬件
大模型的训练需要大量的计算资源。以下是一些常见的训练硬件：
* GPU：GPU 是一种图形处理器，用于加速深度学习模型的训练。GPU 通常具有大量的核心和内存，可用于训练大型模型。
* TPU：TPU 是一种专用处理器，用于加速深度学习模型的训练。TPU 通常具有大量的核心和内存，可用于训练大型模型。
* 集群：集群是一种计算资源，用于加速深度学习模型的训练。集群通常由多个 GPU 或 TPU 组成，可用于训练大型模型。

## Chat GPT篇
GPT，Generative Pre-trained Transformer，是一种大型语言模型 (LLM，Large Language Model)，利用深度学习生成类似人类的文本。神经网络在包含文本和代码的海量数据集上进行训练，使其能够理解并生成连贯且与上下文相关的响应。作为生成式人工智能领域的关键组成部分，GPT 突破了人工智能的极限，使机器能够生成具有创造性和人类品质的内容。

![blogs_ai_chatgpt](/assets/img/blog/blogs_ai_chatgpt.webp)

### GPT 如何工作？
GPT（生成式预训练 Transformer）的工作原理是分析输入文本，并根据训练它的海量文本数据集预测最可能的下一个单词或短语，本质上通过理解上下文并生成看似自然连贯的响应来模仿人类语言模式。

GPT 模型是一种复杂的人工神经元网络，分层组织以深入处理信息，就像人类大脑一样。它的架构被称为 **transformer** ，这是谷歌研究人员于 2017 年发明并开源的一种神经网络设计。transformer 允许它 **同时分析整个句子，而不是按顺序分析，从而掌握单词之间的关系** ，而不管单词之间的距离有多远。 

这种能力源自“自我注意力”，这种机制让模型能够 **权衡每个单词相对于所有其他单词的重要性** ，模仿人类如何关注句子的不同部分以获取上下文。  

训练这个模型需要输入大量的文本数据（书籍、文章、代码、在线对话），让模型接触人类语言的范围和细微差别。通过反复接触和“反向传播”过程，模型会从预测误差中学习，从而改进其语言的内部表征，变得非常善于理解和生成人类质量的文本。

GPT 工作原理的要点：

* 标记化：文本被分解成称为“标记”（单词或单词的一部分）的较小单元，模型对这些单元进行单独处理。
* 在大型数据集上进行训练：GPT 使用来自互联网的海量文本数据进行训练，包括书籍、文章和代码，从而使其能够学习多种语言模式。
* 预测下一个词：该模型分析输入序列的上下文并计算每个可能的下一个标记的概率，选择最有可能的标记作为输出。
* Transformer 架构：这一关键设计元素使模型能够同时考虑整个句子中单词之间的关系，而不是按顺序处理它们。
* 自注意力机制：在 Transformer 中，“自注意力”使模型在进行预测时能够专注于输入序列中最相关的部分。

### GPT的应用？
#### 内容创作
GPT 模型可以帮助为网站、博客、社交媒体等创建高质量的内容。对于需要定期创建引人入胜且信息丰富的内容的企业和个人来说，这可能是一种有价值的工具。

一个例子是使用 GPT 模型根据提供给模型的特定提示和信息来起草自定义社交媒体帖子或撰写产品描述。这可以帮助腾出时间来完成其他任务。

#### 客户服务
这些模型可用于支持聊天机器人和虚拟助手，以提供客户支持、回答问题和解决问题。这可以帮助企业提高客户满意度并降低支持成本。

想象一下，无论白天还是晚上，您都可以随时获得即时客户服务支持，而无需等待或浏览复杂的电话菜单。这就是人工智能客户服务的潜力。

#### 聊天机器人
除了客服之外，聊天机器人还可以被更广泛的受众用来回答问题，甚至进行随意的交谈。随着 GPT 技术的不断发展，未来有望出现更加复杂、更像人类的聊天机器人。

#### 代码生成
GPT 技术有可能彻底改变开发人员的工作方式。它可用于协助计算机代码生成，这对于希望自动化任务或加快开发过程的开发人员来说是一个有价值的工具。

这可以让开发人员腾出时间专注于更复杂、更有创意的任务。想象一下，在未来，即使是那些编码经验有限的人也可以借助人工智能代码生成工具将自己的想法变成现实。

#### 教育
GPT 有可能通过提供针对每个学生需求的个性化学习体验来改变教育。它可以提供量身定制的反馈、练习题、互动模块、学习计划、虚拟导师和语言支持。这种人工智能的整合可以为所有学生创造一个包容、引人入胜且有效的学习环境。

### GPT 为何重要？
GPT 的意义在于它能够通过语言弥合人与机器之间的鸿沟。它能够熟练地理解和生成类似人类的文本，为沟通、自动化和创意表达开辟了新的可能性。

此外，GPT 对各个领域和任务的适应性使其成为一种变革性技术，有可能彻底改变各种各样的行业。 

### GPT 培训
训练 GPT 模型是一个计算密集型的过程，需要输入大量文本数据并采用自我监督学习方法。该模型不依赖于明确标记的数据；相反，它通过识别数据本身的模式和关系来学习。

训练过程通常涉及以下步骤：

* 数据准备：第一步是收集并准备大量文本和代码数据集。该数据集经过精心策划，尽可能多样化和具有代表性，涵盖广泛的主题、写作风格和语言。
* 标记化：然后将文本数据划分为称为“标记”的较小单元。这些可以是单个单词、单词的一部分，甚至是字符，具体取决于特定的 GPT 模型和所需的粒度级别。
* 模型初始化： GPT 模型使用随机参数进行初始化。这些参数将在训练过程中随着模型从数据中学习而进行调整。
* 自监督学习：然后向模型输入标记化的文本数据，并让其预测序列中的下一个标记。例如，给定输入“The cat sat on the”，模型可能会预测“mat”。
* 反向传播和优化：将模型的预测与训练数据中的实际下一个标记进行比较，并使用它们之间的差异来计算“损失”值。此损失表示模型的预测与事实的偏差。然后，模型使用反向传播来调整其内部参数以最大限度地减少此损失。这种预测、损失计算和参数调整的迭代过程会持续许多个时期，模型会逐渐提高其准确预测序列中下一个标记的能力。

训练数据集的大小、GPT 模型的复杂性以及可用的计算资源在确定训练所需的时间和资源方面都起着至关重要的作用。训练大型 GPT 模型可能需要大量时间，需要专门的硬件和大量的能源消耗。

## LLaMA篇
LLaMA 是 Meta 人工智能研究实验室 (AI Research Lab) 的一个项目，它是一种基于 Transformer 架构的大型语言模型 (LLM)。这是一个最先进的基础大型语言模型，旨在帮助研究人员推进人工智能这一子领域的工作。像 LLaMA 这样规模更小、性能更强的模型能让研究社区中无法使用大量基础设施的其他人研究这些模型，从而进一步实现这一重要且快速变化的领域的民主化。

![blogs_ai_llama_model](/assets/img/blog/blogs_ai_llama_model.webp)

在大型语言模型领域，训练像 LLaMA 这样较小的基础模型是非常可取的，因为这需要更少的计算能力和资源来测试新方法、验证他人的工作和探索新的用例。基础模型在大量无标注数据集上进行训练，因此非常适合针对各种任务进行微调。我们提供了多种大小的 LLaMA 模型（7B、13B、33B 和 65B 参数），还分享了 LLaMA 模型卡，详细介绍了我们如何按照负责任人工智能实践的方法构建模型。

在过去的一年里，大型语言模型--拥有数十亿参数的自然语言处理（NLP）系统--在生成创意文本、解决数学定理、预测蛋白质结构、回答阅读理解问题等方面展现出了新的能力。它们是人工智能能为数十亿人带来巨大潜在利益的最明显案例之一。

即使最近在大型语言模型方面取得了诸多进展，但由于训练和运行这种大型模型所需的资源，对这些模型的全面研究仍然受到限制。这种限制性访问限制了研究人员了解这些大型语言模型如何工作以及工作原因的能力，阻碍了在提高其稳健性和减少已知问题（如偏差、毒性和产生错误信息的可能性）方面取得进展。

在更多词块（即单词片段）上训练的小型模型更容易针对特定的潜在产品用例进行再训练和微调。我们在 1.4 万亿个词块上训练了 LLaMA 65B 和 LLaMA 33B。我们最小的模型 LLaMA 7B 是在 1 万亿个词库的基础上训练的。

要解决大型语言模型中存在的偏差、有毒评论和幻觉等风险，还有更多的研究工作要做。与其他模型一样，LLaMA 也面临着这些挑战。作为一个基础模型，LLaMA 被设计成通用型的，可以应用于许多不同的用例，而不是为特定任务而设计的微调模型。通过共享 LLaMA 的代码，其他研究人员可以更轻松地测试限制或消除大型语言模型中这些问题的新方法。我们还在论文中提供了一组评估模型偏差和毒性的基准评估，以显示模型的局限性，并为这一关键领域的进一步研究提供支持。
## Claude篇
Claude，由人工智能研究实验室 (AI Research Lab) 开发。它是一个多模态模型，能够理解和生成文本、图像和语音。

![blogs_ai_claude_model](/assets/img/blog/blogs_ai_claude_model.png)

Claude 3 系列于 2024 年 3 月发布，由三个模型组成： Haiku 优化了速度，Sonnet 平衡了功能和性能，Opus 专为复杂推理任务而设计。与之前的版本相比，Claude 3 Opus 在数学、编程和逻辑推理等领域的能力得到了增强。

Claude 模型是生成式预训练转换器。它们经过预训练，可以预测大量文本中的下一个单词。然后，对它们进行微调，特别是使用宪法人工智能和人类反馈强化学习（RLHF）进行微调。

Claude 2 受到了批评，因为其严格的道德规范可能会降低可用性和性能。用户在提出善意请求时遭到拒绝，例如系统管理问题 "如何杀死我的 ubuntu 服务器中的所有 python 进程？这引发了一场关于人工智能开发中的 “对齐税”（确保人工智能系统对齐的成本）的争论，讨论的核心是如何平衡道德考虑和实际功能。批评者主张用户自主性和有效性，而支持者则强调人工智能道德的重要性。
## Grok篇
Grok 是埃隆·马斯克的公司 **xAI** 开发的一款对话式人工智能聊天机器人。Grok 可以通过社交媒体平台 X 访问实时信息，据说可以回答大多数其他人工智能系统通常拒绝回答的“棘手”问题。与独立的 AI 工具不同，Grok 位于 X（前身为 Twitter）内。要访问它，用户必须 **登录 X 并购买 Grok 订阅** 。这种整合符合马斯克将社交媒体平台转变为“万能应用”的愿景，Grok 等工具可以补充平台的服务生态系统。

![blogs_ai_grok](/assets/img/blog/blogs_ai_grok.jpeg)

Grok 本质上是马斯克对 ChatGPT 的回应，后者的创造者 (OpenAI) 是马斯克于 2015 年共同创立的，但在 与现任首席执行官 Sam Altman发生权力斗争后于 2018 年离开。此后，马斯克谴责 ChatGPT 过于 **左倾和危险** 。据马斯克称，xAI 旨在成为 OpenAI 的直接竞争对手，其 Grok 聊天机器人不仅是 ChatGPT 的“反觉醒”对手，而且还展示了更大的 生成 AI领域的新可能性。

Grok-1 是 Grok 所依赖的大型语言模型，它使用基于软件管理系统 Kubernetes、机器学习框架 JAX 和编码语言 Rust的定制技术栈进行训练，所有这些都帮助 xAI 比其他聊天机器人更快、更高效地开发 Grok。 

与所有 LLM 一样，Grok-1 也接受了从互联网上抓取的大量文本数据的训练，这些数据包括从维基百科文章到科学论文的所有内容。但 Grok 的不同之处在于它 **可以直接访问 X 上的帖子** 。据该公司称，这使得 Grok 能够“实时了解世界”，这让它“比其他模型具有巨大优势”。

### 独特的两种模式
Grok 提供两种交互方式：“趣味模式”和“常规模式”。默认情况下，Grok 以“趣味模式”运行，这会导致聊天机器人呈现出更前卫或更幽默的个性，有时甚至会产生与事实不符的回答。“常规模式”通常会提供更准确的答案，但与所有 AI 聊天机器人一样，xAI 表示它仍然可能生成 虚假或矛盾的信息。

### 交互范围更宽泛
Grok 可以起草电子邮件、调试代码、产生想法等等，而且全部以 流畅的类人语言完成。它只需接收输入（如命令或问题），应用训练数据中的知识，然后使用复杂的 神经网络生成相关的文本输出。

虽然它的使用方式与其他 AI 聊天机器人相同，事实上，Grok 愿意回答大多数其他聊天机器人会拒绝的问题，无论这些问题有多么 禁忌或具有潜在危害。它的设计更像是一个“好玩又有趣的聊天机器人，你可以用它进行更另类或尖刻的对话。”

从用户界面的角度来看，Grok 还可以同时处理多个查询，用户可以在这些答案之间切换，正如 xAI 联合创始人 Toby Pohlen 在视频演示中展示的那样。代码生成可以直接在Visual Studio Code 编辑器中打开，而文本响应可以保存在 markdown 编辑器中以供日后使用。

### 同GPT的对比
#### Grok更具有实时性
Grok 可以直接实时访问 X 上的帖子，而 ChatGPT 的免费版本只知道截至 2022 年 1 月的信息，付费版本只知道截至 2023 年 4 月的信息。这意味着 Grok 可以参与有关最近事件的对话，例如以色列-哈马斯战争或 2024 年超级碗。事实上，根据提出的问题，Grok 实际上会显示它所引用的 X 上的真实帖子，以表明其观点来自何处。

然而，Vice 的一项调查发现，**Grok 倾向于散布有关时事的不准确信息，并让人们相信未经证实的阴谋论**

#### Grok更不具有政治正确性
用马斯克的话来说，Grok 是“最大限度寻求真相”和“基于事实”的，这意味着它毫无歉意并且在交流时不考虑政治正确性。  

AI 创建了一个不太政治正确的聊天机器人，而此时大多数其他大型人工智能公司都在努力让自己的聊天机器人更加政治化。OpenAI 声称，其新推出的 GPT-4 LLM（为 ChatGPT 的付费版本提供支持）对“不允许的内容”请求的响应可能性降低了 82%，其中包括“仇恨、骚扰”和“暴力”的材料。而 Anthropic 的 Claude聊天机器人是使用宪法人工智能进行训练的，这 有助于降低其产生有毒、危险或不道德反应的可能性。

## Gemini篇
Google Gemini（原名 Bard）是 **Google** 设计的一款人工智能 (AI)聊天机器人工具，使用自然语言处理 ( NLP ) 和机器学习模拟人类对话。除了补充 Google 搜索外，Gemini 还可以集成到网站、消息平台或应用程序中，为用户问题提供逼真的自然语言回答。

![blogs_ai_gemini](/assets/img/blog/blogs_ai_gemini.webp)

Google Gemini 是一系列多模式 AI大型语言模型 ( LLM )，具有语言、音频、代码和视频理解能力。

2024 年 12 月 11 日，谷歌发布了其 LLM 的更新版本，其中包含 **Gemini 2.0 Flash** ，这是 Google AI Studio 和 Vertex AI Gemini 应用程序编程接口 (API) 中集成的实验版本。

### 特性
Gemini 集成了 NLP 功能，可提供理解和处理语言的能力。Gemini 还用于理解输入查询和数据。它能够理解和识别图像，使其能够解析复杂的视觉效果，例如图表和数字，而无需外部光学字符识别 ( OCR )。它还具有广泛的多语言功能，可用于翻译任务和跨不同语言的功能。

与谷歌之前的 AI 模型不同，Gemini 本身就是多模态的，这意味着它对跨多种数据类型的数据集进行端到端训练。作为多模态模型，Gemini 具有跨模态推理能力。这意味着 Gemini 可以对一系列不同的输入数据类型进行推理，包括音频、图像和文本。例如，Gemini 可以理解手写笔记、图形和图表来解决复杂问题。Gemini 架构支持直接将文本、图像、音频波形和视频帧作为交错序列提取。

### Google Gemini 如何运作？
Google Gemini 首先在海量数据上进行训练。训练后，该模型使用多种神经网络技术来理解内容、回答问题、生成文本并产生输出。

具体来说，Gemini LLM 使用基于 **Transformer** 模型的神经网络架构。Gemini 架构已得到增强，可以处理不同数据类型（包括文本、音频和视频）的长上下文序列。Google DeepMind 在 Transformer 解码器中使用高效的 **注意力机制** ，帮助模型处理跨越不同模态的长上下文。

Gemini 模型已在 Google DeepMind 的多种多模式和多语言文本、图像、音频和视频数据集上进行了训练，并使用高级数据过滤来优化训练。随着不同的 Gemini 模型被部署以支持特定的 Google 服务，有一个有针对性的微调过程可用于进一步优化用例的模型。在训练和推理阶段，Gemini 受益于使用 Google 最新的张量处理单元芯片 Trillium（第六代 Google Cloud TPU）。与 TPU v5 相比，Trillium TPU 提供了更高的性能、更低的延迟和更低的成本。它们也比以前的版本更节能。

### Gemini 可以执行的任务
Google Gemini 可以实用地应用于完成各种任务。
LLM 面临的一个关键挑战是存在偏见和潜在有害内容的风险。据 Google 称，Gemini 针对偏见和毒性等风险进行了广泛的安全测试和缓解，以帮助提供一定程度的 LLM 安全性。为了进一步确保 Gemini 正常运行，这些模型根据语言、图像、音频、视频和代码领域的学术基准进行了测试。Google 向公众保证，它遵守一系列 AI 原则。

谷歌在 2023 年 12 月 6 日发布时表示，Gemini 将包含一系列不同大小的模型，每种模型都针对特定的用例和部署环境而设计。

* Ultra 模型是最高端的，专为高度复杂的任务而设计。
* Pro 模型专为大规模性能和部署而设计。截至 2023 年 12 月 13 日，谷歌已在 Google Cloud Vertex AI 和 Google AI Studio 中启用了对 Gemini Pro 的访问。对于代码，Gemini 的一个版本用于支持 Google AlphaCode 2 生成式 AI 编码技术。
* Nano 模型针对的是设备上的使用案例。Gemini Nano 有两个不同版本：Nano-1 模型有 18 亿个参数，而 Nano-2 有 32.5 亿个参数。Nano 被嵌入的地方包括 Google Pixel 9 智能手机。

### Gemini 有何用途？用例和应用
Google Gemini 模型有多种用途，包括文本、图像、音频和视频理解。Gemini 的多模态特性还使这些不同类型的输入能够组合起来生成输出。

* 文本摘要。Gemini模型可以从不同类型的数据中总结内容。
* 文本生成。Gemini可以根据用户提示生成文本。该文本也可以由问答类型的聊天机器人界面驱动。
* 文本翻译。Gemini模型具有广泛的多语言功能，可以翻译和理解 100 多种语言。
* 图像理解。Gemini无需外部 OCR 工具即可解析复杂的视觉效果，例如图表、图形和图解。它可用于图像字幕和视觉问答功能。
* 音频处理。Gemini支持100 多种语言的语音识别和音频翻译任务。
* 视频理解。Gemini可以处理和理解视频片段帧，以回答问题并生成描述。
* 多模态推理。Gemini的一个主要优势是它使用多模态 AI 推理，可以混合不同类型的数据以提示生成输出。
* 代码分析和生成。Gemini可以理解、解释和生成流行编程语言的代码，包括 Python、Java、C++ 和 Go。

#### 应用
Google 开发了 Gemini 作为基础模型，以便广泛集成到各种 Google 服务中。开发人员还可以使用它来构建自己的应用程序。使用 Gemini 的应用程序包括：

* AlphaCode 2. Google DeepMind 的 AlphaCode 2 代码生成工具使用了 Gemini Pro 的定制版本。
* Google Pixel。谷歌打造的 Pixel 8 Pro 智能手机是首款运行 Gemini Nano 的设备。Gemini 为现有 Google 应用中的新功能提供支持，例如 Recorder 中的摘要功能和 Gboard 中用于消息应用的智能回复功能。
* Android。Pixel 8 Pro 是首款受益于 Gemini 的 Android 智能手机。Android 开发人员可以通过 Android 操作系统的 AICore 系统功能使用 Gemini Nano 进行构建。
* Vertex AI。Google Cloud 的 Vertex AI 服务提供了开发人员可以用来构建应用程序的基础模型，同时还提供对 Gemini Pro 的访问权限。
* Google AI Studio。开发人员可以使用基于 Web 的 Google AI Studio 工具通过 Gemini 构建原型和应用程序。
* 搜索。谷歌已尝试在其AI 概览中使用 Gemini来减少延迟并提高质量。

### Gemini的局限性
一些限制可能会导致潜在最终用户犹豫。这些限制包括：

* 训练数据。与所有 AI 聊天机器人一样，Gemini 必须学会给出正确的答案。要做到这一点，模型必须接受正确信息训练，这些信息不能不准确或误导。然而，它们还必须能够识别错误或误导性的信息。
* 偏见和潜在危害。人工智能训练是一个永无止境的、计算密集型的过程，因为总有新的信息需要学习。谷歌声称，在所有 Gemini 模型中，它都遵循了负责任的开发实践，包括广泛的评估，以帮助限制偏见和潜在危害的风险。
* 原创性和创造性。Gemini制作的内容的原创性和创造性是有限的。免费版本尤其如此，它在处理复杂的提示、多个步骤和细微差别以及产生足够的输出方面存在困难。免费版本基于 Gemini Pro LLM，其功能更有限；该平台的付费版本提供更高级的功能。

#### 值得担心的事情

Gemini 的一个担忧在于它可能会向用户提供有偏见或虚假的信息。输入 Gemini 的训练数据中存在的任何偏见都可能导致问题。例如，与所有先进的人工智能软件一样，如果训练数据排除了特定人群中的某些群体，则会导致输出结果出现偏差。

Gemini 倾向于产生幻觉和其他虚构内容，并将它们当作真实内容传递给用户，这也是一个令人担忧的问题。自ChatGPT 诞生以来，这一直是其响应面临的最大风险之一，其他高级 AI 工具也是如此。此外，由于 Gemini 并不总是理解上下文，因此其响应可能与用户提供的提示和查询不相关。

## Deepseek篇
DeepSeek 是一家中国人工智能研究实验室，与 OpenAI 类似，由中国对冲基金 High-Flyer 创立。与其他商业研究实验室（可能 Meta 除外）不同，DeepSeek 主要将其模型开源。与 Meta 不同的是，DeepSeek 真正将其模型开源，允许任何人将其用于商业目的。它已发布了多个模型系列，每个模型都以 DeepSeek 命名，后面跟有版本号。

![blogs_ai_deepseek](/assets/img/blog/blogs_ai_deepseek_model.png)

官方介绍：

>我们推出的DeepSeek-V3是一个强大的专家混合（MoE）语言模型，拥有671B个总参数，每个标记有37B个激活参数。为了实现高效推理和低成本训练，DeepSeek-V3采用了多头潜意识（MLA）和DeepSeekMoE架构，这在DeepSeek-V2中得到了充分验证。此外，DeepSeek-V3 还率先采用了无辅助损失的负载均衡策略，并设定了多标记预测训练目标，以提高性能。我们在14.8万亿个不同的高质量代币上对DeepSeek-V3进行预训练，然后在监督微调和强化学习阶段充分发挥其能力。综合评估显示，DeepSeek-V3的性能优于其他开源模型，并可与领先的闭源模型相媲美。尽管性能卓越，DeepSeek-V3 的全部训练仅需 2.788M H800 GPU 小时。此外，其训练过程也非常稳定。在整个训练过程中，我们没有遇到任何不可恢复的损失峰值，也没有进行任何回滚。

### DeepSeek的低成本训练
根据 DeepSeek-R1 白皮书公布的内容，DeepSeek 使用了类似【知识蒸馏】的技术。

> 知识蒸馏的目标是让一个较小的模型（学生模型）学习一个较大的预训练模型（教师模型）的知识，从而在更少的计算资源和参数量的情况下，仍然能够达到与大模型接近的性能。即：“用一个大模型教会一个小模型”

通俗点讲，假如：ChatGPT 是个苦读 10 年的学霸，那么 DeepSeek 就是个花 1 年学完全部精华的高效学霸。

这也解释了为什么 DeepSeek 能以极低的成本做到接近 ChatGPT 的效果——它本质上是在 用 ChatGPT 教 ChatGPT，再加上国产 AI 生态的独特优化，使其更高效。

DeepSeek-R1 并不是人工智能技术的根本进步。它是训练效率的一个有趣的渐进式进步。然而，重新创建类似 GPT o1 的东西总是比第一次训练它更有效率。

### DeepSeek 与 ChatGPT 的差异
DeepSeek 与 ChatGPT 完全不同，DeepSeek 属于 推理型 大模型，在 深度思考-R1 模式下，可以看到详细的推理过程。

而 ChatGPT 属于 指令型 大模型。它们两者的区别在于：

* 指令型：需要给出具体的指令，大模型会根据你的指令执行。因此就会延伸出【提示词】的概念，详细提示词可以看下我之前写的 这篇关于 Cursor  提示词的文章
* 推理型：具有自己独立思考和分析的能力，它会根据你的【目的】，帮你分析需要怎么做的方式。

ChatGPT 更适合明确指令，DeepSeek 更适合复杂思考。
## AI测试评价尺度
不同公司推出的AI大模型，经常可以在他们的主页看到自家大模型在不同方向上的得分。以方便使用者评估大模型在各个不同场景下的表现，这里列举了常见的一些AI测试项目。

### 评测样本数量
深度学习(deep learning)已经在各个领域取得了广泛的应用，例如在图像分类问题下，其准确率目前可以达到不错的成绩。然而，deep learning是一种data hungry的技术，高的准确率建立在预先给模型“喂了”大量的数据，即，需要大量的标注样本才能发挥作用，大多数方法是通过有标签的训练集进行学习，侧重于对已经在训练中出现过标签类别的样本进行分类。

然而在现实场景中，许多任务需要对模型之前从未见过的实例类别进行分类，这样就使得原有训练方法不再适用。因为，现实世界中，有很多问题是没有这么多的标注数据的，或者获取标注数据的成本非常大。

所以，我们思考，当标注数据量比较少时、甚至样本为零时，还能不能继续？

我们将这样的方法称为小样本学习Few-Shot Learning，相应的，如果只有一个标注样本，称One-Shot Learning，如果不对该类进行样本标注学习，就是零样本学习Zero-Shot Learning.

#### 零样本学习
2019年冀中等人在综述文章中将零样本分类的定义分为广义和狭义两种：

> 零样本分类的技术目前正处于高速发展时期, 所涉及的具体应用已经从最初的图像分类任务扩展到了其他计算机视觉任务乃至自然语言处理等多个相关领域. 对此, 本文将其称为广义零样本分类. 相应地, 我们将针对图像分类任务的零样本分类任务称为狭义零样本分类。

在冀中和 WEI WANG的文章中，零样本学习均被视为迁移学习的一个特例。零样本学习中，源特征空间是训练样本的特征空间和目标特征空间是测试样本的特征空间，这两者是相同的。但是源标注空间和目标标注空间分别是可见类别和未见类别，两者是不同的。因此零样本学习属于异质迁移学习（heterogeneous transfer learning）。

零样本学习的实现与另外两个研究领域密不可分，其一是表征学习(representation learning)，其二是度量学习(metric learning)。表征学习是指通过对数据进行变换从而提取数据中的有效信息的一种学习方式，涉及到人工智能相关的诸多领域，如信号处理、目标识别、自然语言处理，以及迁移学习等。度量学习通常建立在表征学习的基础之上，其本质是根据不同的任务，根据特定空间中的数据，自主学习出针对某个特定任务的距离度量函数，目前已被广泛应用于诸多计算机视觉相关的任务, 如人脸识别、图像检索、目标跟踪、多模态匹配等。对于零样本学习，在获取到合适的数据表征空间之后，则需要对跨模态样本间的距离度量进行学习，目的是保证嵌入到语义空间后样本间的语义相似度关系得以保持。

综上所述，零样本学习可以看作是在进行表征学习和度量学习的基础上，通过借助辅助信息(属性或文本) 实现跨模态知识的迁移，从而完成可见类信息到未见类信息推断的迁移学习过程。

### MMMU
MMMU，A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI一种新的基准，旨在评估多模态模型在需要大学水平的学科知识和深思熟虑的推理的大规模多学科任务中的表现。MMMU 包括从 大学考试、测验和教科书 中精心收集的11.5K 个多模态问题，涵盖六个核心学科： 艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程 。这些问题涵盖30 个学科和183个子领域，包括 30 种高度异构的图像类型，例如图表、图表、地图、表格、乐谱和化学结构。与现有基准不同，MMMU 专注于使用领域特定知识进行高级感知和推理，挑战模型执行类似于专家面临的任务。我们对 14 个开源 LMM 和专有 GPT-4V（ision）的评估突出了 MMMU 带来的巨大挑战。即使是先进的 GPT-4V 也只能达到 56% 的准确率，这表明有很大的改进空间。我们相信 MMMU 将激励社区构建面向专家级通用人工智能的下一代多模式基础模型。

![blogs_ai_mmmu_test.jpeg](/assets/img/blog/blogs_ai_mmmu_test.jpeg)

MMMU 旨在衡量 LMM 中的三项基本技能：感知、知识和推理。

[【MMMU github page】](https://mmmu-benchmark.github.io/)

### MMLU
MMLU（Massive Multitask Language Understanding，大规模多任务语言理解）是一个广泛应用于评估大型语言模型（LLM）能力的基准测试工具。它由斯坦福大学的研究人员开发，旨在全面测试模型在多个学科和任务中的知识掌握和问题解决能力。

MMLU（大规模多任务语言理解）基准测试覆盖了57个主题，这些主题涵盖了多个领域。具体来说，MMLU的57个主题包括：

* 数学：包括基础数学、高等数学等。
* 历史：包括世界历史、美国历史等。
* 科学：包括物理、化学、生物、地球科学等。
* 人文：包括文学、艺术、哲学等。
* 社会科学：包括政治、经济、社会学等。
* 法律：包括法律基础、法律伦理等。
* 医学：包括解剖学、临床知识、专业医学、遗传学、大学医学和大学生物学等。

#### 特点
1. 覆盖范围广泛：MMLU包含57个主题，涵盖基础数学、美国历史、计算机科学、法律、伦理等多个领域，难度从初级到高级不等，适用于不同水平的测试。
1. 评估方式：MMLU采用多项选择题的形式，要求模型从多个选项中选择最正确的答案。其评分标准基于模型在所有学科中正确回答的比例，分数范围从0到100%。
1. 应用场景：MMLU被广泛用于评估和比较不同语言模型的能力，例如OpenAI的GPT系列、Claude-3等。此外，它也被用于教育技术、机器翻译系统优化以及跨文化交流等领域。

为了应对传统MMLU在某些问题上的局限性，研究者推出了MMLU-Pro版本，增加了问题的复杂性和真实性，同时提高了评分标准。
#### 测试方式
1. 零样本（Zero-shot）和少样本（Few-shot）测试：MMLU支持零样本和少样本两种测试模式。在零样本模式下，模型仅依赖其预训练的知识；而在少样本模式下，模型可以参考少量示例来完成任务。
1. 参数调整：用户可以通过调整tasks和n_shots两个参数来定制测试内容和难度。例如，tasks参数允许用户指定需要测试的学科列表，而n_shots参数则控制每个学科中使用的示例数量。

#### MMLU的挑战与局限性
* 答案顺序的影响：研究表明，答案顺序可能会影响模型的表现，因此建议在评估时随机打乱答案选项以提高准确性。
* 数据质量：部分研究指出，MMLU中的某些问题可能存在错误或模糊性，这可能影响模型表现的可靠性。
* 跨语言能力：虽然MMLU主要用于英语环境，但也有研究尝试通过多语言版本（如MMMLU）来评估模型在不同语言和文化背景下的表现。

### CMMLU
CMMLU是针对中国背景下的大型语言模型的知识和推理能力的评测，由MBZUAI、上海交通大学、微软亚洲研究院共同推出，包含67个主题，专门用于评估语言模型在中文语境下的知识和推理能力。CMMLU是一个涵盖自然科学、社会科学、工程和人文学科等多个学科的综合性中国基准。是国内两大权威评测之一。
### C-Eva
C-Eval是由清华大学、上海交通大学和爱丁堡大学合作构建的综合性考试评测集，覆盖52个学科，是目前权威的中文AI大模型评测榜单之一。是国内两大权威评测之一。C-Eval是全面的中文基础模型评估套件，涵盖了52个不同学科的13948个多项选择题，分为四个难度级别。
### BIG-bench
BIG-bench是一个面向人工智能(AI)的基准测试套件，该套件涵盖了各种AI任务。BIG-bench任务是一个广泛的自然语言处理（NLP）基准测试，旨在评估语言模型在各种复杂的任务上的性能。它是一个由众多具有挑战性的任务组成的集合，这些任务与现实世界的数据没有直接联系。
### GSM8K
**GSM8K** 是由OpenAI发布的大模型数学推理能力评测基准。一个由8.5K高质量的语言多样化的小学数学单词问题组成的数据集（其中7.5K训练集，1K测试集）。这些问题都是由人类写手创造的。每个问题需要2-8步推理来求解，主要是使用基本的算术运算（+-/*）进行一连串的基本计算，以得出最终答案。
GSM8K是两大知名数学推理基准之一，该项测试在2021年10月份发布，至今仍然是非常困难的一种测试基准。

> 提出背景：像GPT-3这样的大型语言模型有许多令人印象深刻的技能，包括模仿许多写作风格的能力，以及广泛的事实知识。但GPT难以完成需要精确多步骤推理的任务，比如解决小学数学单词问题。为了匹配人类在复杂逻辑领域中的表现，OpenAI使用验证器在许多解决方案中选择了最好的GSM8K, 他们收集了新的GSM8K数据集来评估其方法，并发布该数据集以促进研究。

### MATH
**MATH** 数学领域的推理和解决问题能力测试, 是UC Berkeley提出的一个用于评估机器学习模型的数学问题解决能力的数据集。MATH与GSM8K类似，但是包含了12500道高中数学竞赛题，每道题都有详细的步骤化解法，可用于教模型生成答案推导和解释。MATH数据集目前对现有模型仍非常具挑战性。

MATH是两大知名数学推理基准之一。
### DROP
**DROP** （段落离散推理）基准测试是一项众包、对抗式创建的 96000 个问题的基准测试。它要求系统解析问题中的引用（可能涉及多个输入位置），并对其执行离散操作（例如加法、计数或排序）。这些操作需要对段落内容有比以前的数据集更全面的理解。

DROP 基准主要用于以下领域：

* 评估人工智能系统的离散推理能力。
* 测试系统解决问题中的引用并对其执行离散操作的能力。
* 评估这些系统在理解段落内容方面的表现。

### HumanEval
**HumanEval** 是一个用于评估代码生成模型的基准测试集，由OpenAI于2021年发布。它包含了164个编程问题，涵盖了多种编程语言和领域，旨在测试模型在代码生成任务上的性能。
### Natural2Code 
**Natural2Code** 是一个用于评估自然语言到代码生成模型的基准测试集。它旨在测试模型将自然语言描述转换为可执行代码的能力。
### GPQA
Graduate-Level Google-Proof Q&A Benchmark，是一个具有挑战性的数据集，旨在评估大型语言模型 (LLM) 和可扩展监督机制的能力。GPQA 由研究人员推出，包含 448 道选择题，涉及生物学、物理学和化学等领域，由领域专家精心设计，以确保高质量和高难度。

测试数据集有三种不同的题型，题型长度各不相同：扩展题型（546 道）、主观题型（448 道）和钻石题型（198 道）。最初的研究基准比较了zero-shot, few-shot, CoT和搜索变体。
### Chatgpt-4o-mini官网测试报告举例
下面是一个测试报告的举例，以展示比较热门的测试评价尺度：

![blogs_ai_chatgpt_score](/assets/img/blog/blogs_ai_chatgpt_score.png)