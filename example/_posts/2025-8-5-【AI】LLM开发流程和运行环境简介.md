---
layout: post
description: > 
  本文介绍了LLM一般的开发流程和运行环境相关内容
image: 
  path: /assets/img/blog/blogs_ai_arithmetic_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_arithmetic_cover.png
    960w:  /assets/img/blog/blogs_ai_arithmetic_cover.png
    480w:  /assets/img/blog/blogs_ai_arithmetic_cover.png
accent_image: /assets/img/blog/blogs_ai_arithmetic_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】LLM开发流程和运行环境简介
接上文，了解了LLM的发展历程，从简单的线性回归模型到神经网络，再到生成式预训练转换器。

[【AI】AI大模型是如何工作的](./2025-7-19-【AI】AI大模型是如何工作的.md)

现在更多从实际部署运行的操作上，介绍下LLM在各个平台上的运行。
## LLM一般的开发流程
根据 OpenAI 联合创始人 `Andrej Karpathy` 在微软 Build 2023 大会上所公开的信息，OpenAI 所使用的大规模语言模型构建流程如下图所示。主要包含四个阶段：**预训练、有监督微调、奖励建模、强化学习**。这四个阶段都需要不同规模数据集合以及不同类型的算法，会产出不同类型的模型，同时所需要的资源也有非常大的差别。

![](/assets/img/blog/blogs_ai_training_phases_cn.png)

### **预训练（Pretraining）阶段** 
这个阶段需要利用海量的训练数据，包括互联网网页、维基百科、书籍、GitHub、论文、问答网站等，构建包含数千亿甚至数万亿单词的具有多样性的内容。利用**由数千块高性能 GPU 和高速网络组成超级计算机**，花费数十天完成深度神经网络参数训练，构建基础语言模型（Base Model）。基础大模型构建了长文本的建模能力，**使得模型具有语言生成能力**，根据输入的提示词（Prompt），模型可以生成文本补全句子。也有部分研究人员认为，语言模型建模过程中也隐含的构建了包括事实性知识（Factual Knowledge）和常识知识（Commonsense）在内的世界知识（World Knowledge）。根据介绍，GPT-3 完成一次训练的总计算量是 3640PFlops，按照NVIDIA A100 80G 和平均利用率达到 50% 计算，需要花费近一个月时间使用 1000 块 GPU 完成。
### **有监督微调（Supervised Finetuning）** 
有监督微调也称为指令微调（Instruction Tuning），利用少量高质量数据集合，包含用户输入的提示词（Prompt）和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。

例如：

> 提示词（Prompt）：复旦大学有几个校区？

> 理想输出：复旦大学现有 4 个校区，分别是邯郸校区、新江湾校区、枫林校区和张江校区。其中邯郸校区是复旦大学的主校区，邯郸校区与新江湾校区都位于杨浦区，枫林校区位于徐汇区，张江校区位于浦东新区。

利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础语言模型基础上再进行训练，从而得到有监督微调模型（SFT 模型）。经过训练的 SFT 模型具备了初步的指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备了一定的对未知任务的泛化能力。

由于有监督微调阶段的所需的训练语料数量较少，SFT 模型的训练过程并不需要消耗非常大量的计算。根据模型的大小和训练数据量，通常需要数十块 GPU，花费数天时间完成训练。

SFT 模型具备了初步的任务完成能力，可以开放给用户使用，很多类 ChatGPT的模型都属于该类型，包括：`Alpaca[38]`、`Vicuna[39]`、`MOSS` 、 `ChatGLM-6B` 等。

很多这类模型效果也非常好，甚至在一些评测中达到了 ChatGPT 的 90% 的效果。当前的一些研究表明有监督微调阶段数据选择对 SFT 模型效果有非常大的影响，因此如何构造少量并且高质量的训练数据是本阶段有监督微调阶段的研究重点。
### 奖励建模（Reward Modeling）
奖励建模阶段目标是**构建一个文本质量对比模型**，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序。

奖励模型（RM 模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM 模型与基础语言模型和 SFT 模型不同，RM 模型本身并不能单独提供给用户使用。

奖励模型的训练通常和 SFT 模型一样，使用数十块 GPU，通过几天时间完成训练。由于 RM 模型的准确率对于强化学习阶段的效果有着至关重要的影响，因此对于该模型的训练通常需要大规模的训练数据。

`Andrej Karpathy` 在报告中指出，该部分需要百万量级的对比数据标注，而且其中很多标注需要花费非常长的时间才能完成。

![](/assets/img/blog/blogs_ai_training_ranking_outputs.png)

如图，InstructGPT 系统中奖励模型训练样本标注示例。可以看到，示例中文本表达都较为流畅，标注其质量排序需要制定非常详细的规范，标注人员也需要非常认真的对标规范内容进行标注，需要消耗大量的人力，同时如何保持众包标注人员之间的一致性，也是奖励建模阶段需要解决的难点问题之一。

此外**奖励模型的泛化能力边界**也是在本阶段需要重点研究的另一个问题。如果 RM 模型的目标是针对所有提示词系统所生成输出都能够高质量的进行判断，该问题所面临的难度在某种程度上与文本生成等价，因此如何限定 RM 模型应用的泛化边界也是本阶段难点问题。
### 强化学习（Reinforcement Learning）
强化学习阶段**根据数十万用户给出的提示词**，利用在前一阶段训练的 RM 模型，给出 SFT 模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。

该阶段所使用的提示词数量与有监督微调阶段类似，数量在十万量级，并且不需要人工提前给出该提示词所对应的理想回复。使用强化学习，在 SFT 模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段所需要的计算量相较预训练阶段也少很多，通常也仅需要数十块 GPU，经过数天时间的即可完成训练。

强化学习和有监督微调的对比，在模型参数量相同的情况下，强化学习可以得到相较于有监督微调好得多的效果。关于为什么强化学习相比有监督微调可以得到更好结果的问题，截止到 2023 年 9 月也还没有完整和得到普遍共识的解释。

此外，Andrej Karpathy 也指出强化学习也并不是没有问题的，它会**使得基础模型的熵降低**，从而减少了模型输出的多样性。在经过强化学习方法训练完成后的 RL 模型，就是最终提供给用户使用具有理解用户指令和上下文的类 ChatGPT 系统。

由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加 RM 模型的准确率问题，使得在大规模语言模型如何能够有效应用强化学习非常困难。

### 训练数据 (Training Data)
LLM 的能力来源于海量的训练数据。这些数据包括：
* **文本数据：** 网页、书籍、文章、代码、对话等等。这是最主要的数据来源，让模型学会语言的语法、语义、事实知识以及各种表达方式。
* **多模态数据（对于多模态 LLM）：** 图像、音频、视频等。这让模型能够理解和生成不同形式的信息，而不仅仅是文本。

这些数据通过复杂的预处理，被转换成模型可以理解的**数字表示**（比如词向量或 token embeddings）。
### 模型架构 (Model Architecture)
目前，绝大多数大型 LLM 都基于**Transformer (注意力机制)** 架构。Transformer 是一种特殊的神经网络，它有两个主要部分：
* **编码器 (Encoder)：** 负责理解输入文本的上下文信息。
* **解码器 (Decoder)：** 负责根据编码器的理解生成输出文本。

Transformer 的关键是**自注意力机制 (Self-Attention)**。它允许模型在处理一个词时，同时考虑输入序列中所有其他词的重要性，从而捕捉词与词之间的长距离依赖关系。

Transformer 架构图示：

![](/assets/img/blog/blogs_ai_transformer_arch.png)

## 开源社区
### Huggingface
**Hugging Face** 是一家在人工智能（AI）领域迅速崛起的开源社区平台和公司，专注于让机器学习的构建和使用变得更加简单和开放。它最初是一家开发聊天机器人应用的公司，但后来转型并成为了全球AI领域，尤其是**自然语言处理 (NLP)** 领域的关键资源中心。

Hugging Face 的核心理念是 **开放科学** 和 **民主化AI**，通过**提供一系列工具、库和平台**，使得研究人员和开发者能够轻松地共享、发现、训练和部署机器学习模型和数据集。

Hugging Face Hub 是一个类似 GitHub 的平台，但专门用于托管和共享机器学习模型、数据集和演示空间。
* **模型共享**：用户可以上传和下载各种预训练模型，每个模型都有详细的“模型卡片”介绍其用途、性能、局限性等。这促进了模型的可复用性和社区协作。
* **数据集共享**：Hub 上也托管了大量公开数据集，涵盖文本、图像、音频等多种格式，方便研究人员和开发者快速获取训练数据。
* **Gated Datasets**：一些数据集可能需要申请权限才能访问，以确保数据合规性。
* **版本控制**：Hugging Face Hub 使用类似 Git 的版本控制系统，可以追踪模型的迭代和更新。

Hugging Face 对机器学习社区产生了深远的影响。通过提供开源工具和平台，它极大地促进了AI模型的开放共享和协作。降低AI了门槛，使得非专业人士也能更容易地使用和部署复杂的AI模型，推动了AI的普及。标准化了模型接口和共享机制，加速研究与开发，让研究人员可以更快地构建和迭代新模型，开发者可以更快地将研究成果转化为实际应用。Hugging Face 聚集了全球大量的AI研究者和开发者，共同推动AI技术的发展。

总的来说，Hugging Face 已经成为机器学习领域不可或缺的一部分，无论你是研究人员、开发者还是仅仅对AI感兴趣，它都提供了丰富的资源和工具来帮助你探索和构建AI应用。作为一名安卓开发者，Hugging Face 也能帮助你更便捷地将强大的AI能力集成到你的移动应用中。
### 魔塔
**魔塔AI社区**是一个聚焦于人工智能（AI）技术交流与实践的线上社区平台，致力于为开发者、研究者、学生及AI爱好者提供技术分享、协作创新和资源整合的空间。

分主题板块（如模型架构设计、算法优化、行业应用案例），支持代码片段分享、论文解读和技术争议探讨。定期举办“技术擂台”或“挑战赛”（如模型压缩竞赛、数据集标注比赛），激发创新思维。

同时，提供代码托管功能（类似GitHub集成），支持成员发布AI项目、工具库或插件，并通过社区协作迭代优化。设立“明星项目”推荐板块，帮助优质项目获得曝光和贡献者。

还有学习与资源中心，整理AI领域的经典教材、课程笔记、工具链指南（如PyTorch/TensorFlow实战手册），并附有社区成员的批注与实战反馈。开设“新手村”板块，提供从环境配置到模型训练的逐步教程，降低入门门槛。

分享AI在医疗、金融、游戏等垂直行业的落地案例，讨论技术适配与商业化挑战。设立“内推直通车”和“技能树自测”功能，帮助成员对接企业需求，提升就业竞争力。

魔塔AI社区不仅是技术交流的场所，更是一个推动AI技术民主化、产业化的生态平台。无论是寻求答案的学习者、寻找协作的开发者，还是希望探索商业机会的创业者，都能在这里找到对应的价值支点。
## 运行环境
### 为什么现在的大模型对GPU需求这么高？
大模型之所以对 **GPU（图形处理器）** 有着“饥渴”般的需求，根本原因在于它们的**架构特性**和**计算模式**与 GPU 的设计理念完美契合。

CPU 就像一个拥有几名博士的团队，擅长攻克复杂的独立难题；而 GPU 更像一个拥有成千上万名小学生的大部队，虽然每个小学生只能做简单的加减法，但他们可以同时处理数百万个简单的加减法，效率惊人。大模型的计算任务，恰恰就是这种“数百万个简单加减法”的集合。
#### GPU更擅长海量的并行计算
大模型，特别是深度神经网络，在训练和推理过程中会进行**天文数字般的数学运算**，核心就是大量的**矩阵乘法（Matrix Multiplication）**和**向量运算（Vector Operations）**。

* **特点**：这些运算的特点是**高度并行**，即许多独立的、相似的计算可以**同时进行**。例如，一个矩阵的每个元素或一个向量的每个分量可以并行地进行计算，互不干扰。
* **GPU 的优势**：GPU 内部拥有**成百上千甚至上万个小型计算核心**（流处理器），它们天生就是为并行计算而设计的。**CPU 只有少数几个强大的核心，擅长处理复杂且串行的任务**；而 GPU 则能以“人海战术”的方式，同时处理数百万个简单计算。这使得 GPU 在执行**矩阵乘法**这类并行任务时，比 CPU 快上数十甚至数百倍。

> **在神经网络中，每一层神经元从前一层接收输入，然后通过与权重矩阵相乘来计算其输出。这个过程就是矩阵乘法。** 矩阵乘法计算：一个 **m X n** 的 **A** 行列式，和一个 **n X t** 的 **B** 行列式可以进行乘法运算得到一个 **C** 行列式。拿 **A** 的第 **i** 行（n个元素）和 **B** 的第 **j** 列（n个元素）元素乘积之和，就是 **C** 的第 **[i,j]** 位的元素值。

#### 运算参数量巨大
大模型的“大”体现在其**巨量的参数（Parameters）**上。这些参数就是模型从训练数据中学习到的“知识”和“权重”。一个大模型可能拥有数十亿、数千亿甚至上万亿个参数。

每次进行前向传播（推理）或反向传播（训练）时，每个输入数据点都必须与模型的所有相关参数进行交互计算。参数量越大，需要进行的乘加运算就越多。这些参数通常以巨大的矩阵形式存储，GPU 高效的并行处理能力是处理如此大规模矩阵运算的关键。
#### 包含大量的浮点运算
深度学习中的计算，尤其是权重和激活值的计算，通常使用**浮点数（Floating-point Numbers）**。大模型需要执行大量的浮点数乘法和加法。GPU 在设计时就高度优化了浮点运算单元。例如，NVIDIA 的 Tensor Cores 就是专门为深度学习中的混合精度计算（FP16/BF16）而设计的，能够极大地加速这些浮点运算。
#### 高内存带宽和内存容量
大模型不仅参数量大，每次处理的数据批次（batch size）也可能很大，这意味着在计算过程中需要频繁地读取和写入大量数据。GPU 需要快速地从其显存中读取模型参数和输入数据，并将中间结果写回显存。现代 GPU 配备了**高速的 GDDR 显存**（如 GDDR6 或 HBM），拥有比 CPU 系统内存高得多的**内存带宽**，可以快速地吞吐大量数据。同时，高端 GPU 也提供了数十 GB 甚至上百 GB 的显存容量来存储庞大的模型参数。
#### 专门的软件和硬件优化（CUDA 和 Tensor Cores）
NVIDIA 不仅仅提供了强大的 GPU 硬件，还开发了 **CUDA** 这样的并行计算平台，以及专门针对深度学习的 **cuDNN** 等库。
* **CUDA**：它让开发者能够更容易地编写在 GPU 上高效运行的代码，连接了上层软件和底层硬件。
* **Tensor Cores**：在最新的 NVIDIA GPU 中，集成了专门用于加速深度学习矩阵乘法的 Tensor Cores，它们能够以极高的效率执行低精度浮点运算，进一步提升了大模型训练和推理的速度。

##### **CUDA 是什么？**
**CUDA**（Compute Unified Device Architecture，统一计算设备架构）是 **NVIDIA** 公司开发的一种**并行计算平台和编程模型**。简单来说，它是一套让软件能够利用 **NVIDIA 图形处理器（GPU）** 强大计算能力的工具和接口。CUDA 的核心思想是让你能够编写代码，并将其中适合并行计算的部分“卸载”到 GPU 上执行。

它主要通过以下几个方面实现：

1.  **编程模型**：
    * CUDA 提供了一种基于 **C、C++、Fortran** 等传统语言的扩展，允许开发者编写被称为 **“核函数”（Kernels）** 的代码。
    * 核函数是一段在 GPU 上并行执行的代码。当你在 CPU（称为 **Host**）上调用一个核函数时，它会被分发到 GPU（称为 **Device**）上的成千上万个线程中并行执行。
    * 你需要将计算任务分解成小的、独立的子任务，每个子任务由一个 GPU 线程处理。

2.  **GPU 硬件抽象**：
    * CUDA 提供了一套 API（应用程序编程接口），让开发者能够直接控制 GPU 的内存和计算资源。
    * 它抽象了 GPU 复杂的硬件细节，让程序员可以用相对更高层的方式来编写并行代码，而无需深入了解 GPU 内部的微架构。

3.  **内存管理**：
    * CPU 和 GPU 有各自独立的内存空间。在使用 CUDA 时，你需要将数据从 CPU 内存（Host Memory）传输到 GPU 内存（Device Memory），在 GPU 上进行计算，然后再将结果传回 CPU 内存。CUDA 提供了相应的 API 来管理这些数据传输。

4.  **工具套件**：
    * NVIDIA 提供了一个完整的 **CUDA Toolkit**，里面包含了编译器（如 `nvcc`，用于编译 CUDA C/C++ 代码）、库（如 cuDNN 用于深度学习，cuBLAS 用于线性代数）、调试工具、性能分析工具等，帮助开发者编写、优化和部署 GPU 加速的应用。

### 性能受限设备如何运行大模型
即使没有高性能的独立 GPU 的设备也**可以运行大模型**，但这通常会有一些重要的**限制和权衡**。

**CPU 也能执行并行计算** ，现代 CPU 通常有多个核心，并且支持 **SIMD (Single Instruction, Multiple Data)** 指令集（如 Intel 的 AVX、SSE 指令集），这使得它们能够同时处理少量数据。当没有 GPU 时，机器学习框架（如 TensorFlow 或 PyTorch）会**自动回退到使用 CPU 来执行所有的矩阵乘法和其他计算**。这些框架的 CPU 版本也会进行高度优化，以尽可能利用 CPU 的并行能力。

量化和剪枝是让大模型在资源受限设备上运行的关键技术。
* **量化 (Quantization)**：将模型中原来用 32 位浮点数（FP32）表示的权重和激活值，转换为更低精度的格式，如 16 位浮点数（FP16/BF16）、8 位整数（INT8）甚至 4 位整数（INT4）。可以让 **内存占用大幅减少** ，模型文件更小，加载更快。同时，低精度运算所需的计算资源更少，CPU 处理起来更快。
* **剪枝 (Pruning)**：移除模型中不重要或冗余的连接和神经元，从而减小模型的大小和计算量。

这些优化技术可以在保持模型大部分性能的同时，显著降低其对计算资源（包括 CPU）的需求。

**相比于大模型的训练，推理阶段的计算量相对较小** ，**训练**大模型需要极其强大的 GPU，因为它涉及数万亿次的参数更新，需要多次迭代和反向传播。而在 **推理（Inference）**阶段，即模型用于实际预测时，只需要进行前向传播。虽然计算量依然庞大，但比训练时少得多。对于量化后的模型，推理的计算需求会进一步降低。

**在没有 GPU 的电脑上运行大模型也会面临一些问题。**

首先就是推理速度会比有 GPU 的系统慢很多，尤其是对于较大的模型和批次处理。其次，在这些设备上训练大型模型几乎不可能，因为 CPU 的计算能力和内存带宽远远不足。

在性能受限的设备上通常只能运行经过高度优化、量化甚至剪枝的较小版本的大模型，原始的巨型模型无法加载或运行。由于几乎所有的计算任务都需要 CPU 来完成，CPU 满负荷运行时能耗较高，可能导致设备发热严重。
### 端侧大模型是如何运行的
端侧大模型是指在移动端设备上运行的大模型，通常需要在设备上进行**模型加载、推理和显示**等操作。

大模型的**运行环境** ，并不是简单地局限于像 **JVM** (Java虚拟机) 或 **Python虚拟机** 那样的单一解释器环境。它更像一个复杂的**软件栈（Software Stack）**，其中包含了多种技术和组件。
#### 大模型的软件栈
目前，绝大多数大模型的开发和研究都是用 **Python** 完成的。这得益于Python丰富的科学计算库（如 NumPy, SciPy）以及成熟的机器学习框架。

底层真正执行计算密集型任务（尤其是神经网络的矩阵运算）的部分，通常是用 **C++** 编写并高度优化的，并且大量依赖于 **CUDA** (Compute Unified Device Architecture)。CUDA是NVIDIA为自家GPU设计的并行计算平台和编程模型，允许开发者直接利用GPU的强大计算能力。

**PyTorch** 和 **TensorFlow** 是目前最主流的两大开源机器学习框架。这些框架提供了构建、训练和部署神经网络的工具和接口。

当你在自己的设备上运行一个开源的大模型时，你实际上是在使用这些框架来加载模型、输入数据并执行推理。这些框架的底层实现深度优化，能有效利用GPU。

当你在一个服务器上运行一个Python脚本来使用大模型时，这个脚本确实会在**Python解释器**（也就是你提到的“Python虚拟机”）中执行。但是，这个Python解释器只是整个运行流程的“指挥官”。它会调用底层的PyTorch或TensorFlow库，而这些库又会进一步调用 **C++ / CUDA** 编译好的代码，最终指令被发送到 **GPU** 上执行。

### LLM简要组件和运行流程
一个大模型（特指大型语言模型 LLM）在运行时，其进程内部包含多个核心组件协同工作，才能完成从接收输入到生成输出的全过程。这些组件涵盖了数据处理、模型计算和结果输出等多个环节。

一个典型的大模型运行时进程通常会包含以下核心组件：
#### 1. Tokenizer (分词器)
大模型无法直接理解人类的自然语言文本。**分词器**是模型处理文本的第一步，它将输入的原始文本（如句子、段落）分解成模型能够理解的最小单位，这些单位称为 **“token”**。

> Token 可以是单词、子词（例如 "un"、"happy" 组成 "unhappy"），甚至是单个字符。

分词器还将每个 token 映射到一个唯一的**整数 ID**。

* **示例：** 输入 "Hello, world!" 可能会被分词为 `["Hello", ",", " ", "world", "!"]`，并转换为对应的 ID 序列 `[15496, 11, 220, 995, 0]`。

正确的分词对于模型理解输入和生成连贯的输出至关重要。不同的模型可能使用不同的分词器。
#### 2. Model Core (模型核心 / 神经网络)
这个组件是大模型的心脏，负责执行实际的**推理计算**。它是一个由数千亿甚至上万亿个参数组成的**深度神经网络**（通常基于 Transformer 架构）。

核心接收分词器输出的 token ID 序列作为输入。将这些 ID 转换为**词嵌入（Word Embeddings）**，这是一种高维向量表示，捕捉了 token 的语义信息。通过多层 Transformer 块进行复杂的数学运算（主要是**矩阵乘法**和激活函数），处理这些嵌入，捕捉文本中的上下文关系、语法结构和语义含义。最终输出每个位置上下一个 token 预测的**概率分布**。
* **Embedding Layer（嵌入层）：** 将 token ID 转换为高维向量。
* **Transformer Blocks（Transformer 块）：** 包含多头自注意力机制 (Multi-Head Self-Attention) 和前馈网络 (Feed-Forward Network)，负责学习和转换输入序列的表示。
* **Output Layer（输出层）：** 将最终的隐藏状态转换为词汇表中每个 token 的概率。

#### 3. Generation Strategy / Decoding Algorithm (生成策略 / 解码算法)
模型核心输出的是下一个 token 的概率分布，而**生成策略**则根据这些概率来实际选择下一个 token，并决定何时停止生成。这好比模型给出了所有可能的字词及其概率，而生成策略就是选择最合理的那一个。常见的生成策略有：
* **贪婪搜索 (Greedy Search)：** 每次都选择概率最高的 token。
* **束搜索 (Beam Search)：** 同时跟踪多个最有可能的序列路径。
* **Top-K / Top-P (Nucleus Sampling)：** 从概率最高的 K 个 token 或累计概率达到 P 的 token 中随机选择，引入多样性。
* **温度（Temperature）：** 调整概率分布的“锐利度”，影响生成文本的随机性和创造性。

不同的生成策略会显著影响模型输出的**质量、多样性和流畅性**。
#### 4. Context Management (上下文管理)
大模型在生成文本时需要记住之前的对话或输入内容，这就是**上下文（Context）**。上下文管理组件负责维护和更新模型当前处理的上下文。将新生成的 token 添加到现有上下文中。在上下文过长时（超过模型的**上下文窗口限制**），需要执行**截断**或**滑动窗口**等策略来管理，确保模型始终在有效范围内工作。
#### 5. Output Handler (输出处理器)
将模型生成的新 token ID 转换回人类可读的文本。通过分词器的**逆向操作（detokenization）**转换回字符串。同时可能还包括对输出文本的格式化、清理或后处理，例如删除多余的空格、处理特殊字符等。
#### 6. System/Hardware Interface (系统/硬件接口)
这个组件不是模型本身的一部分，但它是大模型进程运行的底层基础。它负责将模型核心的计算任务调度到**硬件加速器（如 GPU 或 NPU）**上执行，并管理数据在内存和显存之间的传输。比如利用 CUDA、cuDNN (NVIDIA GPU) 或其他 AI 加速器 SDK (如 Android 设备的 NPU 驱动)。可以优化数据加载和计算流，以最大化硬件利用率。如果缺乏强大的硬件接口和底层优化，即使有优秀的模型，也无法高效运行。
#### 运行流程概览：
1.  **用户输入**：用户输入文本（例如 "帮我写一首诗关于秋天。"）。
2.  **分词器处理**：分词器将输入文本转换为 token ID 序列。
3.  **模型核心计算**：token ID 序列被送入模型核心。模型进行前向传播计算，输出词汇表中下一个可能 token 的概率分布。
4.  **生成策略选择**：生成策略根据概率分布选择最合适的下一个 token ID。
5.  **上下文更新**：新选出的 token ID 被添加到当前上下文。
6.  **循环迭代**：重复步骤 3-5，直到达到停止条件（例如生成了完整句子、达到最大 token 数或生成了停止符）。
7.  **输出处理器**：将最终生成的 token ID 序列转换回人类可读的文本输出。

### llama.cpp
**llama.cpp** 是一个由 Georgi Gerganov 开发的开源项目，其核心是用 **C/C++** 编写的。它的主要目标是让大型语言模型 (LLMs) 能够在各种硬件上高效地运行**推理**，尤其是在**本地设备上**，包括那些没有高端独立 GPU 的普通电脑或移动设备。该项目的愿景是 **“民主化”LLMs 的部署** ，让每个人都能在自己的设备上体验和使用这些强大的模型，而不仅仅依赖于昂贵的云服务。

`llama.cpp` 的设计理念是追求**极致的效率、最小化外部依赖、广泛的硬件兼容性以及高度的灵活性**。

![](/assets/img/blog/blogs_ai_llamacpp.png)

llama.cpp 并非一个虚拟机，而是一个高效的、C/C++ 实现的 LLM 推理引擎。它通过模型量化、GGUF 格式、底层硬件优化、多平台支持以及灵活的 API 接口，极大地降低了在个人电脑和边缘设备上运行大型语言模型的门槛。
#### 轻量与高效的 C/C++ 实现
`llama.cpp` 完全使用 C 和 C++ 编写，不依赖于大型的深度学习框架（如 TensorFlow 或 PyTorch）的运行时库。这意味着它编译出来的程序**体积非常小，运行时内存占用也低**。这对于资源受限的移动设备来说至关重要。

除了底层的 BLAS (基本线性代数子程序库) 或特定硬件的计算库（如 CUDA、Metal）， `llama.cpp` 几乎没有其他复杂的外部依赖。这使得它非常容易编译和部署。

`llama.cpp` 对 CPU 进行了大量底层优化，利用了各种 CPU 指令集（如 x86-64 上的 AVX/AVX2/AVX-512，以及 ARM 上的 Neon）。这让 LLMs 可以在没有独立 GPU 的设备上，仅仅依靠 CPU 也能获得令人惊讶的推理速度。
#### 广泛的硬件加速支持
除了强大的 CPU 优化，llama.cpp 还支持多种 GPU 和专用硬件加速，这意味着它是**跨平台的**，跨平台支持在任何行业都备受推崇，无论是游戏、人工智能还是其他类型的软件。赋予开发者在他们想要的系统和环境中运行软件所需的自由永远不是一件坏事。

* **Apple Silicon (Metal)**：针对苹果 M 系列芯片的 Metal API 进行了优化，充分利用了其强大的统一内存架构和神经网络引擎。
* **NVIDIA GPU (CUDA)**：支持 NVIDIA 显卡，通过 CUDA 加速推理，性能非常出色。
* **AMD GPU (hipBLAS)**：兼容 AMD GPU。
* **Intel GPU (SYCL/oneAPI)**：支持英特尔的集成和独立显卡。
* **通用 GPU 后端 (Vulkan/OpenCL)**：这对于 Android 开发者尤为重要。通过 **OpenCL** 后端，llama.cpp 能够利用 Android 设备中 SoC（System on Chip）内置的 GPU 或 DSP（数字信号处理器，如高通骁龙的 Hexagon DSP）进行加速，将推理负载从 CPU 转移到更擅长并行计算的硬件上。
* **CPU + GPU 混合推理**：对于一些较大的模型，即使单张 GPU 的显存不足以容纳整个模型，llama.cpp 也能将模型的某些层加载到 GPU 上运行，其余部分则在 CPU 上运行，实现资源的有效利用。

### 使用 GGUF 文件格式进行模型存储
#### GGML
**GGML** 是一个用 **C 语言** 编写的**机器学习库**，由 Georgi Gerganov（也就是 GGML 名字中 "GG" 的来源）创建。它的核心目标是**高效地在 CPU 上运行大型机器学习模型**，特别是大型语言模型（LLMs），并且支持各种硬件平台。

GGML 的出现，是**将 LLMs 带到消费级硬件上的关键一步**。在它之前，运行大型模型通常需要昂贵的 GPU 和复杂的配置。GGML 通过一系列创新技术，让这些模型能够在普通的个人电脑上也能跑起来。

GGML 的核心是用 C 语言编写的，这意味着它的**执行效率非常高**，并且**依赖性极少**。这使得它非常轻量级，可以在各种不同的操作系统和硬件上轻松编译和运行，包括 macOS、Linux、Windows，甚至 iOS 和 Android。

GGML 最初的重点在于**在 CPU 上实现高效推理**。它利用了现代 CPU 的特性，例如 **SIMD（单指令多数据）指令集** ，比如 Intel 的 AVX、AVX2、AVX512 和 ARM 的 NEON，这些指令允许 CPU 同时处理多个数据点，从而加速矩阵乘法等并行计算。可以利用 CPU 的多个核心并行执行计算任务。

##### **模型量化 (Quantization)：**
这是 GGML 最重要的贡献之一。**量化**是一种技术，可以将模型中通常以 32 位浮点数（FP32）或 16 位浮点数（FP16）存储的权重和激活值，转换为更低精度的格式，如 8 位整数（INT8）、4 位整数（INT4），甚至更低。降低了存储和传输模型的成本。使得更大的模型能够加载到有限的 RAM 或 VRAM 中。

GGML 支持多种量化策略，允许开发者在模型大小、推理速度和精度之间进行权衡。

**GGUF (GPT-Generated Unified Format)** 就是 GGML 文件格式的**最新演进版本**。GGUF 在 GGML 的基础上，提供了更强的灵活性、更好的向后兼容性，并能包含更多的元数据（例如分词器信息、提示模板等），使其成为目前社区首选的本地 LLM 格式。虽然 GGML 格式本身逐渐被 GGUF 取代，但 GGML 作为底层库仍然是 GGUF 文件能高效运行的基础。
### GGUF格式的出现
GGUF 格式的诞生是为了解决早期 GGML 格式的一些限制，并提供了**更好的灵活性、可扩展性和向后兼容性**。可以把它想象成一个 **包含了模型“大脑”里所有知识的“盒子”** ，这个盒子设计得非常紧凑和高效，便于在各种设备上快速打开和使用。

**GGUF** 文件是专门为高效存储 LLM 权重和元数据而设计的。

GGUF 格式支持从全精度 (FP32) 到半精度 (FP16) 以及多种低精度（如 INT8、INT5、INT4、INT2）的**模型量化**。量化可以在牺牲极小精度损失的情况下，大幅**减小模型体积，降低内存占用和计算需求**。这对于在手机上运行大型模型至关重要，能让原本无法加载的模型变得可用。GGUF 文件不仅包含模型权重，还打包了模型的词表、超参数、架构信息和特殊 token ID 等所有运行所需的元数据。一个 GGUF 文件就是一个独立的、可运行的模型包。

此外，GGUF 支持内存映射。这意味着操作系统可以直接将文件内容映射到内存中，而无需将整个模型完全复制到 RAM。这大大加快了模型加载速度，并允许在物理内存不足的情况下也能运行大模型（通过操作系统的虚拟内存管理）。

#### 命名约定
GGUF 遵循命名约定， `<BaseName><SizeLabel><FineTune><Version><Encoding><Type><Shard>.gguf` 每个组件之间用 分隔（-如果存在）。这样做的最终目的是让人们能够一目了然地了解模型的最重要细节。

这些组件包括：
* BaseName：模型基础类型或架构的描述性名称。
* SizeLabel：参数权重类（对排行榜有用）表示为`<expertCount>x<count><scale-prefix>` 
* FineTune：模型微调目标的描述性名称（例如聊天、指导等...）
* 版本：（可选）表示模型版本号，格式为`v<Major>.<Minor>`
* 编码：表示应用于模型的权重编码方案。内容、类型组合和排列由用户代码决定，并可能根据项目需求而变化。
* 类型：表示 gguf 文件的类型及其预期用途
* Shard：（可选）表示并表明模型已被拆分为多个分片，格式为`<ShardNum>-of-<ShardTotal>`。

例如：**Mixtral-8x7B-v0.1-KQ2.gguf**：

```
型号名称：Mixtral
专家数量：8
参数数量：7B
版本号：v0.1
权重编码方案：KQ2
```

#### GGUF 的主要特点和优势

1.  **二进制格式，高效存储和加载**：
    * GGUF 文件是二进制的，这意味着它直接存储原始数据，而不是可读的文本格式（如 JSON 或 YAML）。这使得文件更小，并且解析和加载速度更快，这对于在 Android 设备上快速启动模型至关重要。
2.  **广泛的硬件兼容性**：
    * GGUF 格式本身是硬件无关的，但它与 llama.cpp 项目紧密结合，从而能利用 llama.cpp 在 CPU 和多种 GPU（包括 Android 设备的 NPU/DSP）上的优化。
3.  **支持模型量化 (Quantization)**：
    * 这是 GGUF 的核心优势之一。它支持多种精度级别的量化（例如，从全精度 FP32 到 FP16、INT8、INT5、INT4 甚至 INT2）。量化可以大幅**减小模型的文件大小**，同时**降低推理时的内存占用和计算需求**。这对于资源有限的 Android 设备来说至关重要，因为它能让更大的模型运行起来。
    * 例如，一个 70 亿参数的 FP16 模型可能有 13GB 大小，但经过 INT4 量化后可能只有 4GB 左右。
4.  **包含完整的模型元数据**：
    * GGUF 文件不仅仅包含模型的权重，还包含了模型运行所需的各种**元数据**，如：
        * **词表 (Vocabulary)**：模型理解的单词或 token 列表。
        * **超参数 (Hyperparameters)**：如模型层数、注意力头数、隐藏层大小等。
        * **架构信息**：模型是基于 Llama、Mistral 还是其他架构。
        * **特殊 Token ID**：如 EOS (End-Of-Sequence), BOS (Beginning-Of-Sequence) 等。
        * 这些元数据都存储在一个文件中，简化了模型的管理和加载过程。
5.  **内存映射 (Memory Mapping) 支持**：
    * GGUF 文件支持内存映射。这意味着操作系统可以直接将文件内容映射到内存中，而不需要将整个文件完全复制到 RAM。这大大减少了模型的加载时间，并且即使手机物理内存不足以完全容纳模型，也可以通过操作系统管理虚拟内存来运行。
6.  **向后兼容性与可扩展性**：
    * GGUF 格式在设计时考虑了向后兼容性，这意味着即使 llama.cpp 项目更新，旧的 GGUF 文件通常也能正常工作。同时，它也具有良好的可扩展性，可以方便地添加新的特性和信息。
7.  **社区支持和模型转换**：
    * 现在，许多流行的开源大型语言模型（如 Llama 2/3、Mistral、Gemma、Qwen、Phi-2 等）都有社区贡献者将其转换为 GGUF 格式。这意味着您可以轻松找到这些模型的 GGUF 版本，直接用于 llama.cpp 或您的 Android 应用。

#### GGUF 文件的结构
一个 GGUF 文件大致可以分为两个主要部分：

1.  **头部 (Header)**：包含 GGUF 文件的魔数（用于识别文件类型）、版本号以及一些全局元数据（如模型总层数、维度等）。
2.  **元数据 (Metadata) 和张量 (Tensors)**：
    * **KV 对 (Key-Value Pairs)**：存储了模型的各种超参数、架构信息、词表和一些其他配置。这些都是以键值对的形式存储的，方便访问。
    * **张量数据 (Tensor Data)**：这是模型真正的权重数据。每个张量都会有其名称、维度和数据类型（例如，量化后的 INT4、INT8 或 FP16 等）。这些数据会按照特定的对齐方式存储，以确保高效读取。

![](/assets/img/blog/blogs_gguf_file_structure.png)

```cpp
enum ggml_type: uint32_t {
    GGML_TYPE_F32     = 0,
    GGML_TYPE_F16     = 1,
    GGML_TYPE_Q4_0    = 2,
    GGML_TYPE_Q4_1    = 3,
    // GGML_TYPE_Q4_2 = 4, support has been removed
    // GGML_TYPE_Q4_3 = 5, support has been removed
    GGML_TYPE_Q5_0    = 6,
    GGML_TYPE_Q5_1    = 7,
    GGML_TYPE_Q8_0    = 8,
    GGML_TYPE_Q8_1    = 9,
    GGML_TYPE_Q2_K    = 10,
    GGML_TYPE_Q3_K    = 11,
    GGML_TYPE_Q4_K    = 12,
    GGML_TYPE_Q5_K    = 13,
    GGML_TYPE_Q6_K    = 14,
    GGML_TYPE_Q8_K    = 15,
    GGML_TYPE_IQ2_XXS = 16,
    GGML_TYPE_IQ2_XS  = 17,
    GGML_TYPE_IQ3_XXS = 18,
    GGML_TYPE_IQ1_S   = 19,
    GGML_TYPE_IQ4_NL  = 20,
    GGML_TYPE_IQ3_S   = 21,
    GGML_TYPE_IQ2_S   = 22,
    GGML_TYPE_IQ4_XS  = 23,
    GGML_TYPE_I8      = 24,
    GGML_TYPE_I16     = 25,
    GGML_TYPE_I32     = 26,
    GGML_TYPE_I64     = 27,
    GGML_TYPE_F64     = 28,
    GGML_TYPE_IQ1_M   = 29,
    GGML_TYPE_COUNT,
};

enum gguf_metadata_value_type: uint32_t {
    // The value is a 8-bit unsigned integer.
    GGUF_METADATA_VALUE_TYPE_UINT8 = 0,
    // The value is a 8-bit signed integer.
    GGUF_METADATA_VALUE_TYPE_INT8 = 1,
    // The value is a 16-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT16 = 2,
    // The value is a 16-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT16 = 3,
    // The value is a 32-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT32 = 4,
    // The value is a 32-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT32 = 5,
    // The value is a 32-bit IEEE754 floating point number.
    GGUF_METADATA_VALUE_TYPE_FLOAT32 = 6,
    // The value is a boolean.
    // 1-byte value where 0 is false and 1 is true.
    // Anything else is invalid, and should be treated as either the model being invalid or the reader being buggy.
    GGUF_METADATA_VALUE_TYPE_BOOL = 7,
    // The value is a UTF-8 non-null-terminated string, with length prepended.
    GGUF_METADATA_VALUE_TYPE_STRING = 8,
    // The value is an array of other values, with the length and type prepended.
    ///
    // Arrays can be nested, and the length of the array is the number of elements in the array, not the number of bytes.
    GGUF_METADATA_VALUE_TYPE_ARRAY = 9,
    // The value is a 64-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT64 = 10,
    // The value is a 64-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT64 = 11,
    // The value is a 64-bit IEEE754 floating point number.
    GGUF_METADATA_VALUE_TYPE_FLOAT64 = 12,
};

// A string in GGUF.
struct gguf_string_t {
    // The length of the string, in bytes.
    uint64_t len;
    // The string as a UTF-8 non-null-terminated string.
    char string[len];
};

union gguf_metadata_value_t {
    uint8_t uint8;
    int8_t int8;
    uint16_t uint16;
    int16_t int16;
    uint32_t uint32;
    int32_t int32;
    float float32;
    uint64_t uint64;
    int64_t int64;
    double float64;
    bool bool_;
    gguf_string_t string;
    struct {
        // Any value type is valid, including arrays.
        gguf_metadata_value_type type;
        // Number of elements, not bytes
        uint64_t len;
        // The array of values.
        gguf_metadata_value_t array[len];
    } array;
};

struct gguf_metadata_kv_t {
    // The key of the metadata. It is a standard GGUF string, with the following caveats:
    // - It must be a valid ASCII string.
    // - It must be a hierarchical key, where each segment is `lower_snake_case` and separated by a `.`.
    // - It must be at most 2^16-1/65535 bytes long.
    // Any keys that do not follow these rules are invalid.
    gguf_string_t key;

    // The type of the value.
    // Must be one of the `gguf_metadata_value_type` values.
    gguf_metadata_value_type value_type;
    // The value.
    gguf_metadata_value_t value;
};

struct gguf_header_t {
    // Magic number to announce that this is a GGUF file.
    // Must be `GGUF` at the byte level: `0x47` `0x47` `0x55` `0x46`.
    // Your executor might do little-endian byte order, so it might be
    // check for 0x46554747 and letting the endianness cancel out.
    // Consider being *very* explicit about the byte order here.
    uint32_t magic;
    // The version of the format implemented.
    // Must be `3` for version described in this spec, which introduces big-endian support.
    //
    // This version should only be increased for structural changes to the format.
    // Changes that do not affect the structure of the file should instead update the metadata
    // to signify the change.
    uint32_t version;
    // The number of tensors in the file.
    // This is explicit, instead of being included in the metadata, to ensure it is always present
    // for loading the tensors.
    uint64_t tensor_count;
    // The number of metadata key-value pairs.
    uint64_t metadata_kv_count;
    // The metadata key-value pairs.
    gguf_metadata_kv_t metadata_kv[metadata_kv_count];
};

uint64_t align_offset(uint64_t offset) {
    return offset + (ALIGNMENT - (offset % ALIGNMENT)) % ALIGNMENT;
}

struct gguf_tensor_info_t {
    // The name of the tensor. It is a standard GGUF string, with the caveat that
    // it must be at most 64 bytes long.
    gguf_string_t name;
    // The number of dimensions in the tensor.
    // Currently at most 4, but this may change in the future.
    uint32_t n_dimensions;
    // The dimensions of the tensor.
    uint64_t dimensions[n_dimensions];
    // The type of the tensor.
    ggml_type type;
    // The offset of the tensor's data in this file in bytes.
    //
    // This offset is relative to `tensor_data`, not to the start
    // of the file, to make it easier for writers to write the file.
    // Readers should consider exposing this offset relative to the
    // file to make it easier to read the data.
    //
    // Must be a multiple of `ALIGNMENT`. That is, `align_offset(offset) == offset`.
    uint64_t offset;
};

struct gguf_file_t {
    // The header of the file.
    gguf_header_t header;

    // Tensor infos, which can be used to locate the tensor data.
    gguf_tensor_info_t tensor_infos[header.tensor_count];

    // Padding to the nearest multiple of `ALIGNMENT`.
    //
    // That is, if `sizeof(header) + sizeof(tensor_infos)` is not a multiple of `ALIGNMENT`,
    // this padding is added to make it so.
    //
    // This can be calculated as `align_offset(position) - position`, where `position` is
    // the position of the end of `tensor_infos` (i.e. `sizeof(header) + sizeof(tensor_infos)`).
    uint8_t _padding[];

    // Tensor data.
    //
    // This is arbitrary binary data corresponding to the weights of the model. This data should be close
    // or identical to the data in the original model file, but may be different due to quantization or
    // other optimizations for inference. Any such deviations should be recorded in the metadata or as
    // part of the architecture definition.
    //
    // Each tensor's data must be stored within this array, and located through its `tensor_infos` entry.
    // The offset of each tensor's data must be a multiple of `ALIGNMENT`, and the space between tensors
    // should be padded to `ALIGNMENT` bytes.
    uint8_t tensor_data[];
};
```

## 参考资料
[1] 《大规模语言模型从理论到实践》 张奇 桂韬 郑锐 ⻩萱菁 著