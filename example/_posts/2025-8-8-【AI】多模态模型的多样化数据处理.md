---
layout: post
description: > 
  本文介绍了文本模型之外的多模态AI模型如何处理数据的
image: 
  path: /assets/img/blog/blogs_ai_multimodelai_cover.png
  srcset: 
    1920w: /assets/img/blog/blogs_ai_multimodelai_cover.png
    960w:  /assets/img/blog/blogs_ai_multimodelai_cover.png
    480w:  /assets/img/blog/blogs_ai_multimodelai_cover.png
accent_image: /assets/img/blog/blogs_ai_multimodelai_cover.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】多模态模型的多样化数据处理
经过前面若干篇的学习，我了解到LLM是如何处理输入文本，一轮一轮地进行前向推理，最后输出结果反馈的。

那多模态的AI模型，又是如何处理一帧一帧的图像，或者音频数据呢？现对这些不同于文本的数据处理进行一段学习总结。

简单来说，模型通过专门设计的“编码器”（Encoder）将不同类型的数据“翻译”成同一种“语言”——也就是向量。这个过程可以分为两大步：

1.  **独立编码（Independent Encoding）**：每种数据类型（图像、音频）都有一个专门的“专家”编码器，负责将其从原始格式转换成一个初步的向量序列。
2.  **对齐与融合（Alignment & Fusion）**：通过特殊的训练方法，让这些来自不同专家的向量在同一个“语义空间”里对齐，使得“小狗的图片”和“小狗的叫声”以及文字“小狗”的向量在空间中的位置非常接近。

## 向量嵌入模型
向量嵌入（Vector Embedding）模型是当今许多AI应用的基石。

想象一下，你有一个巨大的图书馆，里面有成千上万本书。现在，你想找到所有和“科幻”相关的书。一个笨方法是逐一阅读每一本书的简介。这太慢了。

一个聪明的图书管理员（我们的AI模型）想出了一个好办法：他没有给书贴上“科幻”、“历史”这样的简单标签，而是为每本书在图书馆里分配了一个**精确的三维坐标**（例如，坐标 `[x, y, z]`）。

这个坐标的分配原则是：
* **内容相似的书，在空间中的位置就非常接近**。比如，《三体》和《银河帝国》的坐标可能非常靠近。
* **内容无关的书，在空间中的位置就非常遥远**。比如，《三体》和《莎士比亚戏剧集》的坐标会离得很远。
* **坐标轴本身也代表了某种“意义”**。也许x轴代表“虚构程度”，y轴代表“科技含量”，z轴代表“年代”。

这样一来，找书就变得非常简单：
1.  你告诉管理员你要找“一部关于星际旅行和外星文明的小说”。
2.  管理员将你的需求也转换成一个坐标。
3.  然后，他在图书馆的这个三维空间里，找到离你的需求坐标**最近**的那些书。

在这个比喻里：
* **书/你的需求**：就是我们要处理的数据（单词、句子、图片、商品等）。
* **坐标 `[x, y, z]`**：就是**向量嵌入 (Vector Embedding)**。它是一个由数字组成的数组（向量），代表了原始数据在高维空间中的位置。
* **整个三维空间**：被称为**嵌入空间 (Embedding Space)**。
* **聪明的图书管理员**：就是**向量嵌入模型**。

**核心思想：** 向量嵌入将世界上各种复杂、离散的数据（如文字、图片）转换成计算机可以理解和比较的、连续的、稠密的数字向量，并在这个过程中保留数据的“语义信息”。

### 为什么需要向量嵌入？
计算机不理解“苹果”这个词。它只懂数字。在AI出现之前，我们可能会用 **One-Hot 编码（独热编码）** 来表示单词。

假设我们的词典里只有5个词：[猫, 狗, 苹果, 香蕉, 橙子]。
* 猫：`[1, 0, 0, 0, 0]`
* 狗：`[0, 1, 0, 0, 0]`
* 苹果：`[0, 0, 1, 0, 0]`

这种方法的**巨大缺陷**：
1.  **维度灾难**：如果词典有10万个词，每个词的向量就有10万维，非常稀疏和浪费空间。
2.  **无法表达语义相似性**：从数学上看，`[1, 0, 0]` 和 `[0, 1, 0]` 之间的距离，与 `[1, 0, 0]` 和 `[0, 0, 1]` 之间的距离是完全一样的。也就是说，模型无法知道“猫”和“狗”的关系比“猫”和“苹果”更近。所有词之间都是孤立的。

向量嵌入完美地解决了这两个问题。它使用一个**更低维度（通常是几百到几千维）** 的**稠密向量**来表示数据，并且向量之间的距离和方向能够反映数据之间的语义关系。
### 向量嵌入模型的工作原理
模型是如何学会给每个单词或句子分配一个“有意义”的坐标的呢？答案是：通过在一个巨大的数据集上进行 **“自监督学习”** 。

核心原理可以用一句话概括：**“一个词的意义，由它周围的词来定义”** 。

我们以一个经典的词嵌入模型 **Word2Vec** 为例来解释这个过程。
#### 训练过程（以 Word2Vec 的 Skip-gram 模式为例）：

1.  **准备数据**：获取海量文本，比如整个维基百科。

2.  **建立任务**：我们给模型设定一个任务——**根据一个中心词，预测它周围的词（上下文）**。
    * 例如，在句子 “一只可爱的**猫**正坐在垫子上” 中。
    * 中心词是 “猫”。
    * 上下文是 “一只”、“可爱的”、“正”、“坐在”。

3.  **模型初始化**：
    * 为词典里的每一个词，随机生成一个向量（比如300维）。此时，这些向量是毫无意义的。

4.  **开始训练（迭代学习）**：
    * **输入**：我们把 “猫” 的随机向量输入到一个简单的神经网络中。
    * **预测**：模型会根据这个输入向量，输出一个预测，表示它认为“猫”周围最可能出现哪些词。在训练初期，这个预测肯定是乱七八糟的。
    * **计算误差**：我们将模型的预测结果与真实的上下文（“一只”、“可爱的”等）进行比较，计算出一个**损失（Loss）**或**误差（Error）**。误差越大，说明模型预测得越差。
    * **反向传播与更新**：算法会根据这个误差，微调（更新）神经网络的权重，尤其是“猫”的输入向量。调整的原则是：**让“猫”的向量变得“更擅长”预测出它周围的词**。
    * **重复**：对文本库里的每一个词都重复这个过程亿万次。

5.  **最终结果**：
    * 训练结束后，词典里每个词的向量都经过了无数次的微调。
    * 因为“猫”和“狗”经常出现在相似的上下文中（比如“可爱的__”、“喂养__”、“宠物__”），为了能同时预测好这些上下文，模型会“自发地”将“猫”和“狗”的向量调整到嵌入空间中非常相近的位置。
    * 而“猫”和“苹果”的上下文几乎完全不同，所以它们的向量在空间中就会相距很远。

最终，我们扔掉用于预测的神经网络，只保留训练好的、**包含所有词及其对应向量的那个查找表**。这个表就是我们的**词嵌入模型**。
#### 嵌入的奇妙特性：

训练好的嵌入向量甚至可以捕捉到更复杂的关系，最经典的例子是：

$$\text{vector('King')} - \text{vector('Man')} + \text{vector('Woman')} \approx \text{vector('Queen')}$$

这表明，嵌入空间中的向量方向也蕴含了语义，例如“性别”或“皇室”等抽象概念。
### 著名/主流的嵌入模型
1.  **Word2Vec (Google)**: 开创性的词嵌入模型，简单高效。它包含两种模式：Skip-gram（根据中心词预测上下文）和 CBOW（根据上下文预测中心词）。
2.  **GloVe (Stanford)**: 另一种经典的词嵌入模型，它利用全局词-词共现矩阵来生成嵌入，考虑了全局统计信息。
3.  **BERT (Google) & Transformer-based Models**: 这是现代嵌入模型的主流。
    * **关键区别**：Word2Vec 为每个词生成的向量是**静态的、唯一的**。但在现实中，词的意义随语境而变。例如，“bank”在 “river bank”（河岸）和 “investment bank”（投资银行）中的意思完全不同。
    * BERT这类模型是**上下文相关的（Contextual）**。它在生成一个词的嵌入时，会同时考虑整个句子的信息。因此，同一个词在不同句子中会得到不同的嵌入向量，这极大地提升了表示的准确性。
4.  **OpenAI Embeddings (如 `text-embedding-ada-002`)**: 目前非常流行和强大的通用文本嵌入模型，广泛用于各种AI应用。
5.  **CLIP (OpenAI)**: 一种强大的**多模态嵌入模型**。它可以为一张图片和一个描述该图片的句子生成非常相似的向量。这使得通过文本搜索图片成为可能。

### 应用场景
向量嵌入是许多现代AI系统的“引擎”，它的应用无处不在：

1.  **语义搜索/向量搜索**：
    * 传统的关键字搜索只能匹配字面内容。而向量搜索可以理解查询的“意图”。你搜索“夏天穿的透气鞋子”，它能返回商品名里没有这些词但符合描述的“网面运动凉鞋”。
    * 这是目前**RAG (Retrieval-Augmented Generation，检索增强生成)** 技术的核心，大语言模型通过向量搜索找到相关知识库内容，再进行回答，以减少幻觉。

2.  **推荐系统**：
    * 将用户和商品都嵌入到同一个向量空间中。一个用户的向量，会和他可能喜欢的商品的向量非常接近。通过计算向量相似度，可以为用户推荐他可能感兴趣的商品、电影或音乐。

3.  **文本分类与聚类**：
    * 将文本转换成向量后，可以轻松地使用机器学习算法进行情感分析（正面/负面评论）、新闻主题分类等。相似的文本向量会自然地“聚”在一起。

4.  **问答系统和聊天机器人**：
    * 将用户的问题和知识库中的“问题-答案”对都转换成向量。通过找到与用户问题向量最相似的问题向量，来返回对应的答案。

5.  **图像搜索**：
    * 以图搜图（找到相似图片）或以文搜图（输入“一只猫在草地上”，返回对应的图片）。

## 图像数据
第一步类似于文本模型，首先要理解输入内容物是什么东西。在将图片信息与其他模态（如文本）进行融合之前，模型需要将原始像素数据转换为**有意义的、可供计算的向量表示**，这称为特征提取。

### 数据特征提取
一般通过 **卷积神经网络 (CNN)**，尤其是像 ResNet、VGG 或 ViT (Vision Transformer) 这样的模型架构。
* **CNN 的作用：** CNN 通过多层**卷积**操作，从图片中自动学习和提取层级特征。浅层提取边缘、纹理等基础特征；深层提取鼻子、眼睛、汽车等高层语义特征。
* **Vision Transformer (ViT) 的作用：** ViT 不使用卷积，而是将图像分割成许多小块（称为 **Patch**），然后使用 **Transformer 的自注意力机制**来捕捉这些小块之间的关系，这与处理文本的方式相似，有助于模态间的对齐。

提取器最终输出一个**图像嵌入向量**，它是一个高维向量，浓缩了整张图片或图片中关键区域的语义信息。
### 一、 图像数据的向量化
原始的图像数据是一个由像素值（RGB）构成的三维矩阵（宽 x 高 x 通道）。当前最主流的方法也是使用 **Vision Transformer** 架构来处理它。

**ViT过程拆解：**

1.  **图像分块 (Image Patching)**
    * 模型不会一次性看整个图像的几百万个像素，这计算量太大了。相反，它会像切拼图一样，将原始图像（例如 `224x224` 像素）切割成一系列固定大小的小方块（Patches），比如每个方块是 `16x16` 像素。
    * 这样，一张 `224x224` 的图像就变成了一个由 `(224/16) * (224/16) = 14 * 14 = 196` 个小方块组成的**序列**。

2.  **展平与线性投射 (Flatten & Linear Projection)**
    * 将每个 `16x16x3` (3是RGB通道) 的小方块展平，变成一个长向量。
    * 然后，通过一个可学习的线性投射层（Linear Projection Layer），将这个长向量映射（降维或升维）到一个固定的维度，比如768维。
    * 现在，我们就得到了一个由196个768维向量组成的序列。这在结构上就和经过词嵌入的句子（由多个词向量组成的序列）非常相似了！

3.  **加入位置编码 (Positional Encoding)**
    * 和文本一样，这些图像块的相对位置非常重要（“耳朵”在“头”的上面）。因此，模型会为每个图像块向量加入一个**位置编码向量**，来告诉模型每个小块的原始位置信息。

4.  **通过 Transformer 编码器 (Transformer Encoder)**
    * 将这个带有位置信息的向量序列输入到一个标准的 Transformer 编码器中。
    * 编码器内部的 **自注意力机制（Self-Attention）** 会让每个图像块去“关注”其他的图像块，从而理解它们之间的关系和全局结构。例如，一个代表“车轮”的图像块会和代表“车身”的图像块建立强关联。
    * 经过多层Transformer Block的处理后，模型就得到了对整个图像内容和结构的深度理解。

5.  **输出最终向量**
    * 通常会借鉴BERT中的 `[CLS]` 思想，在图像块序列的最前面添加一个特殊的 `[CLASS]` 向量。在经过Transformer编码器后，这个 `[CLASS]` 向量对应的最终输出向量，就被认为是代表**整个图像语义**的聚合向量。

**最终，一张复杂的图像就被转换成了一个单一的、高维的、包含丰富语义的向量（例如768维）。**

## 音频数据
原始的音频数据是一维的波形信号（Waveform），它记录了随时间变化的振幅。直接处理这个长序列非常困难。因此，标准做法是先将其转换成一种“像图像一样”的二维表示。

### 预处理：波形转频谱图
音频的核心信息在于不同频率的声音随时间如何变化。通过 **短时傅里叶变换（STFT）** 将原始的一维波形转换成一个 **频谱图（Spectrogram）** 。

这个频谱图是一个二维图像：
* **X轴** 代表 **时间**
* **Y轴** 代表 **频率**
* **颜色/亮度** 代表该频率在该时间的 **能量（音量）**
通常会使用**梅尔频谱图（Mel-Spectrogram）**，因为它更贴近人耳对频率的感知方式。通过这个转换之后，音频数据就变成了一张“图像”！

### 使用类似图像的处理方法
一旦我们有了频谱图这个二维表示，接下来的处理就和上面图像处理的流程非常相似了。模型（例如 Audio Spectrogram Transformer, AST）也会将这张频谱图切割成一系列的**小方块（Patches）**。同样地，对这些方块进行**线性投射**、加入**位置编码**，然后将它们组成的序列送入一个 **Transformer 编码器**。Transformer的自注意力机制能够捕捉音频序列中长距离的依赖关系，类似于理解一句话中前后词语的语境。

### 输出最终向量
与ViT类似，经过Transformer编码器处理后，模型会输出一个代表整个音频片段语义的聚合向量。这个向量捕捉了音频中的内容，比如是人声（说了什么）、音乐（什么风格）还是环境音（狗叫、汽车声）。

## 图像和音频的模态对齐融合
现在我们有了图像向量、音频向量和文本向量（由文本编码器如BERT生成）。但此时它们还处在各自的“世界”里，无法直接比较。让它们统一到同一个语义空间的关键技术是**对比学习（Contrastive Learning）**。这是多模态理解的核心。模型需要学会这些音频向量和文本/视觉向量之间的关系。

以 **CLIP (Contrastive Language-Image Pre-training)** 模型为例，它就是专门用来对齐图像和文本的：
* **数据输入** ：收集数亿个 `(图像, 文本描述)` 的配对数据。
* **训练目标** ：在训练过程中，模型会看到大量的“音频-文本” 键值对，例如： **“一段狗叫声”** 和文本 **“一只狗在叫”** 。将一个图像和它 **正确匹配** 的文本描述分别通过各自的编码器，得到 `image_vector` 和 `text_vector`。模型的目标是 **拉近（Maximize Similarity）** 这对正样本（matched pair）向量的相似度（例如，余弦相似度）。同时，对于一个图像， batch里的所有**其他**文本描述都是负样本（unmatched pairs）。模型的目标是**推远（Minimize Similarity）** 这个图像向量和所有这些错误文本向量的相似度。

最终，“狗叫”的音频向量和“狗叫”的文本向量在语义空间中的位置会非常接近。通过在这种“连连看”式的任务上进行大规模训练，图像编码器和文本编码器会“被迫”学会一种共识。它们会自发地将**语义上相似**的概念映射到向量空间中的**邻近区域**，无论这个概念是来自图片还是文字。

输入一张 **“金毛犬在草地上玩耍”** 的图片所生成的向量，会和句子 **“a golden retriever playing on the grass”** 生成的向量在空间上非常非常接近。

这个对齐过程同样适用于音频。通过训练 `(音频, 文本描述)` 配对数据，音频编码器也能学会将“狗叫声”的音频片段映射到和文字“dog barking”相近的空间位置。

