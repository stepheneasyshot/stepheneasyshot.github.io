---
layout: post
description: > 
  本文介绍了AI领域RAG增强检索生成技术的介绍和实操记录
image: 
  path: /assets/img/blog/blogs_cover_rag.png
  srcset: 
    1920w: /assets/img/blog/blogs_cover_rag.png
    960w:  /assets/img/blog/blogs_cover_rag.png
    480w:  /assets/img/blog/blogs_cover_rag.png
accent_image: /assets/img/blog/blogs_cover_rag.png
excerpt_separator: <!--more-->
sitemap: false
---
# 【AI】RAG技术介绍与实操
> 从零开始构建本地RAG系统：基于Ollama的DeepSeek-R1与Nomic-Embedd实战

大语言模型的出现改变了我们与信息交互的方式，但这些模型的知识局限于训练数据的截止日期，无法覆盖实时信息和企业私有文档。检索增强生成（Retrieval-Augmented Generation, RAG）通过为模型配备外部知识库，巧妙地解决了这一难题。本文将带领你从零开始，使用Ollama平台拉取DeepSeek-R1推理模型和Nomic-Embed-Text向量化模型，构建一个完整的本地RAG系统。

## 一、RAG的出现：解决大模型的知识困境

### 1.1 大模型的固有局限

大型语言模型虽然在语言理解和生成方面表现惊艳，但它们存在两个根本性缺陷：

**知识时效性问题**：模型的训练数据有严格的截止日期，对于新近发生的事件或最新研究成果一无所知。GPT-4的知识截止于2023年，当你询问2024年的新闻时，它要么无法回答，要么提供过时信息。

**私有数据缺失问题**：公开训练数据无法覆盖企业的内部文档、产品手册或个人知识库。如果你想让模型回答关于公司内部流程的问题，仅靠通用模型是无能为力的。

更严重的是，当模型面对超出其知识范围的问题时，它不会坦诚地说"不知道"，而是会**产生"幻觉"（Hallucination）**——自信地编造出看似合理但完全错误的信息。这在企业应用中是不可接受的。

### 1.2 RAG的核心思想：开卷考试

RAG的核心理念很简单：**在回答问题前，先让模型"查阅资料"**。就像开卷考试允许学生翻阅教材，RAG系统在执行生成任务前，会从一个外部知识库中检索出最相关的信息，然后将这些信息作为上下文提供给模型。

这种方法带来的优势是显著的：
- **事实准确性提升**：模型的回答基于检索到的真实信息，而非凭空编造
- **知识可更新**：只需更新知识库，无需重新训练模型
- **来源可追溯**：系统可以提供答案的信息来源，增强可信度
- **领域适应性**：通过更换知识库，同一个模型可以适配不同专业领域

RAG最早由Facebook AI Research在2020年提出，他们使用维基百科作为外部知识库，通过Dense Passage Retrieval（DPR）技术检索相关文本片段，然后输入给BART生成模型。自那时起，RAG迅速普及，成为AI应用开发的核心范式。

## 二、RAG向量化流程详解

RAG系统的核心在于将非结构化文本转换为机器可计算的向量表示。这一过程涉及文档加载、文本切分、向量化生成和向量存储四个关键步骤。

### 2.1 文档加载与切分

原始知识库通常是各种格式的文档——TXT、PDF、Markdown或网页。构建RAG的第一步是加载这些文档并将其切分成适合检索的文本块。

**为什么需要切分？** 大模型有上下文长度限制，直接将整本手册或整篇文章作为上下文既不现实也不高效。切分的目标是：**每个文本块应包含相对完整的语义单元**。

切分策略有多种选择：

**按换行符切分**是最简单直接的方式，适用于行结构清晰的文档：
```python
def split_content(content):
    chunks = []
    lines = content.splitlines()
    for line in lines:
        if line.strip():  # 忽略空行
            chunks.append(line)
    return chunks
```

**递归字符文本切分**更智能，它会尝试按段落、句子、单词的顺序逐步切分，尽可能保持语义完整性。实践中，chunk_size通常设为500-1000个字符，chunk_overlap设为50-100个字符，以保留上下文连贯性。

### 2.2 向量化：文本的数学表达

切分后的文本块需要转换为计算机可以理解和计算的形式——**向量（Vector）**，也称为**嵌入（Embedding）**。

向量的本质可以这样理解：想象我们要给每段文字拍一张特殊的"照片"，这张照片由几百个数字组成（如768个维度）。这张"照片"就是该段文字的"语义身份证"——语义相近的文字，它们的"照片"在数字空间中的距离也会很近。

这个转换过程由**嵌入模型（Embedding Model）** 完成。嵌入模型经过海量文本训练，学会了将文字映射到高维语义空间。在本文的实践中，我们将使用Ollama拉取的`nomic-embed-text`模型，它生成的向量维度为768维。

向量化的核心原则是：**语义相似，向量相近**。这意味着：
- "苹果是一种水果"和"香蕉富含钾元素"的向量距离较近
- "苹果是一种水果"和"Python是一种编程语言"的向量距离较远

### 2.3 向量存储

生成向量后，我们需要一个专门的基础设施来存储和检索这些向量。虽然传统数据库（如MySQL）也能存储向量，但它们无法进行高效的相似性搜索。

**向量数据库（Vector Database）** 专为存储和查询高维向量而设计，内置了高效的**近似最近邻（ANN）搜索算法**，可以在毫秒级内从数百万向量中找出最相似的几个。

常见的向量数据库包括FAISS、ChromaDB、Milvus等。在本文的实践中，考虑到我们追求理解原理而非生产部署，将使用FAISS或甚至直接使用NumPy数组存储向量——这足以让我们看清RAG的本质。

## 三、RAG推理检索流程

当用户提出问题后，RAG系统进入在线推理阶段。这一阶段包括查询向量化、相似度检索、结果重排和提示词构建四个环节。

### 3.1 查询向量化与相似度检索

用户输入的问题同样需要转换为向量——**必须使用与知识库向量化完全相同的嵌入模型**。这样才能确保查询向量和文档向量位于同一个语义空间，距离计算才有意义。

接下来，系统计算查询向量与知识库中所有文档向量的**相似度**。最常用的相似度指标是**余弦相似度（Cosine Similarity）**：

```python
def similarity(e1, e2):
    # 计算余弦相似度
    dot_product = np.dot(e1, e2)                     # 点乘
    norm_e1 = np.linalg.norm(e1)                     # 向量1的范数
    norm_e2 = np.linalg.norm(e2)                     # 向量2的范数
    cosine_sim = dot_product / (norm_e1 * norm_e2)   # 余弦相似度
    return cosine_sim
```

余弦相似度的取值范围是[-1, 1]，值越大表示两个向量的夹角越小，语义越相近。

计算出所有相似度后，系统按分数从高到低排序，取前K个（通常K=3~5）最相似的文本块作为检索结果。

### 3.2 结果重排（可选优化）

基础的向量检索存在一个潜在问题：**排名靠前的文档不一定是最有用的**。检索阶段使用的是**双编码器（Bi-Encoder）** 架构，它将查询和文档分别编码，速度快但精度有限。

为了提高质量，可以引入**重排（Reranking）** 阶段。重排使用**交叉编码器（Cross-Encoder）** 架构，它将查询和文档拼接后一起输入模型，计算相关性得分，精度更高但速度慢。因此，典型的策略是：向量检索先快速召回Top 100，重排模型再从中精筛Top 5。

在本文的简化实现中，我们将跳过重排步骤，直接使用向量检索结果。

### 3.3 提示词构建与生成

检索到相关文档后，需要将它们与用户问题组装成一个结构化的提示词（Prompt），然后发送给大语言模型。

最简单的提示词模板如下：
```python
prompt_template = """
基于以下知识回答问题：

知识：
1: %s
2: %s
3: %s
4: %s
5: %s

问题：%s

请基于上述知识给出准确、详细的回答。如果知识中不包含相关信息，请明确说明。
"""
```

这个模板的设计原则是：
- **明确信息来源**：告知模型知识是从哪里来的
- **限定回答范围**：要求模型"基于知识"回答，减少幻觉
- **允许不知道**：为模型提供"不知道"的出口，避免编造

将组装好的提示词发送给大语言模型（本文使用DeepSeek-R1），模型生成的回答就是最终输出。

## 四、输出整理与优化

RAG系统的输出并非终点，还需要进行整理和可能的优化。

### 4.1 引用来源

高质量的RAG系统应该在回答中**标注信息来源**。这类似于学术论文的脚注，用户可以核实每个事实的真实性。在实践中，可以在返回检索结果时保留文档的元数据（如文件名、段落位置），然后在生成回答时要求模型引用来源编号。

### 4.2 查询重写（高级优化）

用户提问往往含糊不清或包含隐含意图。例如，用户问"告诉我NVIDIA模型的最新更新"，可能暗中对特定功能感兴趣，但这种偏好没有被明确表达。

**查询重写（Query Rewriting）** 技术可以在检索前优化用户查询，弥合用户提问方式与知识库信息结构之间的语义差距。常用方法包括：

- **Q2E（Query2Expand）**：生成同义词和相关短语，扩展查询
- **Q2D（Query2Doc）**：根据查询构建伪文档，匹配文档风格
- **CoT（思维链）查询重写**：让模型逐步推理，分解查询意图

研究表明，使用Llama 3.3 Nemotron Super 49B进行CoT查询重写后，检索准确率@10从43.1%提升至63.8%。

## 五、串联运行：完整Python脚本实现

现在，让我们将所有环节串联起来，编写一个完整的RAG系统Python脚本。本实现将**最小化第三方框架依赖**，仅使用Ollama和NumPy，确保你能够看清每个步骤的本质。

### 5.1 环境准备

首先，确保已安装Ollama并拉取所需模型：

```bash
# 安装Ollama（请访问ollama.com下载对应系统版本）

# 拉取模型
ollama pull deepseek-r1:8b
ollama pull nomic-embed-text

# 安装Python依赖
pip install ollama numpy
```

### 5.2 完整代码实现

```python
"""
本地RAG系统实现
基于Ollama + DeepSeek-R1 + Nomic-Embed-Text
完全手动实现检索流程，无框架依赖
"""

import numpy as np
from ollama import embeddings, chat
from typing import List, Tuple
import os


class KnowledgeBase:
    """知识库类：负责文档加载、切分、向量化和检索"""
    
    def __init__(self, filepath: str):
        """
        初始化知识库
        Args:
            filepath: 知识库文件路径
        """
        print(f"正在加载知识库: {filepath}")
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 文档切分
        self.docs = self._split_content(content)
        print(f"切分完成，共 {len(self.docs)} 个文本块")
        
        # 向量化存储
        print("正在向量化文档（这可能需要几秒钟）...")
        self.embeddings = self._encode_docs(self.docs)
        print(f"向量化完成，向量维度: {self.embeddings.shape[1]}")
    
    def _split_content(self, content: str) -> List[str]:
        """
        按换行符切分文档
        这是最简单的切分方式，实际生产环境可使用更智能的切分策略
        """
        chunks = []
        lines = content.splitlines()
        for line in lines:
            if line.strip():  # 忽略空行
                chunks.append(line.strip())
        return chunks
    
    def _encode_docs(self, texts: List[str]) -> np.ndarray:
        """
        将文档列表批量转换为向量
        """
        embeds = []
        for text in texts:
            response = embeddings(model='nomic-embed-text', prompt=text)
            embeds.append(response['embedding'])
        return np.array(embeds)
    
    @staticmethod
    def _cosine_similarity(e1: np.ndarray, e2: np.ndarray) -> float:
        """
        计算两个向量的余弦相似度
        """
        dot_product = np.dot(e1, e2)
        norm_e1 = np.linalg.norm(e1)
        norm_e2 = np.linalg.norm(e2)
        return dot_product / (norm_e1 * norm_e2)
    
    def search(self, query: str, top_k: int = 5) -> List[str]:
        """
        检索与查询最相关的文档块
        Args:
            query: 用户查询
            top_k: 返回结果数量
        Returns:
            最相关的文档块列表
        """
        # 查询向量化
        query_embed = embeddings(model='nomic-embed-text', prompt=query)['embedding']
        query_embed = np.array(query_embed)
        
        # 计算所有相似度
        similarities = []
        for idx, doc_embed in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embed, doc_embed)
            similarities.append((idx, sim))
        
        # 排序取top_k
        similarities.sort(key=lambda x: x[1], reverse=True)
        best_matches = [self.docs[idx] for idx, _ in similarities[:top_k]]
        
        # 打印检索结果（调试用）
        print(f"\n【检索结果】查询: {query}")
        for i, (idx, sim) in enumerate(similarities[:top_k]):
            print(f"  [{i+1}] 相似度: {sim:.4f} | {self.docs[idx][:50]}...")
        
        return best_matches


class RAGSystem:
    """RAG系统类：整合知识库和大语言模型"""
    
    def __init__(self, model_name: str, knowledge_base: KnowledgeBase):
        """
        初始化RAG系统
        Args:
            model_name: Ollama中的模型名称（如 'deepseek-r1'）
            knowledge_base: 知识库实例
        """
        self.model = model_name
        self.kb = knowledge_base
        self.prompt_template = """
你是一个专业的问答助手。请基于以下提供的知识片段回答用户的问题。

知识片段：
1. {context_1}
2. {context_2}
3. {context_3}
4. {context_4}
5. {context_5}

用户问题：{query}

回答要求：
1. 严格基于上述知识片段回答问题，不要编造不存在的信息
2. 如果知识片段中不包含相关信息，请明确说明“根据提供的知识，我无法回答这个问题”
3. 回答要详细、准确、有条理
4. 可以适当整合多个知识片段中的信息

回答："""
    
    def chat(self, query: str) -> str:
        """
        处理用户查询：检索 -> 增强 -> 生成
        """
        # 1. 检索相关文档
        context_docs = self.kb.search(query, top_k=5)
        
        # 2. 构建增强提示词
        prompt = self.prompt_template.format(
            context_1=context_docs[0] if len(context_docs) > 0 else "无相关知识",
            context_2=context_docs[1] if len(context_docs) > 1 else "无相关知识",
            context_3=context_docs[2] if len(context_docs) > 2 else "无相关知识",
            context_4=context_docs[3] if len(context_docs) > 3 else "无相关知识",
            context_5=context_docs[4] if len(context_docs) > 4 else "无相关知识",
            query=query
        )
        
        # 3. 调用大模型生成回答
        print("\n【发送给模型的提示词】")
        print("-" * 50)
        print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
        print("-" * 50)
        
        response = chat(model=self.model, messages=[
            {
                'role': 'user',
                'content': prompt
            }
        ])
        
        return response['message']['content']


def main():
    """主函数：演示RAG系统使用"""
    
    # 配置
    kb_file = "knowledge_base.txt"  # 你的知识库文件
    llm_model = "deepseek-r1:8b"    # 推理模型
    
    # 检查知识库文件是否存在
    if not os.path.exists(kb_file):
        print(f"错误：知识库文件 {kb_file} 不存在")
        print("请创建该文件并写入一些文本内容")
        return
    
    # 初始化知识库
    print("=" * 60)
    print("初始化RAG系统...")
    print("=" * 60)
    kb = KnowledgeBase(kb_file)
    
    # 初始化RAG系统
    rag = RAGSystem(llm_model, kb)
    
    # 交互式问答
    print("\n" + "=" * 60)
    print("RAG系统已就绪！输入问题开始问答（输入 'exit' 退出）")
    print("=" * 60)
    
    while True:
        query = input("\n【用户】: ").strip()
        if query.lower() in ['exit', 'quit', '退出']:
            print("再见！")
            break
        
        if not query:
            continue
        
        print("\n【系统】正在思考...")
        response = rag.chat(query)
        print(f"\n【助手】: {response}")


if __name__ == "__main__":
    main()
```

### 5.3 使用说明

1. **准备知识库文件**：创建一个名为`knowledge_base.txt`的文本文件，写入你的知识内容（每行一个段落或句子）。

2. **运行脚本**：
```bash
python rag_demo.py
```

3. **开始提问**：输入问题，观察RAG系统如何基于知识库回答。

### 5.4 代码解析

这个实现的核心思想是**最小化抽象，最大化可理解性**：

- **KnowledgeBase类**：封装了文档加载、切分、向量化和检索的全过程。`_encode_docs`方法调用Ollama的embeddings接口将文本转换为向量；`search`方法计算余弦相似度并返回最相关的文档块。

- **RAGSystem类**：负责将检索结果与用户问题组装成提示词，并调用大模型生成回答。提示词模板明确要求模型"基于知识片段回答问题"。

- **主循环**：提供交互式问答界面，让用户实时体验RAG的效果。

## 结语

本文从RAG的出现原因出发，详细讲解了向量化流程、推理检索流程和输出整理优化，并最终给出了一个完整、可运行的本地RAG系统实现。通过这个实战项目，你可以清晰地看到：RAG的本质就是**检索 + 上下文增强 + 生成**这三步曲。

当你运行这个系统，输入"健康的水果推荐"，即使知识库中从未出现"推荐"二字，系统也能通过语义相似度找到苹果、香蕉、橙子的描述——这就是向量检索的魅力。当你看到大模型基于检索到的知识给出准确回答，而不是凭空编造时，你就理解了RAG为什么能成为AI应用开发的基石。

从简单的流水线到自主的Agentic RAG，这项技术仍在快速演进。但无论多复杂的系统，其核心都是本文揭示的这些基本原理。希望这篇文章能帮助你打下坚实的基础，在RAG的世界里走得更远。