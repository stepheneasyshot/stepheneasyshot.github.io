---
layout: post
description: > 
  æœ¬æ–‡ä»‹ç»äº†AIé¢†åŸŸRAGå¢å¼ºæ£€ç´¢ç”ŸæˆæŠ€æœ¯çš„ä»‹ç»å’Œå®æ“è®°å½•
image: 
  path: /assets/img/blog/blogs_cover_rag.png
  srcset: 
    1920w: /assets/img/blog/blogs_cover_rag.png
    960w:  /assets/img/blog/blogs_cover_rag.png
    480w:  /assets/img/blog/blogs_cover_rag.png
accent_image: /assets/img/blog/blogs_cover_rag.png
excerpt_separator: <!--more-->
sitemap: false
---
# ã€AIã€‘RAGæŠ€æœ¯ä»‹ç»ä¸å®æ“
> ä»é›¶å¼€å§‹æ„å»ºæœ¬åœ°RAGç³»ç»Ÿï¼šåŸºäºOllamaçš„DeepSeek-R1ä¸Nomic-Embeddå®æˆ˜

å¤§è¯­è¨€æ¨¡å‹çš„å‡ºç°æ”¹å˜äº†æˆ‘ä»¬ä¸ä¿¡æ¯äº¤äº’çš„æ–¹å¼ï¼Œä½†è¿™äº›æ¨¡å‹çš„çŸ¥è¯†å±€é™äºè®­ç»ƒæ•°æ®çš„æˆªæ­¢æ—¥æœŸï¼Œæ— æ³•è¦†ç›–å®æ—¶ä¿¡æ¯å’Œä¼ä¸šç§æœ‰æ–‡æ¡£ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰é€šè¿‡ä¸ºæ¨¡å‹é…å¤‡å¤–éƒ¨çŸ¥è¯†åº“ï¼Œå·§å¦™åœ°è§£å†³äº†è¿™ä¸€éš¾é¢˜ã€‚æœ¬æ–‡å°†å¸¦é¢†ä½ ä»é›¶å¼€å§‹ï¼Œä½¿ç”¨Ollamaå¹³å°æ‹‰å–DeepSeek-R1æ¨ç†æ¨¡å‹å’ŒNomic-Embed-Textå‘é‡åŒ–æ¨¡å‹ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„æœ¬åœ°RAGç³»ç»Ÿã€‚

## ä¸€ã€RAGçš„å‡ºç°ï¼šè§£å†³å¤§æ¨¡å‹çš„çŸ¥è¯†å›°å¢ƒ

### 1.1 å¤§æ¨¡å‹çš„å›ºæœ‰å±€é™

å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°æƒŠè‰³ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¤ä¸ªæ ¹æœ¬æ€§ç¼ºé™·ï¼š

**çŸ¥è¯†æ—¶æ•ˆæ€§é—®é¢˜**ï¼šæ¨¡å‹çš„è®­ç»ƒæ•°æ®æœ‰ä¸¥æ ¼çš„æˆªæ­¢æ—¥æœŸï¼Œå¯¹äºæ–°è¿‘å‘ç”Ÿçš„äº‹ä»¶æˆ–æœ€æ–°ç ”ç©¶æˆæœä¸€æ— æ‰€çŸ¥ã€‚GPT-4çš„çŸ¥è¯†æˆªæ­¢äº2023å¹´ï¼Œå½“ä½ è¯¢é—®2024å¹´çš„æ–°é—»æ—¶ï¼Œå®ƒè¦ä¹ˆæ— æ³•å›ç­”ï¼Œè¦ä¹ˆæä¾›è¿‡æ—¶ä¿¡æ¯ã€‚

**ç§æœ‰æ•°æ®ç¼ºå¤±é—®é¢˜**ï¼šå…¬å¼€è®­ç»ƒæ•°æ®æ— æ³•è¦†ç›–ä¼ä¸šçš„å†…éƒ¨æ–‡æ¡£ã€äº§å“æ‰‹å†Œæˆ–ä¸ªäººçŸ¥è¯†åº“ã€‚å¦‚æœä½ æƒ³è®©æ¨¡å‹å›ç­”å…³äºå…¬å¸å†…éƒ¨æµç¨‹çš„é—®é¢˜ï¼Œä»…é é€šç”¨æ¨¡å‹æ˜¯æ— èƒ½ä¸ºåŠ›çš„ã€‚

æ›´ä¸¥é‡çš„æ˜¯ï¼Œå½“æ¨¡å‹é¢å¯¹è¶…å‡ºå…¶çŸ¥è¯†èŒƒå›´çš„é—®é¢˜æ—¶ï¼Œå®ƒä¸ä¼šå¦è¯šåœ°è¯´"ä¸çŸ¥é“"ï¼Œè€Œæ˜¯ä¼š**äº§ç”Ÿ"å¹»è§‰"ï¼ˆHallucinationï¼‰**â€”â€”è‡ªä¿¡åœ°ç¼–é€ å‡ºçœ‹ä¼¼åˆç†ä½†å®Œå…¨é”™è¯¯çš„ä¿¡æ¯ã€‚è¿™åœ¨ä¼ä¸šåº”ç”¨ä¸­æ˜¯ä¸å¯æ¥å—çš„ã€‚

### 1.2 RAGçš„æ ¸å¿ƒæ€æƒ³ï¼šå¼€å·è€ƒè¯•

RAGçš„æ ¸å¿ƒç†å¿µå¾ˆç®€å•ï¼š**åœ¨å›ç­”é—®é¢˜å‰ï¼Œå…ˆè®©æ¨¡å‹"æŸ¥é˜…èµ„æ–™"**ã€‚å°±åƒå¼€å·è€ƒè¯•å…è®¸å­¦ç”Ÿç¿»é˜…æ•™æï¼ŒRAGç³»ç»Ÿåœ¨æ‰§è¡Œç”Ÿæˆä»»åŠ¡å‰ï¼Œä¼šä»ä¸€ä¸ªå¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢å‡ºæœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™æ¨¡å‹ã€‚

è¿™ç§æ–¹æ³•å¸¦æ¥çš„ä¼˜åŠ¿æ˜¯æ˜¾è‘—çš„ï¼š
- **äº‹å®å‡†ç¡®æ€§æå‡**ï¼šæ¨¡å‹çš„å›ç­”åŸºäºæ£€ç´¢åˆ°çš„çœŸå®ä¿¡æ¯ï¼Œè€Œéå‡­ç©ºç¼–é€ 
- **çŸ¥è¯†å¯æ›´æ–°**ï¼šåªéœ€æ›´æ–°çŸ¥è¯†åº“ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹
- **æ¥æºå¯è¿½æº¯**ï¼šç³»ç»Ÿå¯ä»¥æä¾›ç­”æ¡ˆçš„ä¿¡æ¯æ¥æºï¼Œå¢å¼ºå¯ä¿¡åº¦
- **é¢†åŸŸé€‚åº”æ€§**ï¼šé€šè¿‡æ›´æ¢çŸ¥è¯†åº“ï¼ŒåŒä¸€ä¸ªæ¨¡å‹å¯ä»¥é€‚é…ä¸åŒä¸“ä¸šé¢†åŸŸ

RAGæœ€æ—©ç”±Facebook AI Researchåœ¨2020å¹´æå‡ºï¼Œä»–ä»¬ä½¿ç”¨ç»´åŸºç™¾ç§‘ä½œä¸ºå¤–éƒ¨çŸ¥è¯†åº“ï¼Œé€šè¿‡Dense Passage Retrievalï¼ˆDPRï¼‰æŠ€æœ¯æ£€ç´¢ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼Œç„¶åè¾“å…¥ç»™BARTç”Ÿæˆæ¨¡å‹ã€‚è‡ªé‚£æ—¶èµ·ï¼ŒRAGè¿…é€Ÿæ™®åŠï¼Œæˆä¸ºAIåº”ç”¨å¼€å‘çš„æ ¸å¿ƒèŒƒå¼ã€‚

## äºŒã€RAGå‘é‡åŒ–æµç¨‹è¯¦è§£

RAGç³»ç»Ÿçš„æ ¸å¿ƒåœ¨äºå°†éç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸ºæœºå™¨å¯è®¡ç®—çš„å‘é‡è¡¨ç¤ºã€‚è¿™ä¸€è¿‡ç¨‹æ¶‰åŠæ–‡æ¡£åŠ è½½ã€æ–‡æœ¬åˆ‡åˆ†ã€å‘é‡åŒ–ç”Ÿæˆå’Œå‘é‡å­˜å‚¨å››ä¸ªå…³é”®æ­¥éª¤ã€‚

### 2.1 æ–‡æ¡£åŠ è½½ä¸åˆ‡åˆ†

åŸå§‹çŸ¥è¯†åº“é€šå¸¸æ˜¯å„ç§æ ¼å¼çš„æ–‡æ¡£â€”â€”TXTã€PDFã€Markdownæˆ–ç½‘é¡µã€‚æ„å»ºRAGçš„ç¬¬ä¸€æ­¥æ˜¯åŠ è½½è¿™äº›æ–‡æ¡£å¹¶å°†å…¶åˆ‡åˆ†æˆé€‚åˆæ£€ç´¢çš„æ–‡æœ¬å—ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦åˆ‡åˆ†ï¼Ÿ** å¤§æ¨¡å‹æœ‰ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œç›´æ¥å°†æ•´æœ¬æ‰‹å†Œæˆ–æ•´ç¯‡æ–‡ç« ä½œä¸ºä¸Šä¸‹æ–‡æ—¢ä¸ç°å®ä¹Ÿä¸é«˜æ•ˆã€‚åˆ‡åˆ†çš„ç›®æ ‡æ˜¯ï¼š**æ¯ä¸ªæ–‡æœ¬å—åº”åŒ…å«ç›¸å¯¹å®Œæ•´çš„è¯­ä¹‰å•å…ƒ**ã€‚

åˆ‡åˆ†ç­–ç•¥æœ‰å¤šç§é€‰æ‹©ï¼š

**æŒ‰æ¢è¡Œç¬¦åˆ‡åˆ†**æ˜¯æœ€ç®€å•ç›´æ¥çš„æ–¹å¼ï¼Œé€‚ç”¨äºè¡Œç»“æ„æ¸…æ™°çš„æ–‡æ¡£ï¼š
```python
def split_content(content):
    chunks = []
    lines = content.splitlines()
    for line in lines:
        if line.strip():  # å¿½ç•¥ç©ºè¡Œ
            chunks.append(line)
    return chunks
```

**é€’å½’å­—ç¬¦æ–‡æœ¬åˆ‡åˆ†**æ›´æ™ºèƒ½ï¼Œå®ƒä¼šå°è¯•æŒ‰æ®µè½ã€å¥å­ã€å•è¯çš„é¡ºåºé€æ­¥åˆ‡åˆ†ï¼Œå°½å¯èƒ½ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚å®è·µä¸­ï¼Œchunk_sizeé€šå¸¸è®¾ä¸º500-1000ä¸ªå­—ç¬¦ï¼Œchunk_overlapè®¾ä¸º50-100ä¸ªå­—ç¬¦ï¼Œä»¥ä¿ç•™ä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚

### 2.2 å‘é‡åŒ–ï¼šæ–‡æœ¬çš„æ•°å­¦è¡¨è¾¾

åˆ‡åˆ†åçš„æ–‡æœ¬å—éœ€è¦è½¬æ¢ä¸ºè®¡ç®—æœºå¯ä»¥ç†è§£å’Œè®¡ç®—çš„å½¢å¼â€”â€”**å‘é‡ï¼ˆVectorï¼‰**ï¼Œä¹Ÿç§°ä¸º**åµŒå…¥ï¼ˆEmbeddingï¼‰**ã€‚

å‘é‡çš„æœ¬è´¨å¯ä»¥è¿™æ ·ç†è§£ï¼šæƒ³è±¡æˆ‘ä»¬è¦ç»™æ¯æ®µæ–‡å­—æ‹ä¸€å¼ ç‰¹æ®Šçš„"ç…§ç‰‡"ï¼Œè¿™å¼ ç…§ç‰‡ç”±å‡ ç™¾ä¸ªæ•°å­—ç»„æˆï¼ˆå¦‚768ä¸ªç»´åº¦ï¼‰ã€‚è¿™å¼ "ç…§ç‰‡"å°±æ˜¯è¯¥æ®µæ–‡å­—çš„"è¯­ä¹‰èº«ä»½è¯"â€”â€”è¯­ä¹‰ç›¸è¿‘çš„æ–‡å­—ï¼Œå®ƒä»¬çš„"ç…§ç‰‡"åœ¨æ•°å­—ç©ºé—´ä¸­çš„è·ç¦»ä¹Ÿä¼šå¾ˆè¿‘ã€‚

è¿™ä¸ªè½¬æ¢è¿‡ç¨‹ç”±**åµŒå…¥æ¨¡å‹ï¼ˆEmbedding Modelï¼‰** å®Œæˆã€‚åµŒå…¥æ¨¡å‹ç»è¿‡æµ·é‡æ–‡æœ¬è®­ç»ƒï¼Œå­¦ä¼šäº†å°†æ–‡å­—æ˜ å°„åˆ°é«˜ç»´è¯­ä¹‰ç©ºé—´ã€‚åœ¨æœ¬æ–‡çš„å®è·µä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Ollamaæ‹‰å–çš„`nomic-embed-text`æ¨¡å‹ï¼Œå®ƒç”Ÿæˆçš„å‘é‡ç»´åº¦ä¸º768ç»´ã€‚

å‘é‡åŒ–çš„æ ¸å¿ƒåŸåˆ™æ˜¯ï¼š**è¯­ä¹‰ç›¸ä¼¼ï¼Œå‘é‡ç›¸è¿‘**ã€‚è¿™æ„å‘³ç€ï¼š
- "è‹¹æœæ˜¯ä¸€ç§æ°´æœ"å’Œ"é¦™è•‰å¯Œå«é’¾å…ƒç´ "çš„å‘é‡è·ç¦»è¾ƒè¿‘
- "è‹¹æœæ˜¯ä¸€ç§æ°´æœ"å’Œ"Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€"çš„å‘é‡è·ç¦»è¾ƒè¿œ

### 2.3 å‘é‡å­˜å‚¨

ç”Ÿæˆå‘é‡åï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸“é—¨çš„åŸºç¡€è®¾æ–½æ¥å­˜å‚¨å’Œæ£€ç´¢è¿™äº›å‘é‡ã€‚è™½ç„¶ä¼ ç»Ÿæ•°æ®åº“ï¼ˆå¦‚MySQLï¼‰ä¹Ÿèƒ½å­˜å‚¨å‘é‡ï¼Œä½†å®ƒä»¬æ— æ³•è¿›è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ã€‚

**å‘é‡æ•°æ®åº“ï¼ˆVector Databaseï¼‰** ä¸“ä¸ºå­˜å‚¨å’ŒæŸ¥è¯¢é«˜ç»´å‘é‡è€Œè®¾è®¡ï¼Œå†…ç½®äº†é«˜æ•ˆçš„**è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰æœç´¢ç®—æ³•**ï¼Œå¯ä»¥åœ¨æ¯«ç§’çº§å†…ä»æ•°ç™¾ä¸‡å‘é‡ä¸­æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å‡ ä¸ªã€‚

å¸¸è§çš„å‘é‡æ•°æ®åº“åŒ…æ‹¬FAISSã€ChromaDBã€Milvusç­‰ã€‚åœ¨æœ¬æ–‡çš„å®è·µä¸­ï¼Œè€ƒè™‘åˆ°æˆ‘ä»¬è¿½æ±‚ç†è§£åŸç†è€Œéç”Ÿäº§éƒ¨ç½²ï¼Œå°†ä½¿ç”¨FAISSæˆ–ç”šè‡³ç›´æ¥ä½¿ç”¨NumPyæ•°ç»„å­˜å‚¨å‘é‡â€”â€”è¿™è¶³ä»¥è®©æˆ‘ä»¬çœ‹æ¸…RAGçš„æœ¬è´¨ã€‚

## ä¸‰ã€RAGæ¨ç†æ£€ç´¢æµç¨‹

å½“ç”¨æˆ·æå‡ºé—®é¢˜åï¼ŒRAGç³»ç»Ÿè¿›å…¥åœ¨çº¿æ¨ç†é˜¶æ®µã€‚è¿™ä¸€é˜¶æ®µåŒ…æ‹¬æŸ¥è¯¢å‘é‡åŒ–ã€ç›¸ä¼¼åº¦æ£€ç´¢ã€ç»“æœé‡æ’å’Œæç¤ºè¯æ„å»ºå››ä¸ªç¯èŠ‚ã€‚

### 3.1 æŸ¥è¯¢å‘é‡åŒ–ä¸ç›¸ä¼¼åº¦æ£€ç´¢

ç”¨æˆ·è¾“å…¥çš„é—®é¢˜åŒæ ·éœ€è¦è½¬æ¢ä¸ºå‘é‡â€”â€”**å¿…é¡»ä½¿ç”¨ä¸çŸ¥è¯†åº“å‘é‡åŒ–å®Œå…¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹**ã€‚è¿™æ ·æ‰èƒ½ç¡®ä¿æŸ¥è¯¢å‘é‡å’Œæ–‡æ¡£å‘é‡ä½äºåŒä¸€ä¸ªè¯­ä¹‰ç©ºé—´ï¼Œè·ç¦»è®¡ç®—æ‰æœ‰æ„ä¹‰ã€‚

æ¥ä¸‹æ¥ï¼Œç³»ç»Ÿè®¡ç®—æŸ¥è¯¢å‘é‡ä¸çŸ¥è¯†åº“ä¸­æ‰€æœ‰æ–‡æ¡£å‘é‡çš„**ç›¸ä¼¼åº¦**ã€‚æœ€å¸¸ç”¨çš„ç›¸ä¼¼åº¦æŒ‡æ ‡æ˜¯**ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰**ï¼š

```python
def similarity(e1, e2):
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    dot_product = np.dot(e1, e2)                     # ç‚¹ä¹˜
    norm_e1 = np.linalg.norm(e1)                     # å‘é‡1çš„èŒƒæ•°
    norm_e2 = np.linalg.norm(e2)                     # å‘é‡2çš„èŒƒæ•°
    cosine_sim = dot_product / (norm_e1 * norm_e2)   # ä½™å¼¦ç›¸ä¼¼åº¦
    return cosine_sim
```

ä½™å¼¦ç›¸ä¼¼åº¦çš„å–å€¼èŒƒå›´æ˜¯[-1, 1]ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºä¸¤ä¸ªå‘é‡çš„å¤¹è§’è¶Šå°ï¼Œè¯­ä¹‰è¶Šç›¸è¿‘ã€‚

è®¡ç®—å‡ºæ‰€æœ‰ç›¸ä¼¼åº¦åï¼Œç³»ç»ŸæŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œå–å‰Kä¸ªï¼ˆé€šå¸¸K=3~5ï¼‰æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—ä½œä¸ºæ£€ç´¢ç»“æœã€‚

### 3.2 ç»“æœé‡æ’ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰

åŸºç¡€çš„å‘é‡æ£€ç´¢å­˜åœ¨ä¸€ä¸ªæ½œåœ¨é—®é¢˜ï¼š**æ’åé å‰çš„æ–‡æ¡£ä¸ä¸€å®šæ˜¯æœ€æœ‰ç”¨çš„**ã€‚æ£€ç´¢é˜¶æ®µä½¿ç”¨çš„æ˜¯**åŒç¼–ç å™¨ï¼ˆBi-Encoderï¼‰** æ¶æ„ï¼Œå®ƒå°†æŸ¥è¯¢å’Œæ–‡æ¡£åˆ†åˆ«ç¼–ç ï¼Œé€Ÿåº¦å¿«ä½†ç²¾åº¦æœ‰é™ã€‚

ä¸ºäº†æé«˜è´¨é‡ï¼Œå¯ä»¥å¼•å…¥**é‡æ’ï¼ˆRerankingï¼‰** é˜¶æ®µã€‚é‡æ’ä½¿ç”¨**äº¤å‰ç¼–ç å™¨ï¼ˆCross-Encoderï¼‰** æ¶æ„ï¼Œå®ƒå°†æŸ¥è¯¢å’Œæ–‡æ¡£æ‹¼æ¥åä¸€èµ·è¾“å…¥æ¨¡å‹ï¼Œè®¡ç®—ç›¸å…³æ€§å¾—åˆ†ï¼Œç²¾åº¦æ›´é«˜ä½†é€Ÿåº¦æ…¢ã€‚å› æ­¤ï¼Œå…¸å‹çš„ç­–ç•¥æ˜¯ï¼šå‘é‡æ£€ç´¢å…ˆå¿«é€Ÿå¬å›Top 100ï¼Œé‡æ’æ¨¡å‹å†ä»ä¸­ç²¾ç­›Top 5ã€‚

åœ¨æœ¬æ–‡çš„ç®€åŒ–å®ç°ä¸­ï¼Œæˆ‘ä»¬å°†è·³è¿‡é‡æ’æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å‘é‡æ£€ç´¢ç»“æœã€‚

### 3.3 æç¤ºè¯æ„å»ºä¸ç”Ÿæˆ

æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£åï¼Œéœ€è¦å°†å®ƒä»¬ä¸ç”¨æˆ·é—®é¢˜ç»„è£…æˆä¸€ä¸ªç»“æ„åŒ–çš„æç¤ºè¯ï¼ˆPromptï¼‰ï¼Œç„¶åå‘é€ç»™å¤§è¯­è¨€æ¨¡å‹ã€‚

æœ€ç®€å•çš„æç¤ºè¯æ¨¡æ¿å¦‚ä¸‹ï¼š
```python
prompt_template = """
åŸºäºä»¥ä¸‹çŸ¥è¯†å›ç­”é—®é¢˜ï¼š

çŸ¥è¯†ï¼š
1: %s
2: %s
3: %s
4: %s
5: %s

é—®é¢˜ï¼š%s

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†ç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœçŸ¥è¯†ä¸­ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
"""
```

è¿™ä¸ªæ¨¡æ¿çš„è®¾è®¡åŸåˆ™æ˜¯ï¼š
- **æ˜ç¡®ä¿¡æ¯æ¥æº**ï¼šå‘ŠçŸ¥æ¨¡å‹çŸ¥è¯†æ˜¯ä»å“ªé‡Œæ¥çš„
- **é™å®šå›ç­”èŒƒå›´**ï¼šè¦æ±‚æ¨¡å‹"åŸºäºçŸ¥è¯†"å›ç­”ï¼Œå‡å°‘å¹»è§‰
- **å…è®¸ä¸çŸ¥é“**ï¼šä¸ºæ¨¡å‹æä¾›"ä¸çŸ¥é“"çš„å‡ºå£ï¼Œé¿å…ç¼–é€ 

å°†ç»„è£…å¥½çš„æç¤ºè¯å‘é€ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆæœ¬æ–‡ä½¿ç”¨DeepSeek-R1ï¼‰ï¼Œæ¨¡å‹ç”Ÿæˆçš„å›ç­”å°±æ˜¯æœ€ç»ˆè¾“å‡ºã€‚

## å››ã€è¾“å‡ºæ•´ç†ä¸ä¼˜åŒ–

RAGç³»ç»Ÿçš„è¾“å‡ºå¹¶éç»ˆç‚¹ï¼Œè¿˜éœ€è¦è¿›è¡Œæ•´ç†å’Œå¯èƒ½çš„ä¼˜åŒ–ã€‚

### 4.1 å¼•ç”¨æ¥æº

é«˜è´¨é‡çš„RAGç³»ç»Ÿåº”è¯¥åœ¨å›ç­”ä¸­**æ ‡æ³¨ä¿¡æ¯æ¥æº**ã€‚è¿™ç±»ä¼¼äºå­¦æœ¯è®ºæ–‡çš„è„šæ³¨ï¼Œç”¨æˆ·å¯ä»¥æ ¸å®æ¯ä¸ªäº‹å®çš„çœŸå®æ€§ã€‚åœ¨å®è·µä¸­ï¼Œå¯ä»¥åœ¨è¿”å›æ£€ç´¢ç»“æœæ—¶ä¿ç•™æ–‡æ¡£çš„å…ƒæ•°æ®ï¼ˆå¦‚æ–‡ä»¶åã€æ®µè½ä½ç½®ï¼‰ï¼Œç„¶ååœ¨ç”Ÿæˆå›ç­”æ—¶è¦æ±‚æ¨¡å‹å¼•ç”¨æ¥æºç¼–å·ã€‚

### 4.2 æŸ¥è¯¢é‡å†™ï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰

ç”¨æˆ·æé—®å¾€å¾€å«ç³Šä¸æ¸…æˆ–åŒ…å«éšå«æ„å›¾ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·é—®"å‘Šè¯‰æˆ‘NVIDIAæ¨¡å‹çš„æœ€æ–°æ›´æ–°"ï¼Œå¯èƒ½æš—ä¸­å¯¹ç‰¹å®šåŠŸèƒ½æ„Ÿå…´è¶£ï¼Œä½†è¿™ç§åå¥½æ²¡æœ‰è¢«æ˜ç¡®è¡¨è¾¾ã€‚

**æŸ¥è¯¢é‡å†™ï¼ˆQuery Rewritingï¼‰** æŠ€æœ¯å¯ä»¥åœ¨æ£€ç´¢å‰ä¼˜åŒ–ç”¨æˆ·æŸ¥è¯¢ï¼Œå¼¥åˆç”¨æˆ·æé—®æ–¹å¼ä¸çŸ¥è¯†åº“ä¿¡æ¯ç»“æ„ä¹‹é—´çš„è¯­ä¹‰å·®è·ã€‚å¸¸ç”¨æ–¹æ³•åŒ…æ‹¬ï¼š

- **Q2Eï¼ˆQuery2Expandï¼‰**ï¼šç”ŸæˆåŒä¹‰è¯å’Œç›¸å…³çŸ­è¯­ï¼Œæ‰©å±•æŸ¥è¯¢
- **Q2Dï¼ˆQuery2Docï¼‰**ï¼šæ ¹æ®æŸ¥è¯¢æ„å»ºä¼ªæ–‡æ¡£ï¼ŒåŒ¹é…æ–‡æ¡£é£æ ¼
- **CoTï¼ˆæ€ç»´é“¾ï¼‰æŸ¥è¯¢é‡å†™**ï¼šè®©æ¨¡å‹é€æ­¥æ¨ç†ï¼Œåˆ†è§£æŸ¥è¯¢æ„å›¾

ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨Llama 3.3 Nemotron Super 49Bè¿›è¡ŒCoTæŸ¥è¯¢é‡å†™åï¼Œæ£€ç´¢å‡†ç¡®ç‡@10ä»43.1%æå‡è‡³63.8%ã€‚

## äº”ã€ä¸²è”è¿è¡Œï¼šå®Œæ•´Pythonè„šæœ¬å®ç°

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æ‰€æœ‰ç¯èŠ‚ä¸²è”èµ·æ¥ï¼Œç¼–å†™ä¸€ä¸ªå®Œæ•´çš„RAGç³»ç»ŸPythonè„šæœ¬ã€‚æœ¬å®ç°å°†**æœ€å°åŒ–ç¬¬ä¸‰æ–¹æ¡†æ¶ä¾èµ–**ï¼Œä»…ä½¿ç”¨Ollamaå’ŒNumPyï¼Œç¡®ä¿ä½ èƒ½å¤Ÿçœ‹æ¸…æ¯ä¸ªæ­¥éª¤çš„æœ¬è´¨ã€‚

### 5.1 ç¯å¢ƒå‡†å¤‡

é¦–å…ˆï¼Œç¡®ä¿å·²å®‰è£…Ollamaå¹¶æ‹‰å–æ‰€éœ€æ¨¡å‹ï¼š

```bash
# å®‰è£…Ollamaï¼ˆè¯·è®¿é—®ollama.comä¸‹è½½å¯¹åº”ç³»ç»Ÿç‰ˆæœ¬ï¼‰

# æ‹‰å–æ¨¡å‹
ollama pull deepseek-r1:8b
ollama pull nomic-embed-text

# å®‰è£…Pythonä¾èµ–
pip install ollama numpy
```

### ç¯å¢ƒé—®é¢˜
#### ç³»ç»Ÿpython 3.9ç¼ºå¤± 
è¿™ä¸ªé”™è¯¯æ˜¯å› ä¸ºPython 3.9ä¸æ”¯æŒ|æ“ä½œç¬¦ç”¨äºç±»å‹è”åˆï¼ˆtype unionï¼‰ã€‚åœ¨Python 3.10åŠä»¥ä¸Šç‰ˆæœ¬ä¸­ï¼Œ|æ‰è¢«æ”¯æŒç”¨äºç±»å‹æç¤ºã€‚ä½ çš„ä»£ç è¿è¡Œåœ¨Python 3.9ç¯å¢ƒï¼Œæ‰€ä»¥æŠ¥é”™ã€‚

éœ€è¦å®‰è£…python 3.10.X ä½¿ç”¨pyenvå·¥å…·ï¼Œåˆ‡æ¢ç”¨æˆ·å±‚çº§çš„pythonç¯å¢ƒï¼Œ

#### Rustç¼–è¯‘å™¨ç¼ºå¤±ï¼šå®‰è£…tiktokenæ—¶æŠ¥é”™can't find Rust compiler

#### SWIGå·¥å…·ç¼ºå¤±ï¼šæŠ¥é”™command 'swig' failed: No such file or directory

#### FAISSç¼–è¯‘å¤±è´¥ï¼šæŠ¥é”™command '/opt/homebrew/bin/swig' failed with exit code 1ï¼ŒFAISSå®‰è£…å¤±è´¥

#### PyPyå…¼å®¹æ€§é—®é¢˜
å‘ç°ä½ åœ¨ä½¿ç”¨PyPyè€Œä¸æ˜¯CPythonï¼Œå¯¼è‡´å®‰è£…faisså’Œchromadbå‡æŠ¥é”™ã€‚

é‡ç½®æ­¥éª¤ï¼š

```bash
# 1. ç¡®ä¿ä½¿ç”¨ CPython
pyenv install 3.10.15
cd /Users/mac/Dev/Desktop/RagDemo
pyenv local 3.10.15

# 2. åˆ›å»ºå¹²å‡€çš„ç¯å¢ƒ
python -m venv venv
source venv/bin/activate

# 3. å‡çº§åŸºç¡€å·¥å…·
pip install --upgrade pip setuptools wheel

# 4. å®‰è£…ä¾èµ–ï¼ˆæŒ‰é¡ºåºï¼‰
pip install chromadb
pip install llama-index-core
pip install llama-index-embeddings-ollama
pip install llama-index-llms-ollama
pip install llama-index-vector-stores-chroma

# 5. éªŒè¯å®‰è£…
pip list | grep -E "chroma|llama"
```

### 5.2 å®Œæ•´ä»£ç å®ç°

```python
"""
åŸºäºLlamaIndexçš„å¤šæ–‡ä»¶RAGç³»ç»Ÿ
å¤„ç†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
æ˜¾ç¤ºDeepSeek-R1çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹
"""

import os
from typing import List, Generator, Optional
import warnings
warnings.filterwarnings('ignore')

# LlamaIndexæ ¸å¿ƒç»„ä»¶
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    Settings,
    Document
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import CompactAndRefine
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.storage import StorageContext

# Ollamaé›†æˆ
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama

# å‘é‡å­˜å‚¨
from llama_index.vector_stores.faiss import FaissVectorStore
import faiss

# æ–‡ä»¶ç›‘æ§ï¼ˆå¯é€‰ï¼‰
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler


class MultiFileRAGSystem:
    """
    å¤šæ–‡ä»¶RAGç³»ç»Ÿ
    æ”¯æŒæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œè‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢
    """

    SUPPORTED_EXTENSIONS = [
        '.txt', '.pdf', '.docx', '.pptx', '.xlsx',
        '.md', '.csv', '.epub', '.html', '.htm',
        '.json', '.xml', '.ipynb'
    ]

    def __init__(
        self,
        docs_dir: str,
        embed_model_name: str = "nomic-embed-text",
        llm_model_name: str = "deepseek-r1:8b",
        ollama_base_url: str = "http://localhost:11434",
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        similarity_top_k: int = 5,
        persist_dir: Optional[str] = "./storage"
    ):
        """
        åˆå§‹åŒ–å¤šæ–‡ä»¶RAGç³»ç»Ÿ

        Args:
            docs_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
            embed_model_name: åµŒå…¥æ¨¡å‹åç§°
            llm_model_name: å¤§è¯­è¨€æ¨¡å‹åç§°
            ollama_base_url: OllamaæœåŠ¡åœ°å€
            chunk_size: æ–‡æœ¬å—å¤§å°
            chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°
            similarity_top_k: æ£€ç´¢è¿”å›çš„ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            persist_dir: ç´¢å¼•æŒä¹…åŒ–ç›®å½•ï¼ˆå¯é€‰ï¼‰
        """
        self.docs_dir = docs_dir
        self.persist_dir = persist_dir
        self.similarity_top_k = similarity_top_k
        self.thinking_buffer = ""
        self.answer_buffer = ""

        print("=" * 80)
        print("å¤šæ–‡ä»¶RAGç³»ç»Ÿåˆå§‹åŒ–")
        print("=" * 80)
        print(f"ğŸ“ æ–‡æ¡£ç›®å½•: {docs_dir}")
        print(f"ğŸ“Š æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(self.SUPPORTED_EXTENSIONS[:5])}ç­‰")

        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(docs_dir):
            raise ValueError(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")

        # 1. é…ç½®åµŒå…¥æ¨¡å‹
        print(f"\nğŸ”§ åŠ è½½åµŒå…¥æ¨¡å‹: {embed_model_name}")
        self.embed_model = OllamaEmbedding(
            model_name=embed_model_name,
            base_url=ollama_base_url,
            ollama_additional_kwargs={"mirostat": 0}
        )

        # 2. é…ç½®å¤§è¯­è¨€æ¨¡å‹
        print(f"ğŸ”§ åŠ è½½è¯­è¨€æ¨¡å‹: {llm_model_name}")
        self.llm = Ollama(
            model=llm_model_name,
            base_url=ollama_base_url,
            temperature=0.7,
            request_timeout=120.0,
            additional_kwargs={
                "num_predict": 2048,
                "top_k": 40,
                "top_p": 0.9
            }
        )

        # 3. è®¾ç½®å…¨å±€é…ç½®
        Settings.embed_model = self.embed_model
        Settings.llm = self.llm
        Settings.chunk_size = chunk_size
        Settings.chunk_overlap = chunk_overlap

        # 4. åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨
        self.node_parser = SentenceSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separator=" ",
            paragraph_separator="\n\n",
            secondary_chunking_regex="[^,.;:]+[,.;:]?"
        )

        # 5. ç´¢å¼•å¯¹è±¡
        self.index = None
        self.query_engine = None

        # 6. æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        self.file_stats = {}

        print("âœ… åˆå§‹åŒ–å®Œæˆ")

    def scan_documents(self) -> dict:
        """
        æ‰«ææ–‡æ¡£ç›®å½•ï¼Œç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯

        Returns:
            æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"\nğŸ“‹ æ‰«ææ–‡æ¡£ç›®å½•: {self.docs_dir}")

        stats = {
            "total_files": 0,
            "supported_files": 0,
            "unsupported_files": 0,
            "file_types": {},
            "total_size_mb": 0
        }

        for root, dirs, files in os.walk(self.docs_dir):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1].lower()
                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB

                stats["total_files"] += 1
                stats["total_size_mb"] += file_size

                if file_ext in self.SUPPORTED_EXTENSIONS:
                    stats["supported_files"] += 1
                    stats["file_types"][file_ext] = stats["file_types"].get(file_ext, 0) + 1
                    print(f"  âœ… {file} ({file_size:.2f} MB) - {file_ext}")
                else:
                    stats["unsupported_files"] += 1
                    print(f"  âš ï¸ {file} ({file_size:.2f} MB) - ä¸æ”¯æŒæ ¼å¼: {file_ext}")

        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"  æ”¯æŒæ–‡ä»¶: {stats['supported_files']}")
        print(f"  ä¸æ”¯æŒæ–‡ä»¶: {stats['unsupported_files']}")
        print(f"  æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
        print(f"  æ–‡ä»¶ç±»å‹åˆ†å¸ƒ: {stats['file_types']}")

        self.file_stats = stats
        return stats

    def load_documents(self, recursive: bool = True) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶

        Args:
            recursive: æ˜¯å¦é€’å½’åŠ è½½å­ç›®å½•

        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        print(f"\nğŸ“„ åŠ è½½æ–‡æ¡£...")

        # é¦–å…ˆæ‰«ææ–‡ä»¶
        self.scan_documents()

        # ä½¿ç”¨SimpleDirectoryReaderåŠ è½½æ‰€æœ‰æ–‡ä»¶
        reader = SimpleDirectoryReader(
            input_dir=self.docs_dir,
            recursive=recursive,
            exclude_hidden=True,  # æ’é™¤éšè—æ–‡ä»¶
            required_exts=self.SUPPORTED_EXTENSIONS  # åªåŠ è½½æ”¯æŒçš„æ–‡ä»¶
        )

        documents = reader.load_data()

        print(f"\nâœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # æŒ‰æ–‡ä»¶ç±»å‹æ˜¾ç¤ºç»Ÿè®¡
        doc_by_type = {}
        for doc in documents:
            file_name = doc.metadata.get('file_name', 'unknown')
            file_ext = os.path.splitext(file_name)[1].lower()
            doc_by_type[file_ext] = doc_by_type.get(file_ext, 0) + 1

        print("ğŸ“Š æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        for ext, count in doc_by_type.items():
            print(f"  {ext}: {count} ä¸ªæ–‡æ¡£")

        # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„ä¿¡æ¯
        for i, doc in enumerate(documents[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            file_name = doc.metadata.get('file_name', 'unknown')
            file_size = doc.metadata.get('file_size', 0) / 1024  # KB
            preview = doc.text[:100].replace('\n', ' ') + "..." if len(doc.text) > 100 else doc.text
            print(f"\n  æ–‡æ¡£ {i+1}: {file_name}")
            print(f"    å¤§å°: {file_size:.1f} KB")
            print(f"    é¢„è§ˆ: {preview}")

        if len(documents) > 5:
            print(f"\n  ... è¿˜æœ‰ {len(documents) - 5} ä¸ªæ–‡æ¡£æœªæ˜¾ç¤º")

        return documents

    def build_index(self, documents: List[Document], force_rebuild: bool = False):
        """
        æ„å»ºå‘é‡ç´¢å¼•

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆå¿½ç•¥å·²æœ‰ç´¢å¼•ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰æŒä¹…åŒ–ç´¢å¼•
        if self.persist_dir and os.path.exists(self.persist_dir) and not force_rebuild:
            try:
                print(f"\nğŸ” å‘ç°å·²æœ‰ç´¢å¼•ï¼Œå°è¯•åŠ è½½...")
                self.load_index()
                return
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
                print("å°†é‡æ–°æ„å»ºç´¢å¼•...")

        print(f"\nğŸ”¨ æ„å»ºå‘é‡ç´¢å¼•...")
        print(f"åˆ‡åˆ†æ–‡æ¡£ä¸ºæ–‡æœ¬å— (size={Settings.chunk_size}, overlap={Settings.chunk_overlap})")

        # æ„å»ºç´¢å¼•ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
        self.index = VectorStoreIndex.from_documents(
            documents,
            embed_model=self.embed_model,
            node_parser=self.node_parser,
            show_progress=True
        )

        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

        # è·å–èŠ‚ç‚¹ç»Ÿè®¡
        nodes = self.index.docstore.docs.values()
        print(f"  ç”Ÿæˆæ–‡æœ¬å—æ•°é‡: {len(nodes)}")

        # è®¡ç®—å¹³å‡æ–‡æœ¬å—é•¿åº¦
        avg_length = sum(len(node.text) for node in nodes) / len(nodes)
        print(f"  å¹³å‡æ–‡æœ¬å—é•¿åº¦: {avg_length:.1f} å­—ç¬¦")

        # ä¿å­˜ç´¢å¼•
        if self.persist_dir:
            self.save_index()

    def setup_query_engine(self):
        """
        é…ç½®æŸ¥è¯¢å¼•æ“
        """
        if not self.index:
            raise ValueError("è¯·å…ˆæ„å»ºç´¢å¼•")

        print(f"\nâš™ï¸ é…ç½®æŸ¥è¯¢å¼•æ“...")

        # 1. åˆ›å»ºæ£€ç´¢å™¨
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=self.similarity_top_k,
            embed_model=self.embed_model
        )

        # 2. åˆ›å»ºå“åº”åˆæˆå™¨ï¼ˆå¯ç”¨æµå¼è¾“å‡ºï¼‰
        response_synthesizer = CompactAndRefine(
            llm=self.llm,
            streaming=True,
            verbose=True
        )

        # 3. åˆ›å»ºåå¤„ç†å™¨
        postprocessor = SimilarityPostprocessor(similarity_cutoff=0.6)  # é™ä½é˜ˆå€¼ä»¥åŒ…å«æ›´å¤šç»“æœ

        # 4. ç»„åˆä¸ºæŸ¥è¯¢å¼•æ“
        self.query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer,
            node_postprocessors=[postprocessor]
        )

        print(f"âœ… æŸ¥è¯¢å¼•æ“é…ç½®å®Œæˆ")
        print(f"  æ£€ç´¢æ•°é‡: top_{self.similarity_top_k}")
        print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼: 0.6")

    def query_with_thinking(self, query: str) -> Generator[dict, None, None]:
        """
        æ‰§è¡ŒæŸ¥è¯¢å¹¶æµå¼è¿”å›æ€è€ƒè¿‡ç¨‹

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Yields:
            åŒ…å«ç±»å‹å’Œå†…å®¹çš„å­—å…¸
        """
        if not self.query_engine:
            raise ValueError("è¯·å…ˆé…ç½®æŸ¥è¯¢å¼•æ“")

        print(f"\nğŸ” æ‰§è¡ŒæŸ¥è¯¢: '{query}'")

        # 1. æ‰§è¡Œæ£€ç´¢
        nodes = self.query_engine.retriever.retrieve(query)

        # ç»„ç»‡æ£€ç´¢ç»“æœ
        retrieval_results = []
        for node in nodes:
            file_name = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
            retrieval_results.append({
                "text": node.text,
                "score": node.score,
                "file_name": file_name,
                "node_id": node.node_id
            })

        # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
        yield {
            "type": "retrieval",
            "content": {
                "query": query,
                "results": retrieval_results
            }
        }

        # 2. æ„å»ºæç¤ºè¯ï¼ˆå¸¦æ–‡ä»¶æ¥æºä¿¡æ¯ï¼‰
        context_parts = []
        for i, node in enumerate(nodes):
            file_name = node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
            context_parts.append(f"[æ¥è‡ªæ–‡ä»¶: {file_name}]\n{node.text}")

        context_text = "\n\n---\n\n".join(context_parts)
        prompt = f"""
åŸºäºä»¥ä¸‹ä»å¤šä¸ªæ–‡ä»¶ä¸­æ£€ç´¢åˆ°çš„çŸ¥è¯†å›ç­”é—®é¢˜ã€‚æ¯ä¸ªçŸ¥è¯†ç‰‡æ®µéƒ½æ ‡æ³¨äº†æ¥æºæ–‡ä»¶ã€‚

{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ä¸¥æ ¼åŸºäºä¸Šè¿°çŸ¥è¯†å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚å¦‚æœçŸ¥è¯†ä¸­ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
åœ¨å›ç­”ä¸­å¯ä»¥æåŠä¿¡æ¯æ¥æºæ–‡ä»¶ï¼ˆå¦‚"æ ¹æ®XXXæ–‡ä»¶çš„æè¿°"ï¼‰ã€‚
"""

        yield {"type": "prompt", "content": prompt[:300] + "..." if len(prompt) > 300 else prompt}

        # 3. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆæµå¼è¾“å‡ºï¼‰
        yield {"type": "start_generation", "content": "å¼€å§‹ç”Ÿæˆå›ç­”..."}

        streaming_response = self.query_engine.query(query)

        # é‡ç½®ç¼“å†²åŒº
        self.thinking_buffer = ""
        self.answer_buffer = ""
        in_thinking = True

        # æµå¼å¤„ç†å“åº”
        for text_chunk in streaming_response.response_gen:
            # æ£€æµ‹æ€è€ƒ/å›ç­”è¾¹ç•Œ
            if "æ€è€ƒï¼š" in text_chunk or "åˆ†æï¼š" in text_chunk:
                in_thinking = True
                self.thinking_buffer += text_chunk
                yield {"type": "thinking_chunk", "content": text_chunk}
            elif "ç­”æ¡ˆï¼š" in text_chunk or "å›ç­”ï¼š" in text_chunk:
                if in_thinking:
                    yield {"type": "thinking_end", "content": ""}
                in_thinking = False
                self.answer_buffer += text_chunk
                yield {"type": "answer_chunk", "content": text_chunk}
            elif in_thinking:
                self.thinking_buffer += text_chunk
                yield {"type": "thinking_chunk", "content": text_chunk}
            else:
                self.answer_buffer += text_chunk
                yield {"type": "answer_chunk", "content": text_chunk}

        # 4. è¿”å›å®Œæ•´ç»“æœï¼ˆå¸¦æ¥æºä¿¡æ¯ï¼‰
        source_nodes = [
            {
                "text": node.text,
                "score": node.score,
                "file_name": node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶'),
                "metadata": node.metadata
            }
            for node in streaming_response.source_nodes
        ]

        yield {
            "type": "complete",
            "content": {
                "thinking": self.thinking_buffer,
                "answer": streaming_response.response,
                "source_nodes": source_nodes
            }
        }

    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        if not self.index or not self.persist_dir:
            return

        os.makedirs(self.persist_dir, exist_ok=True)
        self.index.storage_context.persist(persist_dir=self.persist_dir)
        print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ° {self.persist_dir}")

    def load_index(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•"""
        from llama_index.core import load_index_from_storage

        if not os.path.exists(self.persist_dir):
            raise ValueError(f"æŒä¹…åŒ–ç›®å½•ä¸å­˜åœ¨: {self.persist_dir}")

        storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
        self.index = load_index_from_storage(storage_context)
        print(f"âœ… ç´¢å¼•å·²ä» {self.persist_dir} åŠ è½½")


class DocumentWatcher(FileSystemEventHandler):
    """æ–‡æ¡£ç›®å½•ç›‘æ§å™¨ï¼Œå½“æ–‡ä»¶å˜åŒ–æ—¶è‡ªåŠ¨é‡å»ºç´¢å¼•"""

    def __init__(self, rag_system: MultiFileRAGSystem):
        self.rag_system = rag_system
        self.last_rebuild = time.time()
        self.rebuild_cooldown = 60  # 60ç§’å†…ä¸é‡å¤é‡å»º

    def on_modified(self, event):
        if not event.is_directory:
            self._handle_change(event.src_path)

    def on_created(self, event):
        if not event.is_directory:
            self._handle_change(event.src_path)

    def on_deleted(self, event):
        if not event.is_directory:
            self._handle_change(event.src_path)

    def _handle_change(self, file_path):
        """å¤„ç†æ–‡ä»¶å˜åŒ–"""
        current_time = time.time()
        if current_time - self.last_rebuild > self.rebuild_cooldown:
            print(f"\nğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–: {file_path}")
            print("æ­£åœ¨é‡å»ºç´¢å¼•...")
            self.rag_system.build_index(
                self.rag_system.load_documents(),
                force_rebuild=True
            )
            self.last_rebuild = current_time


def interactive_mode():
    """äº¤äº’å¼é—®ç­”æ¨¡å¼"""

    # é…ç½®
    docs_dir = "./blog_docs"  # å­˜æ”¾æ‰€æœ‰çŸ¥è¯†åº“æ–‡ä»¶çš„ç›®å½•

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = MultiFileRAGSystem(
        docs_dir=docs_dir,
        embed_model_name="nomic-embed-text",
        llm_model_name="deepseek-r1:8b",
        chunk_size=512,
        chunk_overlap=50,
        similarity_top_k=5,
        persist_dir="./storage"  # å¯ç”¨æŒä¹…åŒ–
    )

    # åŠ è½½æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•
    documents = rag.load_documents(recursive=True)
    rag.build_index(documents)
    rag.setup_query_engine()

    # äº¤äº’å¼é—®ç­”
    print("\n" + "=" * 80)
    print("å¤šæ–‡ä»¶RAGç³»ç»Ÿå·²å°±ç»ªï¼è¾“å…¥é—®é¢˜å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰")
    print("=" * 80)

    while True:
        query = input("\nğŸ“ ã€ç”¨æˆ·ã€‘: ").strip()
        if query.lower() in ['exit', 'quit', 'é€€å‡º']:
            print("å†è§ï¼")
            break

        if not query:
            continue

        print("\nğŸ”„ ã€ç³»ç»Ÿã€‘æ­£åœ¨å¤„ç†...")
        print("-" * 80)

        thinking_mode = True
        sources_shown = False

        for event in rag.query_with_thinking(query):
            if event["type"] == "retrieval":
                print("\nğŸ“š ã€æ£€ç´¢ç»“æœã€‘")
                for i, result in enumerate(event["content"]["results"]):
                    preview = result["text"][:60] + "..." if len(result["text"]) > 60 else result["text"]
                    print(f"  [{i+1}] ç›¸å…³åº¦: {result['score']:.4f} | æ¥æº: {result['file_name']}")
                    print(f"      å†…å®¹: {preview}")

            elif event["type"] == "prompt":
                print(f"\nğŸ“ ã€æç¤ºè¯é¢„è§ˆã€‘\n  {event['content']}")

            elif event["type"] == "start_generation":
                print("\nğŸ§  ã€æ¨¡å‹æ€è€ƒä¸­ã€‘")

            elif event["type"] == "thinking_chunk":
                print(event["content"], end='', flush=True)

            elif event["type"] == "thinking_end":
                print("\n\nğŸ’¡ ã€æ€è€ƒå®Œæˆï¼Œå¼€å§‹å›ç­”ã€‘\n")
                thinking_mode = False

            elif event["type"] == "answer_chunk":
                print(event["content"], end='', flush=True)

            elif event["type"] == "complete" and not sources_shown:
                print("\n\nğŸ“Š ã€ä¿¡æ¯æ¥æºã€‘")
                sources = {}
                for node in event["content"]["source_nodes"]:
                    file_name = node["file_name"]
                    if file_name not in sources:
                        sources[file_name] = []
                    sources[file_name].append({
                        "score": node["score"],
                        "preview": node["text"][:50] + "..."
                    })

                for file_name, snippets in sources.items():
                    print(f"\n  ğŸ“„ {file_name}:")
                    for i, snippet in enumerate(snippets):
                        print(f"    [{i+1}] ç›¸å…³åº¦: {snippet['score']:.4f}")
                        print(f"        {snippet['preview']}")

                sources_shown = True

        print("\n" + "=" * 80)


def batch_mode():
    """æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šå¤„ç†å¤šä¸ªé—®é¢˜"""

    # é…ç½®
    docs_dir = "./blog_docs"

    rag = MultiFileRAGSystem(
        docs_dir=docs_dir,
        persist_dir="./storage"
    )

    # åŠ è½½æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•
    documents = rag.load_documents()
    rag.build_index(documents)
    rag.setup_query_engine()

    # ä»æ–‡ä»¶è¯»å–é—®é¢˜åˆ—è¡¨
    questions_file = "questions.txt"
    if os.path.exists(questions_file):
        with open(questions_file, 'r', encoding='utf-8') as f:
            questions = [q.strip() for q in f.readlines() if q.strip()]
    else:
        # ç¤ºä¾‹é—®é¢˜
        questions = [
            "è‹¹æœæœ‰ä»€ä¹ˆè¥å…»ä»·å€¼ï¼Ÿ",
            "é¦™è•‰é€‚åˆä»€ä¹ˆæ—¶å€™åƒï¼Ÿ",
            "å¦‚ä½•ä¿æŒå¥åº·é¥®é£Ÿï¼Ÿ"
        ]

    print("\n" + "=" * 80)
    print("æ‰¹é‡å¤„ç†æ¨¡å¼")
    print("=" * 80)

    results = []
    for i, question in enumerate(questions, 1):
        print(f"\nğŸ“Œ é—®é¢˜ {i}/{len(questions)}: {question}")
        print("-" * 40)

        response = rag.query_engine.query(question)
        print(f"å›ç­”: {response}")

        results.append({
            "question": question,
            "answer": str(response),
            "sources": [
                {
                    "file": node.metadata.get('file_name', 'æœªçŸ¥'),
                    "score": node.score,
                    "text": node.text[:200]
                }
                for node in response.source_nodes
            ]
        })

        print()

    # ä¿å­˜ç»“æœ
    import json
    with open("batch_results.json", "w", encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print("âœ… ç»“æœå·²ä¿å­˜åˆ° batch_results.json")


def watch_mode():
    """ç›‘æ§æ¨¡å¼ï¼šç›‘å¬æ–‡ä»¶å˜åŒ–å¹¶è‡ªåŠ¨æ›´æ–°ç´¢å¼•"""

    # é…ç½®
    docs_dir = "./blog_docs"

    rag = MultiFileRAGSystem(
        docs_dir=docs_dir,
        persist_dir="./storage"
    )

    # åˆå§‹æ„å»º
    documents = rag.load_documents()
    rag.build_index(documents)
    rag.setup_query_engine()

    # å¯åŠ¨æ–‡ä»¶ç›‘æ§
    event_handler = DocumentWatcher(rag)
    observer = Observer()
    observer.schedule(event_handler, docs_dir, recursive=True)
    observer.start()

    print("\n" + "=" * 80)
    print("ç›‘æ§æ¨¡å¼å·²å¯åŠ¨ - æ–‡ä»¶å˜åŒ–æ—¶å°†è‡ªåŠ¨é‡å»ºç´¢å¼•")
    print("è¾“å…¥é—®é¢˜å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰")
    print("=" * 80)

    try:
        while True:
            query = input("\nğŸ“ ã€ç”¨æˆ·ã€‘: ").strip()
            if query.lower() in ['exit', 'quit', 'é€€å‡º']:
                break

            if not query:
                continue

            response = rag.query_engine.query(query)
            print(f"\nğŸ’¬ ã€å›ç­”ã€‘: {response}")

            # æ˜¾ç¤ºæ¥æº
            print("\nğŸ“š ã€æ¥æºã€‘:")
            for node in response.source_nodes[:3]:
                file_name = node.metadata.get('file_name', 'æœªçŸ¥')
                print(f"  - {file_name} (ç›¸å…³åº¦: {node.score:.4f})")

    finally:
        observer.stop()
        observer.join()


def main():
    """ä¸»å‡½æ•°ï¼šé€‰æ‹©è¿è¡Œæ¨¡å¼"""

    print("å¤šæ–‡ä»¶RAGç³»ç»Ÿ - é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. äº¤äº’å¼é—®ç­”")
    print("2. æ‰¹é‡å¤„ç†")
    print("3. ç›‘æ§æ¨¡å¼ï¼ˆè‡ªåŠ¨æ›´æ–°ï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()

    if choice == "1":
        interactive_mode()
    elif choice == "2":
        batch_mode()
    elif choice == "3":
        watch_mode()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œé»˜è®¤äº¤äº’å¼æ¨¡å¼")
        interactive_mode()


if __name__ == "__main__":
    main()
```

ä»£ç è§£æï¼š

```
ç³»ç»Ÿå¯åŠ¨æ—¶ï¼Œé¦–å…ˆé€šè¿‡MultiFileRAGSystemç±»çš„æ„é€ å‡½æ•°å®ŒæˆåŸºç¡€é…ç½®ã€‚åˆå§‹åŒ–è¿‡ç¨‹åŒ…æ‹¬è®¾ç½®æ–‡æ¡£ç›®å½•è·¯å¾„ã€é…ç½®OllamaæœåŠ¡è¿æ¥çš„åµŒå…¥æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨nomic-embed-textï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨deepseek-r1:1.5bï¼‰ï¼Œä»¥åŠå®šä¹‰æ–‡æœ¬å¤„ç†å‚æ•°å¦‚å—å¤§å°512å­—ç¬¦ã€å—é‡å 50å­—ç¬¦å’Œæ£€ç´¢è¿”å›æ•°é‡top_k=5ã€‚åŒæ—¶ï¼Œç³»ç»Ÿä¼šæ£€æŸ¥æ–‡æ¡£ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œé…ç½®å…¨å±€Settingså¯¹è±¡ï¼Œåˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨ï¼Œå¹¶å‡†å¤‡æŒä¹…åŒ–å­˜å‚¨ç›®å½•ã€‚

ç³»ç»Ÿé€šè¿‡scan_documentsæ–¹æ³•é€’å½’æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œç»Ÿè®¡æ€»æ–‡ä»¶æ•°ã€æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼ˆåŒ…æ‹¬txtã€pdfã€docxã€pptxã€xlsxã€mdã€csvç­‰åå¤šç§æ ¼å¼ï¼‰ã€æ–‡ä»¶å¤§å°åˆ†å¸ƒç­‰ä¿¡æ¯ï¼Œå¹¶è¯†åˆ«ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚éšåï¼Œload_documentsæ–¹æ³•è°ƒç”¨SimpleDirectoryReaderåŠ è½½æ‰€æœ‰æ”¯æŒæ ¼å¼çš„æ–‡æ¡£ï¼Œå°†æ¯ä¸ªæ–‡ä»¶è½¬æ¢ä¸ºDocumentå¯¹è±¡ï¼Œä¿ç•™æ–‡ä»¶åã€æ–‡ä»¶å¤§å°ç­‰å…ƒæ•°æ®ä¿¡æ¯ã€‚

åœ¨ç´¢å¼•æ„å»ºé˜¶æ®µï¼Œç³»ç»Ÿä½¿ç”¨SentenceSplitterå°†åŠ è½½çš„æ–‡æ¡£åˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„æ–‡æœ¬å—ï¼ˆèŠ‚ç‚¹ï¼‰ï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«æ–‡æœ¬å†…å®¹å’Œæ¥æºå…ƒæ•°æ®ã€‚é€šè¿‡OllamaEmbeddingå°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œå¹¶å­˜å‚¨åˆ°Faisså‘é‡æ•°æ®åº“ä¸­æ„å»ºVectorStoreIndexã€‚å¦‚æœå¯ç”¨äº†æŒä¹…åŒ–åŠŸèƒ½ï¼Œç´¢å¼•ä¼šè¢«ä¿å­˜åˆ°ç£ç›˜ï¼Œæ–¹ä¾¿åç»­å¿«é€ŸåŠ è½½è€Œæ— éœ€é‡å¤æ„å»ºã€‚æ„å»ºå®Œæˆåï¼Œç³»ç»Ÿä¼šç»Ÿè®¡ç”Ÿæˆçš„æ–‡æœ¬å—æ•°é‡å’Œå¹³å‡é•¿åº¦ã€‚

setup_query_engineæ–¹æ³•è´Ÿè´£é…ç½®å®Œæ•´çš„æŸ¥è¯¢æµæ°´çº¿ã€‚é¦–å…ˆåˆ›å»ºVectorIndexRetrieveræ£€ç´¢å™¨ï¼Œè®¾ç½®ç›¸ä¼¼åº¦æ£€ç´¢çš„top_kå‚æ•°ã€‚ç„¶ååˆ›å»ºCompactAndRefineå“åº”åˆæˆå™¨ï¼Œå¯ç”¨æµå¼è¾“å‡ºå’Œè¯¦ç»†æ¨¡å¼ã€‚æ¥ç€é…ç½®SimilarityPostprocessoråå¤„ç†å™¨ï¼Œè®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ä¸º0.6ä»¥è¿‡æ»¤ä½è´¨é‡ç»“æœã€‚æœ€åå°†è¿™äº›ç»„ä»¶ç»„åˆæˆRetrieverQueryEngineï¼Œå½¢æˆå®Œæ•´çš„æŸ¥è¯¢å¤„ç†é“¾è·¯ã€‚

å½“ç”¨æˆ·æäº¤æŸ¥è¯¢æ—¶ï¼Œç³»ç»Ÿæ‰§è¡Œquery_with_thinkingæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è®¾è®¡ä¸ºç”Ÿæˆå™¨å‡½æ•°ä»¥æ”¯æŒæµå¼è¾“å‡ºã€‚æŸ¥è¯¢å¤„ç†åˆ†ä¸ºå¤šä¸ªé˜¶æ®µï¼š

é¦–å…ˆæ˜¯æ£€ç´¢é˜¶æ®µï¼Œç³»ç»Ÿå°†ç”¨æˆ·é—®é¢˜å‘é‡åŒ–ååˆ°ç´¢å¼•ä¸­æ£€ç´¢æœ€ç›¸ä¼¼çš„top_kä¸ªæ–‡æœ¬å—ï¼Œè¿”å›æ¯ä¸ªå—çš„æ–‡æœ¬å†…å®¹ã€ç›¸ä¼¼åº¦åˆ†æ•°å’Œæ¥æºæ–‡ä»¶ä¿¡æ¯ï¼Œè¿™äº›ç»“æœä»¥"retrieval"ç±»å‹çš„äº‹ä»¶æµå¼è¾“å‡ºã€‚

å…¶æ¬¡æ˜¯æç¤ºè¯æ„å»ºé˜¶æ®µï¼Œç³»ç»Ÿå°†æ£€ç´¢åˆ°çš„æ–‡æœ¬å—æŒ‰ç…§æ¥æºæ–‡ä»¶ç»„ç»‡ï¼Œæ„å»ºåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æç¤ºè¯ï¼Œæ˜ç¡®æ ‡æ³¨æ¯ä¸ªçŸ¥è¯†ç‰‡æ®µçš„æ¥æºæ–‡ä»¶ï¼Œè¦æ±‚LLMä¸¥æ ¼åŸºäºæä¾›çš„çŸ¥è¯†å›ç­”ï¼Œå¹¶å¯æåŠä¿¡æ¯æ¥æºã€‚æ„å»ºçš„æç¤ºè¯ä»¥"prompt"ç±»å‹äº‹ä»¶è¾“å‡ºã€‚

ç„¶åæ˜¯æµå¼ç”Ÿæˆé˜¶æ®µï¼Œç³»ç»Ÿè°ƒç”¨æŸ¥è¯¢å¼•æ“æ‰§è¡ŒæŸ¥è¯¢ï¼Œè·å–æµå¼å“åº”ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿå®æ—¶åˆ†æè¾“å‡ºçš„æ–‡æœ¬æµï¼Œæ™ºèƒ½åŒºåˆ†"æ€è€ƒè¿‡ç¨‹"å’Œ"æœ€ç»ˆç­”æ¡ˆ"ä¸¤éƒ¨åˆ†ã€‚é€šè¿‡æ£€æµ‹"æ€è€ƒï¼š"ã€"åˆ†æï¼š"ç­‰å…³é”®è¯æ ‡è®°æ€è€ƒè¿‡ç¨‹çš„å¼€å§‹ï¼Œæ£€æµ‹"ç­”æ¡ˆï¼š"ã€"å›ç­”ï¼š"ç­‰å…³é”®è¯æ ‡è®°æ€è€ƒä¸å›ç­”çš„è¾¹ç•Œã€‚æ€è€ƒè¿‡ç¨‹çš„æ¯ä¸ªæ–‡æœ¬å—ä»¥"thinking_chunk"ç±»å‹è¾“å‡ºï¼Œå›ç­”éƒ¨åˆ†ä»¥"answer_chunk"ç±»å‹è¾“å‡ºï¼Œå®ç°ç±»ä¼¼DeepSeek-R1çš„æ€è€ƒè¿‡ç¨‹å±•ç¤ºæ•ˆæœã€‚

æœ€åæ˜¯ç»“æœæ•´åˆé˜¶æ®µï¼Œç”Ÿæˆå®Œæˆåï¼Œç³»ç»Ÿæ”¶é›†å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆï¼ŒåŒæ—¶æ•´ç†æ‰€æœ‰å¼•ç”¨çš„æºèŠ‚ç‚¹ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¯ä¸ªèŠ‚ç‚¹çš„æ–‡æœ¬é¢„è§ˆã€ç›¸ä¼¼åº¦åˆ†æ•°å’Œæ¥æºæ–‡ä»¶ï¼Œä»¥"complete"ç±»å‹äº‹ä»¶è¿”å›å®Œæ•´ç»“æœã€‚

ä¸‰ç§è¿è¡Œæ¨¡å¼è¯¦è§£
äº¤äº’å¼æ¨¡å¼æ˜¯ç³»ç»Ÿçš„ä¸»è¦ä½¿ç”¨æ–¹å¼ï¼Œåˆå§‹åŒ–RAGç³»ç»Ÿåè¿›å…¥é—®ç­”å¾ªç¯ã€‚ç”¨æˆ·è¾“å…¥é—®é¢˜åï¼Œç³»ç»Ÿå®æ—¶å±•ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æœ¬å—åŠå…¶ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç„¶åé€å­—æ˜¾ç¤ºæ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼Œæ¥ç€æ˜¾ç¤ºæœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆï¼Œæœ€ååˆ—å‡ºæ‰€æœ‰ä¿¡æ¯æ¥æºæ–‡ä»¶åŠå…·ä½“å¼•ç”¨çš„æ–‡æœ¬ç‰‡æ®µï¼Œå®ç°å®Œå…¨é€æ˜çš„é—®ç­”è¿‡ç¨‹ã€‚

æ‰¹é‡å¤„ç†æ¨¡å¼é€‚ç”¨äºéœ€è¦å¤„ç†å¤šä¸ªé¢„å®šä¹‰é—®é¢˜çš„åœºæ™¯ã€‚ç³»ç»Ÿä»questions.txtæ–‡ä»¶è¯»å–é—®é¢˜åˆ—è¡¨ï¼Œæˆ–ä½¿ç”¨å†…ç½®ç¤ºä¾‹é—®é¢˜ï¼Œä¾æ¬¡å¤„ç†æ¯ä¸ªé—®é¢˜ã€‚æ¯ä¸ªé—®é¢˜çš„å›ç­”ç»“æœå’Œæ¥æºä¿¡æ¯è¢«æ”¶é›†å¹¶ä¿å­˜ä¸ºç»“æ„åŒ–çš„JSONæ–‡ä»¶ï¼Œä¾¿äºåç»­åˆ†æå’Œå¤„ç†ã€‚

ç›‘æ§æ¨¡å¼é€šè¿‡watchdogåº“å®ç°æ–‡ä»¶ç³»ç»Ÿçš„å®æ—¶ç›‘æ§ã€‚å½“æ£€æµ‹åˆ°æ–‡æ¡£ç›®å½•ä¸­çš„æ–‡ä»¶è¢«ä¿®æ”¹ã€åˆ›å»ºæˆ–åˆ é™¤æ—¶ï¼Œç³»ç»Ÿè‡ªåŠ¨è§¦å‘ç´¢å¼•é‡å»ºï¼Œç¡®ä¿çŸ¥è¯†åº“çš„å®æ—¶æ€§ã€‚åŒæ—¶ï¼Œç”¨æˆ·ä»å¯è¿›è¡Œé—®ç­”äº¤äº’ï¼Œç³»ç»Ÿä¼šä½¿ç”¨æœ€æ–°çš„ç´¢å¼•æä¾›æœåŠ¡ã€‚ä¸ºé˜²æ­¢é¢‘ç¹é‡å»ºï¼Œè®¾ç½®äº†60ç§’çš„é‡å»ºå†·å´æ—¶é—´ã€‚
```

## ç»“è¯­

æœ¬æ–‡ä»RAGçš„å‡ºç°åŸå› å‡ºå‘ï¼Œè¯¦ç»†è®²è§£äº†å‘é‡åŒ–æµç¨‹ã€æ¨ç†æ£€ç´¢æµç¨‹å’Œè¾“å‡ºæ•´ç†ä¼˜åŒ–ï¼Œå¹¶æœ€ç»ˆç»™å‡ºäº†ä¸€ä¸ªå®Œæ•´ã€å¯è¿è¡Œçš„æœ¬åœ°RAGç³»ç»Ÿå®ç°ã€‚é€šè¿‡è¿™ä¸ªå®æˆ˜é¡¹ç›®ï¼Œä½ å¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°ï¼šRAGçš„æœ¬è´¨å°±æ˜¯**æ£€ç´¢ + ä¸Šä¸‹æ–‡å¢å¼º + ç”Ÿæˆ**è¿™ä¸‰æ­¥æ›²ã€‚

å½“ä½ è¿è¡Œè¿™ä¸ªç³»ç»Ÿï¼Œè¾“å…¥"å¥åº·çš„æ°´æœæ¨è"ï¼Œå³ä½¿çŸ¥è¯†åº“ä¸­ä»æœªå‡ºç°"æ¨è"äºŒå­—ï¼Œç³»ç»Ÿä¹Ÿèƒ½é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦æ‰¾åˆ°è‹¹æœã€é¦™è•‰ã€æ©™å­çš„æè¿°â€”â€”è¿™å°±æ˜¯å‘é‡æ£€ç´¢çš„é­…åŠ›ã€‚å½“ä½ çœ‹åˆ°å¤§æ¨¡å‹åŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†ç»™å‡ºå‡†ç¡®å›ç­”ï¼Œè€Œä¸æ˜¯å‡­ç©ºç¼–é€ æ—¶ï¼Œä½ å°±ç†è§£äº†RAGä¸ºä»€ä¹ˆèƒ½æˆä¸ºAIåº”ç”¨å¼€å‘çš„åŸºçŸ³ã€‚

ä»ç®€å•çš„æµæ°´çº¿åˆ°è‡ªä¸»çš„Agentic RAGï¼Œè¿™é¡¹æŠ€æœ¯ä»åœ¨å¿«é€Ÿæ¼”è¿›ã€‚ä½†æ— è®ºå¤šå¤æ‚çš„ç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒéƒ½æ˜¯æœ¬æ–‡æ­ç¤ºçš„è¿™äº›åŸºæœ¬åŸç†ã€‚å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ æ‰“ä¸‹åšå®çš„åŸºç¡€ï¼Œåœ¨RAGçš„ä¸–ç•Œé‡Œèµ°å¾—æ›´è¿œã€‚